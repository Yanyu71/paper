
第一章绪论
1.1研究背景及意义
近年来，随着电子信息技术的飞速发展，物联网、5G网络、云计算、大数据等 新型应用不断出现，网络数据流量也随之急剧增长。为满足这种流量增长，数据中心 的核心层和汇聚层的带宽从10G向25G、40G> 100G发展⑴，如图1-1所示。同时 随着业务类型的升级和用户数量的增多，在接入层对高吞吐量、低时延的需求愈加 明显，这对通用服务器内部的网卡和中央处理器(Central Processing Unit, CPU)的 处理性能要求逐渐变高。

\ervers Servers	servers	servers | /40G I【uuo

图1-1数据中心网络架构
同时，在金融证券交易等一些应用领域，用户的增多和业务类型的升级使得金 融虚拟应用对通用处理器的硬件性提出了更高的要求。毫秒级的算法交易时延难以 满足实时处理和实时响应的要求，业内普遍需要不高于10微秒的处理时延[2],以及 百兆每秒的吞吐率。当面对一些消耗计算资源较大的业务时，CPU难以保证业务的 服务质量，给CPU处理数据带来了很大的挑战【％在通用服务器内部，传统的协议 栈工作方式和不断增大的网络带宽使得通用处理器在处理大量网络数据时资源占用 率高且效率低，增加了网络数据处理的时延。而且目前国内金融行业的交易系统大 部分基于传统通用CPU架构，交易提速存在瓶颈，传统的软件技术或以软件为核心 的加速技术难以满足微秒级的实时处理和实时响应的要求⑷。
为了满足高吞吐量以及低时延的处理需求，同时加速通用处理器对网络数据的 处理，一些大规模数据中心和云服务提供商需要能够从软件层面上卸载各种关键数 据中心应用〔5-7〕，以减轻CPU负担。目前业内研究人员试图将部分软件层面的功能下 放到硬件网卡来实现，释放更多的CPU资源。在网卡硬件上进行加速是目前的主要 思路。多年来，硬件的能力迅速提升，网卡上的处理器能够提供的性能已经远远超出 了简单的数据包接收转发的需求，这在技术上提供了许多原先由软件实现的功能下 移现在由网卡硬件直接完成的可能，从以太网，IP数据报文头的校验和计算、加解 密等功能，发展到对高级协议的分析处理罔。因此，能够满足高速的网络处理需要、 卸载CPU不适合的网络处理任务、提供一定编程灵活性的智能网络终端设备——高 速智能网卡，应运而生，并且在协议处理、网络功能、数据中心以及金融交易等诸多 场景中发挥了重要作用【9］。可以预见，高速智能网卡技术逐渐进入数据中心和金融证 券交易领域，未来高速智能网卡直接提供的功能将会占据越来越大的比重，成为未 来的科技趋势冋。
1.2国内外研究现状
在学术界，高速智能网卡的雏形由微软亚洲研究院2014年基于现场可编程门阵 列(Field Programmable Gate Array, FPGA)提出，属于一种用于加速大规模数据中 心服务的可重构网络［⑴，并在后续的一系列研究中逐渐发展。
Daniel F, Andrew P, Sambhrama M 等人在 2018 年 NSDI 会议上发表了 论文《Azure accelerated networking: SmartNICs in the public cloud》，此文章基于 FPGA 实现了 用于 Azure加速网络的定制高速智能网卡，其中网卡主要实现了主机网络卸载、软件可编 程性等功能，实验表明其效率和性能可满足Azure网络〔叨。Caulfield A, Chung E等 人在 2016 年 ACM 会议上发表了 论文《A Cloud-Scale Cceleration Architecture》，此文 章同样基于FPGA实现了在云架构中网络交换机和服务器之间的高速智能网卡，其 通过可重构逻辑对网络层面的功能和应用进行加速，并可应用在数据中心级的大型 网络处理场所〔⑶。Caulfield A, Costa P等人在2018年HPSR会议上发表了论文 《Beyond SmartNICs: Towards a fully programmable cloud》，此文章重点讨论了基于 FPGA的高速智能网卡和可编程思想去实现云基础设施中的硬件加速，并阐述了二 者结合的优势以及所面临的挑战WMiano S, Doriguzze R等人在2019年IEEE Access 期干ll 上发表 T 论文《Introducing smaitnics in server-based data plane processing: The ddos mitigation use case》，此文章将一部分DDoS缓解规则卸载到高速智能网卡中，从而 增强边缘服务器的缓解能力，优化数据包的处理〔"I。
在产业界，高速智能网卡的产品最初主要由有一定市场和技术储备的成熟网络 设备生产商 Mellanox[16], Netronome[17], Broadcom[18], Cavium[19]提供，Netronome 公 司于2016年9月在公司网站发文，对高速智能网卡的需求和定义进行了阐述，提出 高速智能网卡必须具备实现复杂网络数据平面功能的能力，可以灵活地更改数据平 面，并且与现有生态无缝衔接。而Mellanox公司则于2018年8月发文，借助 PCMagazine对高速智能网卡的定义 能够卸载CPU通用处理任务的网卡，介绍 了该公司推出的3种基于不同处理器架构的高速智能网卡产品。此外，近几年陆续 出现了许多推出高速智能网卡的小公司，如Bitt Ware, Etliemity等，而Amazon ＞华 为这样的大公司在近5年里也陆续收购小公司或者研发高速智能网卡用于自身部署， 甚至推向市场。
综上所述，高速智能网卡具有更强的处理能力和更灵活的技术实现[2°],可满足 目前数据中心、金融证券交易、网络虚拟化等特殊场景的应用。随着万兆以太网的普 及，网卡作为连接网络和主机的纽带，其数据传输性能将直接影响整个系统的效率， 对高速智能网卡展开研究具有一定指导价值和意义。
1.3论文研究内容
无论从学术角度还是产业发展的角度，对于高速智能网卡的实现依然需要做很 多细致的工作，这对国内关于高速智能网卡的发展将产生重要的指导作用，同时为 了满足数据中心以及金融领域等场景对于高吞吐量低时延需求，本文将基于千兆以 太网卡架构进行改进和优化。不直接使用国外的IP核进行实现主要由于：第一，支 持国内关于高速智能网卡研究和实现的发展，提升自主设计能力。第二，对于数据安 全非常重视的金融交易和军事等领域，避免造成数据泄露。
本文基于现有千兆以太网卡架构重新设计成万兆以太网卡，并且对其中的关键 模块进行改进和优化。具体改进优化如下：设计实现基于64b/66b编码技术的物理编 码子层(Physical Coding Sublayer, PCS)；采用内容可寻址存储器(Content Addressed Memory, CAM)技术在介质访问控制层(MediaAccess Control, MAC)中对 MAC 地址的匹配查找进行设计，增加基于MAC地址卸载加速模块；设计实现支持基于 PCI-Express的直接内存访问(Direct Memory Access, DMA)工作方式，并采用基于 描述符(Buffer Descriptor, BD)的方式对PCIE事务层以及DMA控制器进行设计 实现。本文通过VHDL硬件描述语言通过Vivado平台对各部分进行RTL级的代码 设计，并且使用Vivado+Modelsim进行联合仿真。下面对各部分内容进行介绍。
第一，网络数据流量的增多主要由于高速带宽的升级，从过去的千兆以太网到 现在的万兆以太网普及，用户对于线路速率的需求越来越大，千兆以太网接口速率 难以满足目前带宽的需求。本文基于64b/66b编码设计实现PCS模块，根据 IEEE802.3ae中PCS模块的规范设计出64b/66b编解码器、加解扰器、收发变速箱等 模块，其中采用一种滑动机制重点设计实现了接收通道中的Sync_Block同步头模块, 解决了高传输速率下数据快速同步的问题，满足了 lOGbps带宽的需求并且降低了传 输时延。
第二，传统的协议栈工作方式成为网络加速的瓶颈，目前业界将网络功能或网 络协议下放到硬件网卡中，从而减轻处理器的负载，释放CPU的资源，加快网络处 理速率。本文在MAC层对基于CAM存储器的MAC地址的配置、解析、匹配查找 和动作模块进行设计。
第三，用户对于多业务类型以及超低时延的处理需求日益增多，网络虚拟化技 术的出现虽然可以支持多业务类型，但随着业务类型的升级，数据流量快速增长，在 数据转发到主机侧这一过程中同样占用大量的CPU资源。本文设计基于PCIE的 DMA技术降低转发时延。其中基于描述符的DMA工作方式对PCIE事务层逻辑以 及DMA控制器进行重点设计和实现。
1.4论文结构安排
基于上述研究内容，本文分为五章，各章节内容安排如下：
第一章为绪论，首先在数据中心以及在金融领域随着网络虚拟化的兴起对于高 吞吐量低时延的需求给CPU带来更多的负载,影响数据的处理效率。在这种需求下， 高速智能网卡应运而生。然后对目前国内外学术界以及产业界关于高速智能网卡的 研究现状进行了调研。最后提出了本文的主要研究内容。
第二章为高速智能网卡总体研究分析，首先根据产业界不同架构的高速智能网 卡进行功能和技术的总结分析，并对重点技术理论进行阐述和分析。最后提出了本 文网卡的主要功能和技术指标以及总体设计框图，并对关键模块进行分析。
第三章为高速智能网卡关键模块设计分析，首先深入网卡系统对关键模块进行 设计分析，并提出各关键模块的总体设计框图。然后总结出在设计各关键模块过程 中的难点和重点。针对这些难点重点，通过研究分析目前具体技术解决方案提出了 本文的设计方案。
第四章为高速智能网卡关键模块具体设计实现，基于第三章对各关键模块的总 体设计，对关键模块分别进行设计实现。首先设计实现基于64b/66b编码的PCS模 块，其中重点设计了一种滑动机制完成了同步头的锁定；然后对MAC地址卸载加速 功能进行设计，其中在MAC层基于CAM存储器重点完成了匹配查找过程的设计； 最后基于PCIE总线协议对PCIE DMA模块设计实现，主要对PCIE事务层以及DMA 控制器重点进行设计实现，使其支持基于描述符的DMA工作方式。本章大部分内容 通过代码设计或者原理设计进行实现。
第五章为系统测试与验证。首先基于IP核生成Example工程进行了测试框架的 搭建。然后对PCS模块、MAC地址卸载加速模块以及PCIE DMA模块进行了仿真 测试，结果表明各模块功能正确。其中在对收发数据包的验证中，表明了网卡系统整 体功能正确。
第六章为工作总结与展望。对本文改进和调整后的高速智能网卡系统中的相关 设计实现工作作出进一步的总结，并针对每个部分的不足以及后续可能进一步的研 究内容和方向进行了展望。
第二章高速智能网卡总体研究分析
高速智能网卡发展迅速，已经成为数据中心等大型计算中心重要的设备。本章 对高速智能网卡的整体进行研究分析，根据不同架构和不同应用场景的智能网卡进 行分析比较，对目前高速智能网卡功能进行了总结，归纳出主要功能并对重点技术 原理和方法进行了分析，最后提出本文的网卡主要功能、技术指标以及总体逻辑架 构。
2.1高速智能网卡分析
本小节将按照基础架构以及应用场景对目前国内外高速智能网卡进行分析，通 过对不同产品的特点进行归纳总结，从不同维度比较其优劣，从而确定本文实现的 网卡的设计思路以及总体指标。
2.1.1基础架构分析
基于软件实现的加速优化方案无法从根本上带来网络数据转发的性能提升0】， 基于硬件平台实现的加速架构成为了研究趋势。目前围绕硬件网卡实现架构主要有 三种主流的架构：片上系统(System-on-Chip,SoC )架构、特定应用集成电路 (Application Specific Integrated Circuit, ASIC)架构以及现场可编程门阵列(Field Programmable Gate Array, FPGA)架构。下面将依次对网卡不同的实现架构进行分 析。
(1) 基于SoC架构
片上系统是一类具有片上存储器的嵌入式处理器内核，使用的处理器核包括专 用的网络处理器(Network Processor, NP)以及通用处理器(General Processor,GP), 下面将对这两种处理器进行介绍。
a. 基于NP-SoC智能网卡
基于NP-SoC的智能网卡特定应用于通信领域的各种任务，类似包处理、协议分 析、服务质量(Quality of Service, QoS)等。比如Cavium推出基于cnMIPS III网络 处理器的LiquidlOH］系列智能网卡。它包括对基于Linux和DPDK的完整网络软件 堆栈，QoS 和 GENEVE ( Generic Network Virtualization Encapsulation )等网络卸载功 能的支持。LiquidlO III架构基于0CTEON架构处理器，拥有36核，最高频率可达 2.2GHzo同时提供5端口的1 OOGbps的以太网适配器以及两个PCIE Gen4 X 16接口。
b. 基于GP-SoC智能网卡
基于GP-SoC的智能网卡使用通用处理器，这种网卡在性能上低于基于NP-SoC 的智能网卡，但在编程方面相比于NP-SoC要友好。像基于GP-SoC的智能网卡产品 比如Mellanox推出了基于BlueField IPU(I/O Processing Unit)系列可编程智能网卡 ［⑹一ConnectX-5。其可提供两个25/50/1 OOGbps端口或者单个200Gbps端口，通过 PCIeGen4X16连接到主机。同时可支持在线TLS动态数据加密、网络虚拟化、RoCE (RDMA over Converged Ethernet)和 NVMe-oF (NVMe over Fabrics )存储加速等卸 载功能。
基于NP-SoC和GP-SoC的智能网卡架构一般如图2-1所示，均含有以下重点模 块：1.网络接口模块，支持如以太网、InfiniBand协议等；2.多种已经成熟的加速部 件，如CAM存储器、Hash计算等；3。用于与主机通信的PCIE接口，多数支持SR- 10V； 4.多种与外设通信的接口，如PC, JTAG等；5.片上NP或者GP多核，用于 OVS,接收端缩放(Receive Side Scaling, RSS)等网络功能，以及用户自定义功能。

智能网卡上的存储
图2-1基于SoC智能网卡架构

(2) 基于ASIC架构
目前，市面上仅存在少量的基于ASIC的智能网卡，ASIC芯片在智能网卡中的 主要负责网络控制器的功能，比如Mellanox的ConnectX系列㈤，Broadcom的 NetXtreme系列〔却以及Cavium的FastLinQ系列〔绚。此类ASIC网卡芯片不仅具备一 定的卸载CPU处理能力和可编程性，还能够满足传统的网络协议(如TCP)处理需 求。以Mellanox最新的ConnectX-6产品为例，其在一定程度上提供对数据平面的可 编程处理和硬件加速功能，可硬件卸载网络虚拟化中的VxLAN(Virtual Extensible
Lock Area Networks) , NVGRE ( Network Virtualization use Generic Routing Encapsulation)等协议，并且提供虚拟化、SDN的支持，卸载网络安全中的部分加解 密运算，同时支持PCIE4.0X16主机侧接口和200Gbps双端口的吞吐量。
(3) 基于FPGA架构
基于FPGA架构的智能网卡通过硬件编程语言如VHDL和Verilog实现不同的 配置文件，将配置文件写入FPGA芯片中，从而使FPGA实现不同的功能，因此FPGA 同样具有很高的可编程性。2014年，微软提出了基于高端FPGA——AlteraStratixV D5〔25啲Shell (通用逻辑)+Role (可重构处理逻辑)的可重构数据中心云服务加速 方案，用于解决商用服务器满足不了飞速增长的数据中心业务需求、定制化加速器 成本开销大且灵活性不足的问题［⑴。如图2-2所示，其中Shell包括很多可重用的逻 辑位，这些逻辑位对所有应用程序都通用，并将它们抽象为一组定义良好的接口，例 如基于PCIE的DMA核，2个DRAM内存管理单元和4个lOGbps以太网接口。 Role位于FPGA芯片的固定区域中，与用户加速应用的逻辑紧密相关。
DRAM
Socket服务器
CPU

10
10 10 Gbps

图2・2基于FPGA的Catapult智能网卡架构
综上所述，如表2-1所示。1.在性价比方面，基于ASIC的智能网卡基本上可以 满足多数通用网络处理的应用场景，可以在预定义的范围内对数据平面进行可编程 处理，并且只能提供有限范围内的硬件加速支持，但对于模块的移植性和灵活性不 具优势；2.在编程复杂度方面，基于FPGA和ASIC的智能网卡要远低于基于SoC的 智能网卡；3.在使用灵活性方面，基于FPGA和SoC的智能网卡要优于基于ASIC的 智能网卡。因此相比SoC, FPGA的处理效率更高，速度更快。相比于ASIC芯片, FPGA的开发难度小，而且开发周期更短，并且FPGA高速并发的特征保证了其高 吞吐率和低延时，而且对于模块的移植性和灵活性，FPGA更具有优势。本文将基于 FPGA架构对网卡进行开发设计。
8

表2-1不同架构网卡的特点对比
核心处理器
FPGA
NP-SoC
ASIC
GP-SoC
性能
较高
较高
高
低
价格
较高
较低
低
中等
可编程复杂度
较低
高
较低
高
灵活性
较高
较高
低
高
典型产品
Microsoft,
Xilinx Alveo
Netronome,
LiquidlO
Mellanox,
ConnectX
BlueField,
Broadcom
2.1.2应用场景分析
高速智能网卡作为满足高效处理协议、支持多种网络功能以及提供高速的网络 速率的产物，其用场景广泛，主要包括网络协议处理、网络功能处理以及金融算法交 易，下面将从以上场景进行介绍。
(1) 网络协议处理
网卡作为一种网络设备，最基础的应用场景是对网络协议进行处理。目前网络 协议繁多，高速智能网卡普遍支持标准的以太网和标准的Infiniband以及自定义的网 络协议，有些高速智能网卡甚至支持NVMe-oF存储方面的协议〔⑹。
以微软公司为例，其高速智能网卡在初期已经将TCP/IP协议族中的部分协议进 行卸载，如 TCP 减负引擎(TCP Offload Engine )、UDP 减负引擎(UDP Offload Engine) "I,处理此类协议时，高速智能网卡通常以流水线方式进行处理。为了应对密集型 的网络流量，在之后引入了 RSS〔27]以及Flow Director卸载引擎，以支持用户自定义 网络协议的卸载。近年来，在虚拟化的云计算场景中，NVGRE以及VxLAN〔28]等协 议也在网卡中进行卸载，此类协议对网卡的并行能力和隔离性有着一定的要求。
(2) 网络功能处理
网络功能的作用是传统网元在组网时对数据包所进行的检测和修改，比如防火 墙(Firewall服务质量(QoS)、负载均衡(LoadBalance)等，此类网络功能通常 采用流水处理的实现方法。高速智能网卡主要对软件自定义网路(Software Defined network, SDN)、网络功能虚拟化(Network Function Virtualization, NFV)等新型网 络功能进行处理，此类网络功能需要较高的计算复杂度。
比如Xilinx公司最新发布的Alveo™ SmartNIC系列，代表产品Alveo™ SN1000[29] 面向各类功能卸载提供了软件定义硬件加速功能，既可以直接卸载CPU密集型任务 以优化联网性能，又可以卸载SR-IOV和OVS以加速网络数据处理。在2.1.1小节 提到的ConnectX-6产品同样支持此类网络功能，其充分利用了软件定义和硬件加速 等先进技术，支持先进的虚拟化和容器化功能处理、加速交换和数据包处理(ASAP2) 等，保障了高性能的可扩展网络处理功能。
(3) 金融算法交易
目前在一些金融证券交易场所，业内均采用软件进行行情解码，提供行情服务， 但基于软件的行情解码无法满足服务时延需求，基于硬件的高速智能网卡越来越多 的应用于此场景。
比如Xilixn公司发布的Alveo™ U50I30],其可提供超高吞吐量、小数据包性能与 超低时延，旨在解决金融算法交易等场景中服务时延大的问题。主要通过低时延关 键值存储技术(Low latency Key-Value Store, KVS)实现，其时延可控制在400ns以 下，搜索速率可达150M次搜索/秒，充分降低时延和用电成本。
综上所述，如表2-2所示，目前高速智能网卡在不同的应用场景中的侧重点不 同，主要对计算复杂度、并行能力、隔离性以及流水处理能力这几点具有一定的需 求，本文在基于FPGA架构对网卡进行设计实现，可保证上述需求基本得到满足。
表2-2不同应用场景网卡的特点对比
应用场景
网络协议处理
网络功能处理
金融算法交易
计算复杂度
较低
较高
较高
并行能力
较高
较低
较高
隔离性
高
较高
较低
流水处理能力
较高
较高
低
典型产品
Microsoft
catapult
Alveo™SN10005
ConnectX-6
Alveo™ U50
经过上述对不同架构和不同应用场景的网卡产品进行分析分比较，可以看出,
目前智能网卡提供的功能大致可以归纳如表2-3。
表2-3典型产品基于功能划分
典型产品
Microsoft catapult,Mellanox ComiectXJntel FPGA PAC N3000
网络侧带宽
双端口 10/25/50/1 OOGbps
硬件加速功能
RoCE、NVME-oF存储加速卸载；TCP/UDP、OVS网络协 议功能卸载
主机侧接口
PCIE Gen2/3/4 x 8, PCIE Gen2/3/4 x 16
1. 在网络接口部分，提供支持lOGbps甚至更高速率的以太网协议；
2. 在网卡内部，提供可编程的硬件加速卸载功能，比如网络协议卸载(TCP、UDP 等)，网络功能卸载(OVS, RSS等)；
10
3. 在主机侧接口部分，采用高速PCIE总线接口，普遍支持PCIE2.0 X 8通道(4GB/s) 以上速率。
本文目的在于提供智能网卡的整体设计思路，同时满足目前智能网卡的基本功 能需求，因此本文将从以上三方面进行设计开发。
2.2网卡关键技术理论研究
在对各种网卡架构分析之后，本小节对上一小节归纳的功能技术进行深入分析， 其中主要对10G以太网接口部分、网卡内部卸载加速功能、主机侧接口部分中的相 关技术理论进行研究分析，为后文的设计工作做好铺垫准备。
2.2.1 64b/66b编码技术
目前以太网协议中的接口部分中的PCS层采用的编码方式不同。如图2-3所示， 在千兆以太网中，以1000Base-T协议为例，其采用8b/10b的编码方式。这种编码方 式最大的特性为保证直流平衡，并且通过特殊的控制字符进行同步(lOOOBase-T协 议中使用K字符Pi】)。然而，这种编码技术在发送10比特位时，其中只有8比特位 是有效数据位，信道的传输效率只有80%,开销很大，带宽利用率不高。
为了支持高速率以太网协议，IEEE 802.3ae工作组提出了 64b/66b编码技术【词, 目的是减少编码开销，降低硬件的复杂性，并且相比于8b/10b的传输效率，其传输 效率达到了 97%。其可作为8b/10b编码的另一种选择，以支持新的数据和程序，在 后续以太网标准中都沿用该编码方式。由于其可接受的游程长度和低开销，64b/66b 编码技术成为高速互联通道、芯片间以及背板互联的主要编码技术【3現


11
目前，64b.66b编码技术主要应用于10G光纤通道(Fiber Channel)、10G以太 网［34-35］、40G以太网［叭1OOG以太网［37］和Xilinx的Aurora协议〔砌。在对64b/66b编 码技术进行设计实现时，不同于8b/10编码技术，主要体现在以下方面：
(1) 编解码规则复杂
64b/66b的编码规则需要根据输入的64bit数据块来产生相应的同步头(Sync Header块类型区(BlockType Field)以及块有效载荷(Block Payload)。64bit 的数 据块由8个字符表示，每个字符都有相应的意义。如果8bit为用户数据，则使用D 字符进行表示；如果8bit为控制数据，需要按照编码规则用不同的字符(S、T、0) 进行表示。当用正确的字符表示完成后，还需要按照字符的排列添加块类型区，不同 块类型区也代表着不同的含义。解码需要进行编码的一系列逆向操作［迢。
(2) 需要进行扰码/解码
对于64b/66b编码技术来说，其无法像8b/10b编码技术保证直流平衡。8b/10b 可以在8bit中插入2bit使长0或长1的位数不超过5位。但64b/66b从64bit中插入 2bit无法保证直流平衡。因此在64b/66b编码技术中需要采用扰码(ScamblingCode) 进行直流平衡［39］,即对64bit的数据块进行比特级的随即处理，减少长0和长1的岀 现，从而减少码间干扰和抖动，方便接收端的时钟提取。在发送端需要特殊的扰码机 制进行扰码，并且在接收端需要进行解扰。
(3) 位同步对齐
在8b/10b编码技术中，有用于接收端同步的K字符。而在64b/66b编码中只有 2bit的同步头用于同步，需要在lOGbps传输速率下在66bit中找到2bit的同步头， 从而进行同步对齐，因此在接收端需要一种快速的同步机制，且不会做重复判断。目 前对于这部分内容普遍采用滑动窗口机制对同步头的进行锁定〔*35,40］,即以66bit为 窗口进行滑动，每次对前2bit进行判断，如果判断为有效头，则继续判断，直到判 断次数到达设定值；如果判断为无效头，需要移位重新选取新的66bit进行滑动。
2.2.2硬件卸载加速技术
目前高速智能网卡普遍提供硬件卸载加速功能，比如前文提到的TCP/UDPBS］ 等网络协议卸载功能以及RSS和OVS等硬件加速技术，这些加速技术通常是可编 程的，可完成网卡的智能化操作。业界将这部分的功能全部或部分下移到硬件网卡 中去实现［44］,从而减轻协议栈的工作压力，释放CPU的资源，如图2-4所示。



图2-4协议栈功能卸载示意图
目前硬件卸载加速技术主要面向数据密集型的网络流量，这种网络流量需要高 速智能网卡至少具备3个特点［9〕： 1.减少网卡到主机侧对数据搬移的开销；2.向用户 提供可编程接口，满足用户级的处理需求，以提高网卡的智能化；3.将软件编程和硬 件架构之间进行合理的映射，以实现对数据流进行高效的流水处理。关于硬件卸载 加速技术主要有两种典型的架构：微软提出的ClickNP架构味］和基于可重构匹配表 (Reconfigurable Match Table, RMT)架构〔佝。
(1)基于ClickNP架构
微软针对Catapult架构提出了 ClickNP编程架构，主要用于商用服务器中，以 提供高性能的网络功能，如防火墙、网关等。ClickNP向用户提供了类似C语言语 法，并且提供了近100个处理单元(elements)库，如图2-5所示。编写的程序经过 编译后会由FPGA提供的后端编译器和主机CPU的程序进行编译处理，将不同的任 务分配到FPGA和CPU中，使其协同工作。FPGA内部采用模块化设计，多个elements 并行加速处理，通过缓冲通道连接。

图 2-5 ClickNP 架构

13
(2)基于RMT架构
FlexNIC【8］的设计继承了 RMT架构，是目前典型的流水线操作处理形式。目前 的网络I/O加速方法是将固定功能卸载特性放到网卡［何中，虽然有用，但这些卸载 绕过应用程序和操作系统，只能执行低级功能，如校验和处理和通常使用的标准协 议(例如，UDP和TCP)的包分割，无法做到更高级别的卸载。FlexNIC提供了一个灵 活的DMA可编程接口的网络I/O,允许应用程序和操作系统在网卡上安装数据包包 处理规则，如图2-6所示。对数据包的处理分为入口流水线和出口流水线两部分，每 一部分都含有包解析、Match-Action、数据包整合3个处理阶段，可编程的功能以 Match-Action的形式映射到多核处理器中，对数据包中自定义的域进行流水处理，并 且增加了 DMA引擎来降低数据搬移的开销。


图 2-6 FlexNIC 架构
综上，对于ClickNP架构，其设计之初主要面向商用服务器中网络功能加速的 应用场景，更强调用户可编程性。而FlexNIC作为高效的流水线处理架构，具有更 灵活的匹配查找方式和简易的编程特点，本文基于FlexNIC架构对硬件卸载加速技 术进行设计，但可以看出当面对复杂的操作时，需要的Match-Action操作数量过多 时会大大增加处理时延，本文考虑到入口流水线的操作已完成包处理的匹配过程， 因此将出口流水线进行简化，减少Match-Action次数；同时在DMA引擎中将数据 包队列替换成描述符队列，以更小的信息内容作为载体进行传递，减少处理时延。
2.2.3高速串行总线技术
网卡为保证与主机侧进行高速的数据搬移，普遍采用PCIE串行技术作为接口协 议，PCIE总线接口作为目前计算机内部常用的总线接口，其釆用串行传输技术，目 前己经发展到PCIE4.0技术［48】，但需要更高成本的主板支持，因此PCIE2.0以及 PCIE3.0仍被硬件网卡所使用。PCIE总线层次结构与计算机网络的TCP/IP协议采用 的OSI模型类似。PCIE的总线协议层次结构如图2-7所示。PCIE总线的层次结构 分为三层,从上至下依次为事务层(Transaction Layer)>数据链路层(Data Link Layer)>
14
物理层(PhysicalLayer)。每个协议层都有相应的数据包，本文采用Xilinx公司提供 的7系列IP硬核R9],此IP硬核为用户提供了事务层的逻辑开发，减少开发的复杂 性。


2.2.4 DMA多队列技术
网卡在进行数据处理后需要将数据快速的搬移到主机内存，目前高速智能网卡 普遍采用DMA方式进行数据搬移a〕，即当网卡需要进行数据搬移时，通过中断或 者轮询机制通知CPU,然后再发起DMA操作进行数据搬移。DMA可在外部设备和 存储器之间进行高速的数据搬移，因此数据只需要少量的CPU参与或者不需要CPU 参与[J】。如图2-8所示。减少CPU的参与可释放CPU的资源，同时将数据传输的 功能通过DMA控制器实现，将工作负载分担，可提高效率。由于DMA技术在网卡 中进行实现，而目前网卡与内存进行数据交互时在主机侧的接口普遍采用PCIE总 线，因而目前DMA的工作方式基于PCIE总线协议实现a〕。
15

系统总线
PCIE
图2-8 DMA工作方式示意图
目前PCIE DMA技术在高速智能网卡应用广泛，作为与主机端连接的部分，其 传输速率影响着网卡整体的效率【刃，一直以来是网卡系统高速传输中的研究重点。 因此本部分内容的研究重点在于设计一种高效的DMA传输方式。目前DMA的传输 方式分为两种：一种是基于寄存器的传输方式，一种是基于描述符的传输方式，下面 将对这两种传输方式进行分析。
(1) 基于寄存器的DMA传输
基于寄存器的传输方式主要面向连续的内存地址空间，尤其当面对大块的数据 需要连续的地址空间来储存时。因为这种方式DMA控制器只会完成一次DMA请求 响应。当有数据需要传输时，CPU进行初始化等一系列操作通知网卡启动DMA操 作，网卡随之将内存中指定的数据进行搬移，结束后中断CPUo因此这种方式适合 一次性传送大量连续的数据。如图2-9所示。


(2) 基于描述符的DMA传输
基于描述符的传输方式主要面向内存地址空间不连续的大量的数据，相比于基 于寄存器的DMA传输方式，即CPU发起一次DMA请求，DMA控制器执行一次数 据传输，基于描述符的传输方式，即CPU在发起DMA操作前，驱动在内存中初始 化多个描述符网。如图2-10所示。这样CPU要传输数据时，只需要发送一次DMA 请求，DMA控制器先将指向这些数据的描述符取到硬件中进行解析，然后再将这些
描述符所指向的数据从内存中搬移到网卡中。

图2-10基于描述符的DMA传输方式
描述符是一种信息载体，它描述了数据所在内存的地址以及数据本身的信息， 同时还带有一些标志位。一个描述符可以描述一个非常小的内存片段，也可以描述 非常大的内存片段。为了合理利用内存空间，以及面对密集性的网络流量，相比于寄 存器式的传输方式，这种方式更为灵活［旳。本文的目的在于完成高速数据的搬移， 从而应对更复杂更密集的网络数据。基于上述两种传输方式，在本文中决定采用基 于描述符的DMA传输方式。
同时数据流量在lOGbps高速率的传输背景下，单队列网卡模式已无法满足数据 转发的时延需求。网卡中的队列一直以来与CPU核进行绑定，早期的多数计算机, 处理器可能只有一个核，从网卡上收到的以太网报文都需要这个处理器处理。随着 半导体技术发展，CPU核的数量在不断增加，通过可以增加网卡中队列的数量并绑 定到不同的核上来满足高速流量的需求，如图2-11所示。
17


图2-11队列与应用的逻辑对应
多队列技术发展至今已经和DMA技术相结合来支持网络虚拟化，即每条收发 队列都以DMA的方式通过PCIE总线与内存进行数据交互，并且通过队列绑定核的 形式对网络虚拟化进行支持。每个虚拟应用通过驱动可以绑定到不同CPU核上，使 得每个虚拟应用逻辑对应队列［旳，从而使得数据在经过对数据包头处理后转发每个 应用对应的队列中，然后通过DMA传输机制完成数据的快速搬移，从而在硬件层面 完成了数据的分流。利用该技术，可以做到分而治之，应用可以根据自己的需求对数 据包进行控制【"I,本文采用描述符多队列机制完成数据的分流。
2.3主要功能和技术指标
高速智能网卡作为在主机侧和网络侧中高速数据传输的设备，主要实现数据接 收，数据处理以及数据发送功能，保障数据传输的高吞吐量和低时延需求。根据对目 前高速智能网卡的功能提出本文网卡的如下功能和关键技术指标：
(1) 支持lOGbps以太网速率
本网卡支持lOGbps的接口速率。千兆以太网的lGbps的吞吐量难以满足数据 中心接入层的吞吐量需求，因此本网卡基于目前万兆以太网协议族中的10GBase-R 协议标准，对其PCS模块进行设计实现，支持lOGbps高速率。
(2) 支持硬件卸载加速
本网卡将协议栈功能进行卸载加速。在第一章分析到，传统的协议栈工作方式 会增加CPU的负载，增加网络处理时延。本网卡将在MAC层中增加MAC地址卸 载加速模块，对MAC地址匹配查找方式进行设计。
18
(3) 支持数据包的快速搬移
本网卡支持基于PCIE的DMA工作方式，在CPU少量的参与下，完成数据包 在网卡和内存之间的快速搬移。因此本文对PCIE的事务逻辑层以及DMA控制器进 行设计实现，减少CPU中断开销以及上下文切换。
本文的关键技术指标如下表2-4：
表2-4系统关键技术指标
设计指标
参数值
接口吞吐量
1 OGbps
PCIE总线
Gen2.OX8通逾殉
转发时延
微秒级(WlOOus)
2.4网卡总体架构研究分析
241总体设计框图
高速智能网卡逻辑架构主要分为物理接口、PCS模块、MAC地址卸载加速模块、 FIFO缓冲、PCIEDMA模块，总体设计框图如图2-12所示。其中对PCS模块、MAC 地址卸载加速模块、PCIE DMA模块进行重点研究设计。下面对各部分功能进行详 细阐述。


2.4.2各模块功能分析
(1)物理接口
物理接口主要包括物理介质相关子层(Physical Medium Dependent Sublayer,
19 PMD)和物理介质连接子层(Physical Medium Attachment Sublayer, PMA)。其中 PMD 层对各种实际的物理媒体完成真正的物理连接，根据不同的物理接口进行不同的信 号转换，比如光电转换等。PMA层主要负责完成串并转换，将以太网线路的串行速 率转换成并行数据。之所以将这两部分放在一起进行讨论主要由于PMD和PMA位 于物理底层，与物理媒体介质关系紧密，已经固化到硬件，不是本文的设计重点。
(2) PCS模块
PCS模块是物理编码子层，同样位于物理层，为物理层的上层部分，与PMA和 10G 介质独立接口 (10 Gigabit Media Independent Interface, XGMII)接口相连，主 要完成64b/66b编解码工作，PCS模块全部功能主要围绕64b/66b编码进行设计囚〕。 在接收过程中，由于PMA和XGMII接口位宽不同，XGMII数据接口为64bit位宽， PMA接口可根据选16bit、32bit、64bit,因此PCS模块将接收到的数据首先进行位 宽转换；然后在lOGbps数据率下完成同步操作，找出数据的起始位置；最后进行解 扰以及解码操作,恢复原始数据，在通过XGMII接口以lOGbps的数据率发送到MAC 层。发送过程为接收过程的逆操作，但不需要进行同步对齐。此部分内容为本文的设 计研究重点。
(3) MAC地址卸载加速模块
此部分内容主要完成基于MAC地址匹配和转发功能。首先将从PCS模块发送 过来的数据包头进行解析，然后提取48bits的MAC地址(目的地址或源地址)进行 匹配查找，最后按照匹配规则对匹配上的数据包执行动作，丢弃或者转发到不同接 收队列。发送过程不进行匹配查找，直接在MAC中进行处理。由于MAC层中主要 对数据进行拆包以及校验计算等操作，接近协议栈的工作内容，在MAC层中进行协 议栈功能的卸载符合设计要求。此部分内容是本文的设计研究重点。
(4) FIFO 缓冲
FIFO缓冲主要用来解决前后模块之间数据接口类型不同、数据位宽不同以及时 钟不同的问题。对于lOGbps的数据率来说，需要10.3125Gbps的线路速率，PMA层 中采用64bit的位宽，线路侧的时钟频率设计为10.3125G妙s十64阳心161.13MHz。为 了减少跨时钟操作，MAC层同样采用此时钟频率。但对于PCIE2.0X8通道来说， 其传输速率为4GB/S,采用128bit的位宽，PCIE端的时钟频率设计为 4GB/SX8一 128加7 = 250辭。由于两端的时钟、位宽不同，需要FIFO缓冲对数据 进行缓冲。考虑到以太网帧最大为1500Bytes,本文将FIFO容量设计为10倍的最大 以太网帧长度，即128bits宽，深度为940,工作时钟频率为200MHzo
(5) PCIE DMA 模块
数据经过FIFO缓冲后将通过PCIE接口转发到主机内存中。本文采用DMA方 式进行数据搬移，DMA与PCIE接口紧密相连，需要依靠PCIE接口进行传输。本
20
文使用PCIE2.0X8通道模式，其理论传输速度为4GB/S,满足万兆以太网的线路速 率。在接收过程中，数据以DMA写的方式通过PCIE发送引擎从网卡写入到内存中； 在发送过程中，数据以DMA读的方式通过PCIE接收引擎从内存读到网卡中进行发 送。此部分内容为本文的设计研究重点。
2.5本章小节
在本章，首先对目前产业界高速智能网卡按照架构和应用场景进行分析总结， 分析表明：基于FPGA的架构进行网卡开发具有成本相对低廉，可编程性高以及处 理效率高等特点，适合网卡硬件的设计开发。然后基于不同类型的网卡架构归纳出 其基本功能，并对实现基本功能的关键技术原理和方法进行分析，本文选择了对基 于64b/66b编码技术的PCS模块、MAC地址卸载加速模块以及基于描述符的PCIE DMA模块进行设计开发。最后提出了本文网卡的主要功能、技术指标以及网卡的总 体设计框图。
第三章高速智能网卡关键模块设计分析
在上一章对高速智能网卡的整体进行分析研究，并对关键模块的功能以及技术 原理进行分析之后。本章将根据实现技术对关键模块进行设计分析，提出各模块的 设计架构，并对各模块中的重点难点部分进行分析设计。
3.1关键模块总体设计分析
3.1.1 PCS模块设计分析
PCS模块的功能是完成编解码操作，主要围绕64b/66b编码技术进行设计。以发 送过程为例，由于采用了 64b/66b的编码方式，万兆以太网线路速率为10.3125Gbps, 即PCS模块发送到PMA的速率为10.3125Gbps,考虑到FPGA要求低时钟频率，需 要提高发送数据位宽，本文采用64bits数据位宽，PMA发送时钟为 10.3125Gbps-646/75 »161.13MHz □同时为了保证时钟同步，MAC的发送时钟同样 设计为161.13M血。
在64b/66b编码方式中，不能保证直流平衡，即不能避免1和0连续的出现。因 此需要对64bits数据进行扰码，需要扰码器做伪随机处理。在经过编码后，采用多 项式进行扰码器的设计。在进行发送之前，还需要将66bits的数据位宽转为64bits数 据位宽，因此再经过扰码后，本文加入变速箱机制做位宽转换。接收过程为发送过程 的逆过程，需要完成接收时的位宽转换，解码以及解扰操作。并且，接收过程中需要 做同步头锁定的操作，即找到66bits中的2bits的同步头,是本文设计的重点和难点， 经过上述分析，本文的PCS模块设计如图3-1所示。
MAC层发送时钟	MAC层接收时钟

图3-1 PCS模块设计框图

22
在PCS模块中将数据通道分为两部分：发送通道和接收通道，分别记为TX通 道和RX通道，两条通道互逆。在TX通道中设计了 64b/66b编码器、扰码器、TX 变速箱。在RX通道中设计了 64b/66b解码器、解扰器、RX变速箱以及Sync_Block 同步模块。第四章将对收发过程以及各模块的功能进行具体设计。
3.1.2 MAC地址卸载加速模块设计分析
MAC地址卸载加速模块主要完成基于MAC地址的匹配查找操作。具体工作流 程为：在预处理阶段，用户根据实际需求下发解析字段和匹配规则。在处理阶段，首 先对经过MAC处理后的数据进行解析，解析操作只对包头部分进行处理，数据部分 保持不变。然后根据流表项对关键域进行匹配查找，重点对关键域的匹配查找方式 进行研究。每查找到相应的流表项后，根据流表项的规则进行转发。最后数据被丢弃 或者被转发到各个队列，完成数据的处理［佝。经过上述对数据处理工作流程的大致 分析，本文将设计方案定为如图3-2所示。


图3-2 MAC地址卸载加速模块设计框图
MAC地址卸载加速模块包括四个模块，即解析模块、匹配查找模块、配置模块、 动作模块。以接收为例，每个模块的具体功能为；解析模块负责对数据进行解析，将 数据包头中的域进行提取，之后将域发送到匹配查找模块；匹配查找模块将接收到 的域进行匹配，得到匹配结果并发送到动作模块；配置模块负责对解析模块和匹配 查找模块下发所要解析的字段和匹配规则。比如MAC目的地址或者包的长度；动作 模块则根据匹配上的域执行相应的动作，在第四章将对基于MAC源地址卸载模块 进行具体设计。
23
3.1.3 PCIE DMA模块设计分析
数据经过MAC处理后进入到此模块，此模块主要负责网卡与主机之间的数据 交互，通过基于PCIE接口的DMA方式降低数据搬移时延。在第二章对DMA传输 方式的介绍中提到，DMA传输方式基于PCIE总线实现，需要对PCIE的事务层和 DMA控制器进行设计实现。
PCIE事务层在协议规范中包括PCIE接收和PCIE发送模块，为了对接收和发 送进行控制，本文加入PCIE控制器。总体设计如图3-3所示，主要包括PCIEIP核、 PCIE发送引擎、PCIE接收引擎、中断控制模块、DMA控制器以及PCIE控制器， 各个模块的功能以及设计将在第四章进行详细阐述。对于网卡来说，需要对描述符 进行读取以及解析，这部分功能由DMA控制器进行实现。其中对描述符的读取需要 以DMA读的方式将描述符从内存中读取回来，这个过程需要驱动以存储器写请求 的方式对网卡中描述符的相关寄存器进行配置，其中相关寄存器控制着描述符的读 取方式以及软硬件的交互，对描述符的读取设计是基于描述符的DMA传输方式的 重点。

图3-3 PCIE DMA模块设计框图

3.2各模块中关键点分析
（1）PCS模块接收同步头的锁定
对于同步头的锁定是本文设计的难点和重点。首先，不同于千兆以太网中的
8b/10b编码方式有专门的控制字符用于同步，64b/66b编码方式只有2bit的同步头数
24 据，需要在66bit中找到这2bit的同步头数据进行对齐同步。其次，需要在lOGbps 高速传输的速率下找到2bit的同步头数据。最后，要保证以最短的时间进行同步， 减少等待时间，提升整体传输效率。
(2) MAC地址匹配查找
目前基于数据包头查找方式主要有：线型查找法、二叉树查找法、哈希表查找法 等，这些查找方法都基于软件进行查找，共同特点是查找速度慢，满足不了高速实时 通信系统(如10G/40G)的极速查找需求。本文在硬件中实现匹配查找功能，可以满足 极速查找需求，但无法像软件查找法可以自由灵活的选择查找方式，受硬件功能限 制，需要选择支持匹配查找的硬件设备。并且受硬件资源限制，需要根据硬件资源对 匹配查找方式进行设计。
(3) 基于描述符多队列的读取设计
在基于描述符的DMA工作方式中需要引入描述符功能，描述符是真实存在于 内存的队列结构，硬件如何从内存中读取描述符以及通过何种方式进行读取是本模 块的研究重点，并且引入描述符多队列机制，即有多条接收描述符队列可支持不同 应用的接收需求。此部分内容涉及到很多PCIE总线和计算机驱动内容，需要充分了 解计算机内部PCIE总线架构和驱动层面的管理模式，为此本文在前期对这部分内容 进行了大量的调研。此部分的难点在于保证驱动对DMA读取的控制管理以及需要 在PCIE总线的基础之上进行设计，并且需要结合硬件资源和应用需求增加多条接收 队列。
3.3关键点解决方案的设计及分析
在上一小节主要对网卡系统的关键部分的工作原理进行分析，并且在分析的过 程中总结出了关键部分的难点和重点：PCS模块中对同步头的锁定；在MAC中提高 匹配查找速度以及进行高效的DMA传输。下面将对这些难点和重点提出相对应的 技术解决方案。
3.3.1 PCS中同步头锁定设计
目前对于这部分的设计普遍采用滑动窗口法，即通过滑动66bit的代码块对所 2bit的同步头进行判断。判断为有效同步头时，Index向后滑动66bit再次判断；当 判断为无效头时，Index向后滑动lbit重新进行判断。
上述方式在大概率下可以不重复的完成同步，但在小概率下会造成重复的同步 判断。由于PMA位宽是16bit或32bit或64bit,总小于66bit,需要对数据进行拼接， 即前文提到的RX变速箱的功能，而Index是RX变速箱中的一个滑动指针。RX变
25
速箱对数据缓存然后拼接成66bit进行发送，Sync_Block同步模块再对66bit进行同 步头的判断。当判断为无效头时，Sync_Block同步模块通知RX变速箱将Index向 后滑动lbit,但从Sync_Block同步模块向RX变速箱发出无效头反馈信息需要时延， 在这一过程中，RX变速箱一直在发送66bit的信息，对于发送多少个66bit无法掌 控，因此Sync_Block同步模块无法对刚刚判断错误的下一个66bit进行判断。有可 能正确的同步头的位置，就在这一过程进行发送，而Sync_Block同步模块已经略过 正确的同步头的位置。如图3-4所示。


图3-4同步头的重复判断
针对这一小概率问题，本文提岀一种基于滑动窗口的改进方式。即将当判断为 无效头时，并不会立刻通知RX变速箱将Index进行滑动，而是继续进行66bit的接 收，一直接收到上次判断为无效头的下一个66bit,这样可保证在进行判断时不会略 过正确的同步头的位置。在进行同步头的锁定过程中，同步头的位置固定重复出现， 即每一个66bit的正确的同步头的位置在经过一个循环周期后会再次出现在相同位 置，因此只需要跳过这个循环周期，并且再多接收一次66bit,即指向了下一个66bito 为避免长0或长1出现，规范规定同步头11或00为无效头，10或01为有效头，具 体设计如下：
滑动机制在本文特指两种滑动一横向滑动和纵向滑动，分别代表两个指针Index 和header_count,通过两种滑动完成同步头的锁定。首先，64bit的数据需要在RX变 速箱进行缓存，然后以66bit将数据进行发送，等待进行同步。为了提高同步效率, 对每次收到的66bit数据的第一、二位进行同步头的判断，如果为01或10,则 header_count+l。当header_count累加到指定数值后，表示完成同步头的锁定，即第 一、二位是同步头所在位置。如图3-5所示。
26


当header_count在未达到指定数值前，对66bit数据同步头的判断时发现了无效 头，则需要继续进行数据的接收，但此时不再做判断。假如在对第3个66bit判断时， 发现了无效头，则继续进行数据的接收，接满一个循环周期后，向下滑动，将第4个 66bit放在第一位置，即开始对第4个66bit进行同步头的判断。如图3-6所示。

图3・6判断失败时滑动示意图
27

当header_count持续累加到一定数值后发现无效头，这种情况出现次数累加到 指定数值后，Index开始进行横向滑动，即从第0位置向右滑动到第1位置，开始对 66bit的第二、三位开始判断，如图3-7所示。可以看出，Index的最大限度为65,即 从第0位置向右滑动到第65位置。通过以上滑动机制，通过硬件进行实现，可大大 减少同步头的锁定时间，提升整体效率，同时对于硬件实现要求不高，简化实现难 度。本部分内容将在第四章进行具体设计。

图3-7持续判斷失败滑动示意图
3.3.2 MAC地址匹配查找方式设计
目前关于硬件的匹配查找主要使用基于FPGA的CAM存储器。在FPGA硬件 资源中，分布着很多通用存储器，这些存储器可以配置为CAM存储器。当使用CAM 存储器进行査找时，整个表项空间的所有数据在同一时刻被查询，每个时钟周期可 完成一次查找。基于CAM存储器技术允许应用程序和操作系统在网卡上安装数据
28

包处理规则，这些规则指示网卡在与主机内存交换数据包时对数据包执行简单操作。 同时这些规则允许应用程序和操作系统对如何与主机交换数据进行细粒度控制。 CAM可以支持根据应用需求进行包的匹配和处理，具有很强的灵活性，同时满足现 在网络可编程的思想，对于未来网络协议也具有很强的支持性，可以随时根据新的 协议内容进行包的处理，更具可扩展性【59］。
如图3-8所示,CAM主要由以下几部分组成:译码器（Decoder 编码器（Encoder）、 CAM阵列（M）、搜索线驱动（SLDrivers）o CAM阵列负责数据的存储和比较，每 一行由多个单元组成，每个单元称为位（bit）。每一列代表不同的条数。CAM容量 由位宽和条数来表示。当地址数据从左进入，进行并行搜素找到指定的地址，如果匹 配成功，则将条数中的内容进行输出A。】。本文选择基于CAM存储器对MAC地址的 匹配查找进行设计。具体设计如下：

图3-8 CAM框图
考虑到硬件资源的分布，即CAM存储器分布在硬件的各处，不会形成太大的存 储结构，不利于匹配和查找，基本以18Kbit为一块进行分布。在对CAM容量的设 计中，需要确定位宽和条数。设CAM容量为卩，位宽代表匹配的规则数为b,条数 代表匹配的长度为/,则可得容量公式：
(3-1)
对于48bits的MAC地址来说，48bits为条数，需要经过译码器将48bits换算成 248个地址，但显然18Kbit的容量无法满足248的地址信息。
!SK«bx24S
因此需要对48bits进行分段匹配，即将48bits分成等大的比特数分别进行匹配。 考虑到目前包头字段的大小，无论MAC地址还是IP地址，都是8bits的整数倍，本
29
文以8bits为单位进行截取，即将48bits分成6段，每段8bits。8bits经过译码器换成
2*个地址，即256个地址。对于18Kbits的容量，深度为256,根据公式（3-1）

宽度可得为70,但输出结果需要进行编码，即将匹配到的结果进行二进制编码, 对于70来说：
27 > 70 > 26
需要7位二进制来表示，将宽度设为70不能完全利用资源，因此将宽度同样设 置为8的整数倍，再结合匹配规则数的数量，本文将宽度设置为32bits,即支持32 条匹配规则。在本文CAM容量的设计为：深度为256,宽度为32,如图3-9所示。

通过上述设计方案提高了空间利用率，同时增加了一定的匹配条数，本文以 MAC地址为例进行介绍，而对于长度为32bits的IP地址，可将其分成4段，即需 要4个CAM。其他数据包头的关键域都可按照此原理进行设计。
3.3.3基于DMA描述符多队列的读取设计
描述符是一种信息载体，指向一块缓冲区，其内容包含这块缓冲区的内存地址 以及大小等〔61],如图3-10所示。描述符由驱动部分进行创建分配，对于硬件网卡来 说，需要关注如何读取描述符以及如何使用描述符。
30


图3-10内存描述符缓冲区

驱动部分与硬件网卡通过读写寄存器进行交互，描述符的交互也通过此类方式。 考虑到描述符信息需要循环利用，描述符在硬件中的缓冲区采用环状的结构。为了 实现环形描述符队列，需要设计两个指针用来指示队列的头尾，头和尾之间的距离 为描述符的数量，并且头尾随着网卡不断使用描述符而变化。因此本文将关于描述 符的寄存器设计为基地址寄存器(BASE),用来指示描述符在内存中的位置；描述 符头指针寄存器(HEAD),用来指示当前描述符的起始位置；描述符尾指针寄存器 (TAIL),用来指示当前描述符的结束位置；描述数量寄存器(SIZE),用来指示网卡一 次需要读取的描述符数量；描述符使能寄存器(CTRL),用来使能该描述符队列。其 中描述符头指针寄存器由网卡进行写入，其他寄存器由驱动部分完成写入，如图3- 11所示。

图3-11环形描述符队列

31

具体接收数据过程如下：
1. 驱动首先在内存中分配描述符缓冲区，并创建好相应的描述符；
2. 驱动写BASE基地址寄存器，通知网卡描述符在内存中的基地址；
3. 驱动写TAIL寄存器，写入0x04,表示有4个描述符需要网卡进行读取；
4. 网卡根据BASE基地址寄存器信息，向主机发起DMA读描述符操作，读回 4个描述符；
5. 根据描述符中的具体内容将接收数据写入到内存中，每用掉一个描述符，网 卡会写入HEAD寄存器进行移位，如图3-12所示；
6. 当SIZE小于某个数值时，驱动会再次对TAIL寄存器进行写入，增加新的 描述符，以保证网卡中有足够的描述符进行使用。

图3-12队列工作示意图
采用DMA描述符多队列技术可以在硬件中做到数据分流处理，从而可以支持 不同的应用。在本文多队列特指描述符的多队列，即在网卡中有多条接收描述符队 列，发送描述符队列不需要进行分流处理，单队列可满足需求，如图3-13所示。当 数据包经过MAC地址卸载加速后，可以根据MAC地址决定使用不同的接收队列， 接收队列的数量受硬件资源限制，每个接收队列的长度也与应用需求有关。
32


图3-13描述符多队列示意图
本模块基于FPGA中的FIFO模块进行设计，重点研究描述符队列的数量以及 长度。如果不受计算机资源限制，描述符队列的数量和长度的取值尽可能的大。但由 于计算机内存每页大小为4KB,为了减少上下文切换并且提高数据包的命中率，理 论上一个描述符指向一页，即128bit的描述符指向缓冲区为4KB的内存空间。32GB 的内存空间理论上可创建8192个描述符。
不考虑其他应用所占内存，假设共有8192个描述符，其中4096个描述符用于 接收。在硬件中需要4096X128bit=64MB的存储器空间。硬件中RAM存储器以 18Kbit为一块进行分布，即在一块18Kbit的RAM里最多可以存放140个描述符 (18K 十 128^140)。
对于4096个描述符来说，共需要30块RAM ( 4096-140 « 29.2 ),为了使得每 块RAM中的接收描述符数量一致,将每块RAM的接收描述数量设定为128。此时， 正好需要32块RAM (4096十128 = 32),即接收描述符队列的数量为32个，每个接 受描述符队列长度为128,这种描述符队列的平均分配如图3-14所示。

对于不同应用需要做到数据流的分类处理，即在网卡中提供这种分类处理功能, 本文通过接收描述符队列进行支持。对于不同类型的应用所需内存缓冲区以及描述 符的数量不同，比如在线视频数据这类信息，需要细粒度更小的缓冲区空间，更多的 描述符数量；而电子邮件数据这类信息，需要细粒度更大的缓存区空间，较少的描述 符数量。描述符队列的平均分配无法满足不同应用的需求，因此本文设计三种针对 不同类型应用的接收队列：第一种，在线视频类应用，这种应用实时性要求高，缓冲 区小；第二种，音频类应用，这种应用实时性要求不高，缓冲区空间要求不高；第三 种，下载类应用，这种应用实时性要求很低，但需要很大的缓冲区空间。目前以太网 上的最大报文大小不超过1500bytes,而又因为计算机内存每页大小为4K,为了减少 上下文切换并且提高数据包的命中率，理论上不应该将数据包放到两页去储存，即 缓冲区大小最大不超过4K字节。
在本文中设计三种不同大小的缓冲区空间，分别是64bytes, 512bytes, 2048bytes, 与三个不同的接收描述符队列进行绑定。当数据包小于等于64bytes时，使用接收描 述符队列x,对应着第一种在线视频类应用；当数据包大于64bytes但小于等于 512bytes时，使用接收描述符队列y,对应着第二种音频类应用；当数据包大小超过 512bytes时，使用接收描述符队列z,对应着第三种下载类应用。
对于三个接收描述符队列总共需要384个接收描述符(3x128 = 384),但根据 Internet流量特征指导［旳，数据包大小在0到64bytes的约占50%；数据包大小在 64bytes到512bytes的约占30%；数据包大小在512bytes到4096bytes约占20%,即 第一种应用按照比例分布需要192个接收描述符(384x50% = 192)；第二种应用需要 115个接收描述符(384x30% = 115 )；第三种应用需要76个接收描述符 (384x20%®76),如图 3-15 所示。

对于描述符数量平均分配来说，总共需要328KB内存空间，如下式(3-5)
128x64B + 128x512B + 128x2048B
1024
对于基于应用需求的分配方式，总共需要221.5KB内存空间，如下式(3-6)

192x64^ + 115x512B +76x20483
1024
根据应用需求对接收描述符队列的长度进行分配，可以有效的利用描述符的数 量，并且在这种分配方法下，会占用更少的内存缓冲区空间。
3.4本章小节
本章首先对PCS模块、MAC地址卸载加速模块以及PCIE DMA模块进行分析 设计，提出各个模块设计框图。并在设计分析过程中提出了各模块的重点难点，针对 这些重点难点进行设计，从而满足高速智能网卡的功能需求。
35
第四章高速智能网卡关键模块具体设计实现
在上一章提出了各个关键模块的设计架构，本章根据各个关键模块的设计架构 对PCS模块、MAC地址卸载加速模块以及PCIE DMA模块进行具体设计。
4.1 PCS模块设计与实现
根据第三章提出的PCS模块设计框图，对内部模块进行具体设计，其中分为两 部分：TX通道和RX通道，下面对这两个通道路径上的模块进行详细设计。
4.1.1 TX 通道



WIS service interface or PMA service interface
图4-1 TX通道发送数据
如图4-1所示[321,从MAC层发送过来的数据经过XGMII接口变为64bits的数 据(XGMn_TXD[63:0J)和 8bits 的控制信号(XGMII_TXC[7:0])发送到 64b/66b 编码 器。其中控制信号的每lbit对应8bits的数据。当控制信号为0时，表示8bits为用 户数据，即当XGMII_TXC[0]=0时,XGMII_TXD[7:0]为用户数据。控制信号为1时， 表示8bits为控制字符。经过编码后将66bit的数据进行扰码以保证直流平衡，然后 将数据发送到TX变速箱进行位宽转换，最后发送到PMA层。
(1) 64b/66b 编码器
首先，64b/66b编码的基本元素由2位同步头和64位数据组成。一个块被指定
36
为2位同步，后跟64位数据。如图4-2所示。
Block
2bit同步头
64bit数据
图4-2 64b/66b编码块
同步头可以分为三类：控制、数据和无效。根据sync报头的值，后面的64位数 据将被相应地解释。同步头为10时，数据被视为控制块，同步头为01时，数据被 视为用户数据。00或11个同步头无效。
有效的同步头为10或01的原因是64b/66b编码需要保证最大游程长度低于某 个值，从而允许时钟恢复电路正常工作El。游程长度描述为连续1或0的数目。考 虑64位数据要么全部为1,要么全部为0的情况。如果允许00或11为同步头，则 66位可能全部为1或全部为0,则游程长度超过了 66位。将有效的同步头限制为01 或10可以保证以66位的间隔进行转换，即使数据都是1或0,仍然可以保证最大 的游程长度。除此之外，01或10同步头还用于接收端的同步。
本文采用查找表法完成64b/66b编码器的功能。64b/66b编码器根据8bits的控 制信号进行判断，当其为0时，64bits使用D字符表示用户数据；当其不全为0时， 64bits使用C、S、T、O等控制字符表示。其中S表示数据帧的开始，O表示有序 集。由于64bit数据由两个32bit数据组合而成，S和O只会出现在64bit的第一字节 和第五个字节。T表示数据帧的结束，可以出现在64bit的任意字节。如附图1所示。 此时根据表查找64bit的具体组合方式并添加上指定的Block Type Field,随后在添 上10的同步头。之后再按照附图2进行控制字符的编码。
在对64b/66b编码器进行设计时，由于PCS模块的两端时钟频率一致，而在 64b/66b编码器中需要加入2bit,显然两端的速率不能匹配。需要每经过32个时钟 周期后停顿一次编码操作，即再经过32次加入2bit数据后，停顿发送一次64bit, 正好弥补了之前多发送的32次的2bit数据，这样便可以保证速率一致。
(2) 扰码器
64b/66b编码方式相比于8b/10b编码方式提高了编码效率，但缺点是不能保证 直流平衡，即不能避免1和0连续的出现。为了保证直流平衡，采用对64b/66b编码 后的数据进行扰码。扰码器中普遍采用如下多项式进行扰码。
G(x) = 1 + x39 + x58	(4-1)
对应的编码电路如图4-3所示［32］。
37





图4-3扰码电路
由上图可知，扰码电路由58个D触发器(SO〜S57表示)和两个异或门组成。 每个D触发器输出的结果用X1-X58表示。具体工作原理是：在某一状态下数据进 入，将S38和S57输出的结果X39和X58进行异或，将结果X0与输入的数据继续 进行异或作为输出结果。基于以上原理，通过VHDL硬件描述语言完成RTL级的代 码设计。
(3) TX变速箱
64bit的数据经过扰码后与2bit的同步头组合进入TX变速箱做位宽转换。由于 PMA层的位宽为64bit,扰码后输出位宽为66bit, TX变速箱需要做速率匹配，即在 同一时钟频率下将66bit位宽转换成64bit位宽发送到PMA层。本文釆用最小公倍 数法则：66X32-64X33=2112,即TX变速箱通过一个2112bit位宽的先入先出队列 (First Input First Output, FIFO)将收到66bit数据进行缓存，每收32个正好可发33 个64bit数据。如图4-4所示。

图4-4 TX变速箱位宽转换图

38
4.1.2 RX 通道



图4-5 RX通道接收数据
如图4-5所示a】，PMA层将64bit的数据发送到PCS模块，数据首先进入RX 变速箱进行位宽转换，RX变速箱结构不同于TX变速箱，RX变速箱为192bit位宽 的FIFO,将数据进行缓存然后以66bit位宽发送给Sync_Block模块。Sync_Block模 块负责锁定同步头，通过第三章设计的滑动窗口机制进行同步。在未同步之前，只传 送IDLE字符，且Sync_Block不会将这些数据发送到解扰器。完成同步后，数据进 入到解扰器进行解扰。解扰结束后，将66bits的数据发送到64b/66b解码器。64b/66b 解码器对同步头进行判断，按照附图1和2进行解码，最后发送到MAC层。
(1) RX变速箱
在接收过程中，需要对64bit的数据转换成66bit,需要进行位宽转换，RX变速 箱用以实现这个功能。RX变速箱不同于TX变速箱，由于Index在标识同步头过程 中，可最多指向第65位，从第65位向后截取66bit的数据需要13Obit位宽，在进行 滑动发送的时候，由于每次进64bit,但输出为66bit,每次输出都需要多发2bit,即 在发送第1个64bit时，需要占用第二个64bit的前2bit数据；在发送第2个64bit 时，需要占用第三个64bit的前4bit数据。当发送到第32个64bit时，又回到了发送 第一个64bit的位置，而这时需要向后取66bit数据，加上之前发送的64bit数据，至 少需要130bit的缓存大小，而又因数据以64bit为单位进入，因此向上取整，将RX 变速箱的缓存大小设置为64X3即192bit,如图4-6所示。并且其有一个用来标识同 步头的指针Index»主要功能是接收满192bit的数据后以66bit位宽将数据发给
39

Sync_Block模块，同时会收到Sync Block模块的反馈信息来将Index进行移位操作。 其具体工作原理同Sync Block在下面进行分析。
Index
V	192 	►
Block 2	Block 1	Block 0	”
I			n-
I
| 6 60 66 66 0 1
图4-6 RX变速箱内部结构
(2) Sync_Block 同步模块
Sync_Block模块用来确定数据里同步头的位置，由于66bit中的数据只有2bit的 同步头，需要接收方通过同步模块进行同步，本文结合实际情况将此模块原理设计 如下：
未同步之前，发送端会持续发送IDLE字符用于同步，即发送方会持续发送 2112bit (64X33)的数据块，在这个2112bit的数据块中，每个66bit的同步头出现 的位置相同。通过RX变速箱以192bit的FIFO对其进行缓存，并且按66bit的数据 发送到本模块。
Sync_Block模块会持续接收32个66bit数据，即收满2112bit数据。令收到第一 个的66bit为Block 0,并且假定第一位和第二位是同步头所在位置，如果第一、二 位是01或10,则将数据记录下来，并继续对Blockl做判断，直到判断完两个2112bit

步头的位置，如图4-7所示。

如果在header_count未达到64的时候，同步头出现了 11或00,则停止判断， 但仍要收满32个Blocko收满32个Block后，开始从Blockl的第一、二位进行同 步头的判断，counter+1 o当counter=32时，即滑动判断回到了 Block。，此时将通知 RX变速箱将Index后移一位，即从BlockO的第二、三位开始判断，并继续重复上述 操作，如图4-8所示。
40


图4-8同步头锁定失败流程
在最坏的情况下，即同步头在Block31的最后两位时，需要进行Index=65次判 断，也就是对2112bit的数据进行33X65次判断，如图4-9。其中所花费的时间为 2112X65X32+161130000827.2ms,通过硬件来做最多只需要27.2ms即可完成同



图4-9最长时间完成同步头锁定流程
(3) 解扰器
在Sync_Block模块锁定好同步头的位置后，接收和发送端结束同步阶段。此时， Sync_Block模块会将66bit数据发送到解扰器进行解扰。解扰器是扰码器的逆运算, 原理同扰码器一致。同样采用如公式4-1的多项式。对应的编码电路如图4-10所示 [32]
同样采用VHDL硬件描述语言进行RTL级的代码设计。

图4-10解扰电路
(4) 64b/66b 解码器
64b/66b解码是64b/66b编码的逆操作，采用查找表的方式完成解码。具体操作 为：首先对2bits的同步头进行判断，如果同步头为01,则表示64bits的数据为用户
41
数据，将8bits的控制信号全部译码为0,并和64bits的用户数据一起发送到MAC 层。如果同步头为10,则表示64bits中包含用户数据和控制字符，将8bits的控制信 号全部译码为1,并且需要按照附图1确定66bit数据中Block Type Field的种类进 行解码，并且按照附图2的控制字符对应表进行解码转换，从而得到64bits的数据， 与8bits的控制信号一起发送到MAC层。
4.2 MAC地址卸载加速模块设计
本部分将对协议栈对于数据包头的匹配查找和转发功能进行卸载，这一功能放 在MAC层中进行设计。在与基于软件的查找方式做比较后，选择了基于硬件的CAM 存储器技术。以其灵活高效查找匹配方式可减少网卡的处理时延。本小节将基于此 技术进行设计。
（1）解析模块
解析模块主要负责对数据包进行解析，并且进行关键域的提取。然后将关键域 发送到匹配查找模块。对于MAC帧，如图4-11所示，此模块将通过移位寄存器从 帧头向后移位8bytes进行MAC地址的获取（前导域为8bytes）。假设MAC目的地 址=AA:BB:CC:DD:EE:FFO

> MAC目的地址 	A

图4-11提取数据包头关键域
（2）匹配查找模块
匹配查找模块为MAC地址卸载加速模块中的核心，在匹配查找模块中基于 CAM存储器进行具体设计，具体设计如图4-12所示。当8bits数据进入CAM1中， 若此8bits为AA （170）,则将170地址中的32bits值读出，之后再经过32-5编码, 若输出00001,则表示成功匹配，如图4-13 （左）所示；当8bits为任意其他数，则 将此数地址中的32bits值读出并进行编码，由于并未配置此数的规则，其地址的值 为0,如图4-13 （右）所示。将6个CAM读出的值进行与操作，最终的5bits数不 为0,则代表此MAC目的地址匹配成功；如果为0,则表示匹配失败。最后，将5bits 的信息作为地址输出到动作模块。
42


图4-12 MAC目的地址匹配查找设计框图




（3）配置模块
在预处理阶段，驱动会通过此模块向CAM写入匹配查找规则，CAM初始全为 0o此模块主要包括如图4-12虚线部分。首先驱动会发送32bits的Index,这里的 Index指向CAM空间一段长度为256bits的块，每块代表一条流表项，但不需要全部 写满，根据具体需要匹配的字段进行写入。对于32bits位宽的CAM而言，最多可添 加32条流表项。随后，将具体规则写入，每次写入32bits。对于AA来说，换算成 十进制为170,将CAM1地址为170的寄存器次低位置1,如图4-14所示。BB将 CAM2地址为187的寄存器次低位置1,以此类推。
43


Index
0
1
0

0
0
0







0

1
0




0



170
255
图4-14写入匹配规则
1 0
31
（4）动作模块
此模块同样基于CAM存储器进行实现。将收到的5bits的进行译码操作，同样 将CAM的宽度设计为32bits,深度也为32bitso本文设计了两种动作，一种是转发， 01表示丢弃。其中转发是转发到不同 可1, 01表示转发到队列2, 10表示转
31
0
1
嫂发到队列2
1 0
31
图4-15转发
（左）丢弃（右）
44








































4.3 PCIE DMA模块设计与实现
在第三章分析到，本部分基于描述符的DMA方式进行设计实现，主要对PCIE 事务层以及DMA控制器进行逻辑设计，其中重点对DMA控制器进行设计，包括对 描述符相关的寄存器以及DMA读写模块的设计。
4.3.1 PCIE事务层逻辑设计
(1) PCIE IP核设计
本文采用Xilinx公司提供的7系列FPGA解决方案R9],如附图3所示，用于配 置PCIE的7系列FPGA集成模块。该集成模块遵循PCIE基础规范，包括物理层、 数据链路层和事务层，通过使用TLP包在各层之间交换信息。
IP核需要通过Vivado下的IP核例化工具Customize IP进行例化设计。如图4- 16所示，在①中设置成PCIE端点设备，因为网卡作为端点设备Endpoint接入设备 与主机中根复合体Root Complex相连，选择对应的实际开发板KC705 REVA；在② 中设置成8个通道，以及速率设置成5.0GT/S,因为此IP核最高支持5.0GT/S速率， 且在8个通道数才可实现此速率。而实际带宽在8个通道下可达到4GB/S,满足万 兆以太网中线速率的要求；在③中将AXI-Stream接口的时钟频率设置为250MHz, 数据位宽设置为128bit；在④中将参考时钟设置为lOOMHzo

图4-16 PCIE IP核配置图
在图4-17中主要对PCIE的BAR空间进行配置。在①中配置PCIE端点设备的 BAR0空间，Type类型为Memory,寻址位宽为32bit,大小为1MB；在②中配置PCIE 端点设备的BAR1空间，与BAR0空间的配置参数一致。在本文中，BAR0负责与 DMA相关的寄存器，如DMA描述符基地址，DMA描述符的头尾指针等。BAR1空 间用以存放控制FPGA其他相关寄存器地址信息。
45


图4-17 PCIE IP核BAR空间配置界面
(2) PCIE发送引擎设计
PCIE发送引擎的主要功能是发送TLP包，根据PCIE总线事务对于TLP包的 定义，其主要发送三种类型的TLP：存储器读、存储器写以及完成数据包。本文按 照TLP包的类型设计三种状态：SIART_READ和 WRITE_READ_TLP状态， S1ART_WRITE、WRITE_DAIA 和 WRITE_BD 状态,START_CMPL 和 WRITE_CMPL_TLP状态，在这三种状态下分别完成存储器读、存储器写以及CMPL 包的发送处理。
处理工作主要包括将DMA控制器要发送的数据添加头尾组成TLP格式的包进 行发送，通过PCIE发送引擎对这三种类型的TLP发送。其状态所表述的具体定义 在表4-1中给出了具体的描述信息。其状态转换设计如图4-18所示。
表4-1 PCIE发送引擎模块状态机描述
状态名
状态定义
TX IDLE
空闲状态，请求完成和等待请求
SIART READ
使能存储器读请求
WRITE READ TLP
开始发送读请求
START WRITE
使能存储器写请求
WRITE DAIA
开始发送需要写入的数据
WRITE BD
回写描述符，通知驱动
START CMPL
使能CMPL包发送请求
WRITE CMPL TLP
开始发送CMPL包
46

如上图所示，PCIE发送引擎状态机分为三种状态READ、WRITE以及CMPL。 不同的状态对应不同的请求，下面做详细的介绍。
TX_CMPL状态表示发送CMPL包。当PCIE控制器收到来自己驱动发出的存储 器读请求后，PCIE控制器会向PCIE发送引擎发出CMPL使能信号，PCIE发送引 擎在之后的时间里接收到从PCIE控制器模块传输过来的数据，将数据组装成TLP 包发送给RCo
TX_READ状态用于发送读请求的TLP包，DMA读操作将开启此状态。在驱动 完成对DMA控制器中DMA寄存器的配置后，DMA寄存器向PCIE发送引擎发出 读请求使能信号，之后PCIE发送引擎开始组装并且发送。
TX_WRITE状态用于发送写请求的TLP包，DMA写操作将开启此状态。DMA 控制器会向PCIE发送引擎模块发送写请求使能信号，先将网络上传送过来的数据 进行发送，然后回写描述符。
(3) PCIE接收引擎设计
PCIE接收引擎模块的主要功能是接收TLP包，同样根据接收的TLP包类型将 状态设计为:RX_READ 和 RX_RD_WAIT 状态，RX_WRITE> RX_WR_WAIT 状态， RX_CMPL_DATA状态，在这三种状态下分别完成存储器读、存储器写以及CMPL 包的接收处理。其状态所表述的具体定义在表4-2中给出了具体的描述信息。其状态 转换设计如图4-19所示。
47

表4-2 PCIE接收引擎模块状态机描述
状态名
状态定义
RXIDLE
空闲状态，请求完成和等待请求
RXREAD
存储器读请求，读取PCIE控制器或DMA控制器中的相关寄 存器信息
RX_WRITE
存储器写请求，对PCIE控制器或DMA控制器的相关寄存器 进行写入
RX_WR_WAIT
若发起DMA读写操作的写请求，则通知DMA控制器准备 进行读写操作
RXRDWAIT
等待发送CMPL包
RXCMPLDATA
CMPL包，有两种情况：收发BD和用户数据


图4-19 PCIE接收引擎模块状态转换图
RX_READ状态表示接收来自驱动对PCIE控制器或DMA控制器中相关寄存器 的读请求。读请求发送到PCIE控制器或DMA控制器，然后PCIE控制器或DMA 控制器通知PCIE发送引擎模块准备发送CMPL包，并返回状态机RX_IDLE状态。
RX_WRITE状态表示驱动对PCIE控制器或DMA控制器中相关寄存器的写请 求。驱动在初始化DMA控制器时，通过此操作完成初始化工作，包括对相关寄存器 进行配置并使能DMA读写操作，若发起DMA读写操作的写请求，则通知DMA控 制器准备进行写操作。
RX_CMPL状态表示用来接收主机端发送到网卡的数据。主要会收到带有收发 BD和用户数据的TLP包，当主机端驱动准备好了在内存中的数据，包括收发BD信 息和用户数据。PCIE发送引擎发送完读请求后，PCIE接收引擎模块会收到此类信
48

息。
(4) PCIE控制器设计
PCIE控制器模块主要用于对PCIE接收引擎模块收到的TLP包做进一步处理。 当收到读请求信息后，将相关寄存器的值通过PCIE发送引擎进行发送以及当收到 写请求后，对相关寄存器进行写入。PCIE控制器模块的状态机描述信息如表4-3所 示。PCIE控制器模块的状态机设计如图4-20所示。PCIE控制器的状态机详细介绍 如下：
H2F_WR表示在进行写请求时所进入的状态。如果为存储器写请求，进入到该 状态并根据指定的寄存器地址信息找到相应寄存器进行写入。
F2H_RD状态表示完成读请求。如果为读请求，进入到该状态并根据指定的寄 存器地址信息找到相关寄存器的配置信息，同时通知PCIE发送引擎准备发送带数 据的CMPL包。在随后的时间将所要读取的寄存器信息发送至PCIE发送引擎模块 组成TLP包进行发送。
RECV状态表示监控PCIE接收引擎接收到的信息，PCIE接收引擎模块将收到 的信息发送至PCIE控制器模块，PCIE控制器模块进入此状态。PCIE控制器模块对 收到的信息进行判断，当收到的数据为写请求时，进入H2F_WR状态；当收到的数 据为读请求时，进入F2H_RD状态。
PCIE控制器工作流程如下：
1. 在RECV状态持续收到PCIE接收引擎模块传送过来的数据，并根据Fmt 和Type进行事务类型的判断；
2. 当判断为读请求时，进入到F2H状态。此时，PCIE控制器模块根据寄存器 地址信息寻址到指定的寄存器读取相应的数据并使能PCIE发送引擎模块 进行组包，完成CMPL包的发送。
3. 当判断为写请求时，进入到H2F状态。则按照具体寻址信息找到指定的寄 存器完成写入。
4. 当完成请求时，则进入RECV状态继续等待数据的到来。
表4-3 PCIE控制器模块状态机描述
状态名
状态定义
H2F IDLE
初始状态，初始化配置信息
H2F RECV
根据PCIE接收引擎收到的信息进行判断
H2F WR
当TLP包为写请求时进入此状态，根据具体地址信息完成写请求
F2H_RD
当TLP包为读请求时进入此状态，根据具体地址信息完成读请求 工作，并通知PCIE发送引擎发送CMPL包
49


4.3.2 DMA控制器逻辑设计
本文所设计的DMA控制器结构设计方案如图4-21所示，从图中可以观察到， DMA控制器包括DMA读写操作模块、数据缓存模块、DMA寄存器模块以及中断 控制模块。其中，DMA读写操作模块主要负责对DMA读写操作的响应发起以及读 写操作的时序控制；数据缓存模块主要负责缓存从主机传送过来要发送到网络上的 数据以及从网络进入要发送到主机的数据，其中也对收发BD进行缓存；DMA寄存 器模块主要包括内存中地址的寄存器以及与收发BD相关的寄存器，这些寄存器都 对DMA控制器的功能、状态以及控制相关信息起着重要作用。各个模块的功能设计

(1) DMA读写操作逻辑设计
DMA读写操作模块主要负责响应来自DMA寄存器发起的DMA读写操作请求, 并且与前端的PCIE事务层进行交互，保障数据流稳定传输，同时对收发BD进行解 析以完成数据的写入和读取。DMA读写操作模块不同于PCIE发送引擎和PCIE接 收引擎模块。DMA读写操作模块状态机如图4-22所示。


图4-22 DMA读写操作模块状态转移图
首先对DMA读操作模块进行设计。在进行DMA读开始前，驱动需要配置DMA 寄存器相关寄存器的值，即PCIE接收引擎模块收到的DMA读请求信号并按照信号 内容进行配置完成寄存器的初始化操作，从而产生DMA读使能信号，DMA读操作 模块随即进入到DMA读操作状态。在完成上述配置等初始化工作后，正式进入到读 操作开始状态。下面以主机发送数据为例阐述DMA读操作流程：
1. 驱动在内存锁存一段物理空间地址，并创建好发送BDo
2. 驱动写DMA寄存器相应的寄存器，包括地址寄存器(高、低位)和控制DMA 读操作的寄存器。
3. DMA寄存器通知DMA读操作模块发起DMA读操作，根据第二步写入的信息 向内存中读取发送BDo
4. DMA读操作模块将读取回来的发送BD进行解析，根据发送BD的信息内容再 一次发起DMA读操作，向内存中读取用户数据。
5. 将用户数据读取回来放到数据缓存模块进行缓存发送。
6. 通知驱动本次发送数据完毕。
DMA写操作的工作流程与DMA读操作的状态机过程大致一样，但DMA写操 作并非由驱动配置寄存器发起，而是接收到数据后主动发起。DMA写操作在收到写
51 操作使能信号后进入到DMA写状态，对接收BD信息进行解析，获取DMA写操作 的相关地址信息，然后进入到写操作开始状态。下面将以主机接收数据为例阐述 DMA写操作流程：
1. 驱动在内存锁存一段物理空间地址，并创建好接收BD =
2. 驱动写DMA寄存器相应的寄存器，包括地址寄存器和控制DMA写操作的寄存 器。
3. DMA寄存器通知DMA读操作模块发起DMA读操作，根据第二步写入的地址信 息向内存中读取接收BD =
4. 将读取回来的接收BD放入数据缓存模块进行缓存，随后DMA写操作模块开始 解析接收BD,并根据接收BD的信息内容发起DMA写操作，向内存中写入数据。
5. 通知驱动本次接收数据完毕。
在进行DMA读写操作时，需要以PCIEIP核进行与主机的交互，本文通过使用 7系列的IP核，并选择PCIEGen2X8通道模式，即在理论上可以提供4GB/s的带宽 速率，而万兆以太网lOGbps转换单位为：
些迴"25GB/S	(4-2)
8	丿
可以看出
4GB/s>1.25GB/s	(4-3)
因此，本文在主机侧的硬件性能是可以满足网络侧lOGbps的吞吐量。
同时以上述接收过程为例，从流程1-7共需要4次存储器写请求，其中包括写 低位地址寄存器，写高位地址寄存器、写使能接收描述符队列、写TAIL寄存器(具 体寄存器定义在下一小节进行详细介绍)，设每次存储器写请求所需时间为8(恋)，一 次传输量为, Windows处理一次DMA同时时间约为20%S ,则进行一次DMA 传输操作理论速度为：
N
V =
丄J + (20 + 0.032)”s	(4_4)
4GB/s
根据公式4-4,当N为1MB时，理论速率约为3703.26MB/S,足以满足lOGbps 的吞吐量。
(2) DMA寄存器逻辑设计
在DMA控制器的设计当中，DMA寄存器模块包含了 DMA控制器的相关控制 功能，比如发起DMA的读写操作，还包含了一些状态信息用来通知上层驱动，通过 不同的寄存器地址区分不同的执行功能。其中本文对BAR0进行设计。空间大小为 1MB, 32位寻址大小。下面详细介绍BAR0空间的寄存器设计。
本文的硬件寄存器地址设置为如下式(4-5)。
52
DMA _ REGISTER _ ADDR = BMO(Ox8()OFOOOO)	( 4-5 )
-\-ChannelAddr(24bit)
+Register A ddr(\ 6bit)
其中 BARO 为基址寄存器地址，ChannelAddr=PORT_OFFSET*QueueNumbe, ChannelAddr代表通道地址，代表不同的收发队列。PORT_OFFSET代表固定值 0x100000, QueueNumber代表队列号，可选0〜2,表示3个队列。PORT_OFFSET是 为了间隔每个队列，让每个队列的控制寄存器有足够的空间。RegisterAddr表示每个 具体的寄存器地址，具体寄存器地址如下表4-4,其中RX代表接收队列对应的BD 信息，TX代表发送队列对应的BD信息。
表4-4基址寄存器功能定义
RegisterAddr
偏移量
描述信息
DMA REG RX BASE LOW
0x0000
设置RX队列低32位
DMA REG RX BASE HIGH
0x0200
设置RX队列高32位
DMA REG RX BD SIZE
0x0400
设置RX队列描述符的大小
DMA REG RX BD HEAD
0x0600
设置RX队列中接收BD的Head指针
DMA REG RX BD TAIL
0x0800
设置RX队列中接收BD的Tail指针
DMA REG RX CTRL
OxOAOO
使能RX队列
DMA REG TX BASE LOW
0x1000
设置TX队列低32位
DMA REG TX BASE HIGH
0x1200
设置TX队列高32位
DMA REG TX BD SIZE
0x1400
设置TX队列描述符的大小
DMA REG TX BD HEAD
0x1600
设置TX队列中发送BD的Head指针
DMA REG TX BD TAIL
0x1800
设置TX队列中发送BD的Tail指针
DMA REG TX CTRL
OxlAOO
使能TX队列
下面举一个例子介绍硬件寄存器地址的计算过程，假设要找到2号队列发送BD 的Tail寄存器。BAR0基地址为0x800F0000 ,根据上述公式，2号队列的 ChannelAddr=0xl00000*2=0x200000 , DMA_REG_TX_BD_TAIL=Ox 1800 ,则 DMA_REG_TX_BD_TAIL 的地址=Ox800FOOOO+Ox300000+0x 1800=0x802F 1800, 0 号 队列的发送BD的Tail寄存器为0x800F1800o
(3) 中断控制逻辑设计
中断控制模块与DMA写操作模块以及PCIE IP核相连，当一次写操作完成后， 向PCIE IP核发出中断请求，而PCIE IP核通过MSI中断机制向主机端发出中断信 号。
53

中断控制模块的主要端口以及定义如表4-5所示。dma_write_done_l~3信号表 示3个队列完成写操作的中断信号；pcie_int_out_l〜3和pcie_int_rdy_l〜3信号连接 着PCIEIP核，对应着3个接收队列，每个队列有自己的中断信号和返回通知信号； int_free信号表示中断释放。
表4-5中断控制模块信号描述
信号名称
位宽/bit
方向
描述
dma write done 1 〜3
1
I
各队列DMA写操作完成信号
int free
1
I
中断释放信号
pcie int out 1 〜3
1
0
向PCIE IP核发出中断请求信号
pcie_int_rdy_ 1 〜3
1
I
为1表示PCIEIP核成功发送MSI中断 信号
中断控制模块的状态机如图4-23所示，其中包括4个状态:
INT_IDLE闲置状态：该状态为初始状态。状态机在该状态下等待DMA写操作 的传输完成，当有队列的DMA传输工作完成后，便进入下一状态INT_SEND状态。
INT_SEND中断请求发送状态：该状态为中断信息发送状态。状态加在该状态 下向PCIE IP核发出中断请求，发送成功后进入下一状态INT_WAITO
INT_WAIT中断请求发送完成状态：该状态为中断请求发送完成后，等待PCIE IP核返回pcie_int_rdy_l〜3信号。当PCIE IP核成功发送MSI中断信息后，会以 pcie_int_rdy_l〜3为信号通知状态机进入下一状态INT_BUSY=
INT_BUSY中断忙碌状态：该状态表示主机端正在处理中断，状态机在此状态 下不会发出新的中断请求。当主机端完成响应后，将int_&ee信号置1,通知状态机 跳转到INTJDLE状态。

图4-23中断控制模块状态转移图

54
4.4本章小结
在本章中，围绕着上一章提出的网卡系统关键模块进行设计实现，在PCS模块 中对64b/66b编解码器、加解扰器、TX/RX变速箱以及Sync_Block模块进行设计实 现，其中重点对Sync_Block同步模块采用滑动机制进行了设计实现。在MAC地址 卸载加速模块中基于CAM存储器设计了解析模块、配置模块、匹配查找模块以及动 作模块，其中重点完成了配置模块以及匹配查找模块的相关设计。在PCIE DMA模 块基于PCIE总线完成了 PCIE逻辑事务层、DMA控制器的整体设计，其中重点完 成基于描述符的工作方式的设计实现。
55
第五章系统测试与验证
在完成对各部分的设计之后，本章将进行各部分的时序功能测试工作。其中对 PCS模块、MAC地址卸载加速模块以及PCIE DMA模块进行仿真测试。主要工作 包括搭建测试环境，测试环境包括测试激励、测试验证、被测部分，并且采用回环测 试法，构造了完整的测试路径。同时借助Vivado和Modelsim联合仿真完成各部分 的功能仿真，以此验证各部分的功能是否正确。
5.1仿真测试环境搭建


图5-1测试框架图
本系统的测试框架如图5-1所示。分为两部分，左侧部分是IP核部分。为了可以 构建出从网络线路侧到主机侧以及从主机侧到网络线路侧这一环路，通过Vivado工 具生成 了 10G Ethernet PCS/PMA (10GBAS&R/KR) IP核以及7 Series Integrated Block For PCI Express IP核，Vivado为用户提供了每个IP核的IP Example Design,可用于对 本IP核进行测试和仿真。利用此特性，通过生成10G Ethernet PCS/PMA (10GBASE- R/KR)的IP Example Design产生数据激励源以及进行数据检验工作。同时通过生成7 Series Integrated Block For PCI Express的IP Example Design将其作为根复合体可与End Point进行PCIE协商。
右侧为被测部分，包含了 PCS模块部分，MAC地址卸载加速模块部分，以及基 于PCIE的DMA部分。其中为了测试PCIE DMA模块，需要与主机侧的根复合体进 行PCIE协商。具体做法是将数据通过路径转换模块进行转发，在回环路径上形成一 条指向主机侧的路径，从而将数据流向PCIE DMA以及根复合体。因此，本框架分
56
为两条通路，一条从网络侧到网络侧的回环路径：由网络侧发送激流源，经过PCS 接收、MAC接收(卸载加速)、路径转换、MAC发送、PCS发送，再回到网络侧， 这一过程可测试PCS模块以及MAC地址卸载加速模块的功能。另一条从网络侧到 主机侧的路径：由网络侧发送激励源，经过PCS接收、MAC接收(卸载加速)、路 径转换、PCIEDMA模块，再到达主机侧中的根复合体，这一过程可测试PCIEDMA 模块的功能。通过这两条路径可完成所有部分的功能测试。按照以上框架搭建电路， 将完整设计导入Modelsim仿真工具。
5.2 PCS模块功能仿真测试
PCS模块包括TX通道和RX通道。其中TX通道由64b/66b编码器、扰码器、 TX变速箱组成；RX通道由RX变速箱、Sync_Block同步模块、解扰器、64b/66b解 码器组成。PCS模块的主要功能是将由XGMII接口发送过来的数据进行编码、扰码 等处理，将66bit进行64bit转换后发送到PMA子层；同时将由PMA子层发送过来 的64bit数据进行66bit转换后进行同步，同步后再进行解扰、解码等处理，最后将 64bit数据发送到XGMII接口。
为了能够测试PCS整体模块的功能，采用仿真测试环境中的回环路径，即由10G Ethernet PCS/PMA (10GBASE-R/KR)的 IP Example Design 产生数据激励源，首先经 过PCS模块的RX通道，再由TX通道发送出去，在这一过程对PCS模块进行仿真， 可验证其功能的正确性，其中主要对Sync_Block同步模块和RX变速箱以及PCS模 块整体进行仿真，其他模块的仿真放在PCS模块整体中进行仿真。
(1)Sync_Block同步模块仿真测试
对于Sync_Block同步模块以及RX变速箱的仿真测试主要关注在设计中的两个 滑动指针：RX变速箱中的Index以及Sync_Block同步模块中的header_count。在设 计中，同步头的锁定标志为header_count>=64,即block_sync信号置1。
如图5-2所示，高亮显示的gtx_Index信号和Sync_counter信号表示第四章设计 的RX变速箱中的Index和Sync_Block同步模块中的counter□在第四章分析过， Index在Sync_Block同步模块接收了 32次的66X32bits数据后通知RX变速箱向后 移动一位进行发送。图中的Sync_counter信号显示为325h20,换算成十进制为32, 表示其已经接收了 33次的66X32bits数据。因此，gtx_Index信号向后移动一位，由 6匕04变为65h05=此时同步头还未锁定。
57



图5-2同步头锁定功能整体仿真图
当block_sync信号置1时，表示同步头锁定，如图5-3所示。此时 sync_header_count 信号为 85h41,即 header_count 为 8，h41,换算成十进制为 65,表 示已经连续收到超过64个Block,并且同步头全为01或10。以上结果表明RX变 速箱和Sync_Block同步模块的相关功能正确。

图5-3同步头锁定成功
(2)整体功能仿真测试
在整体测试PCS模块功能中，对RX通道以及TX通道进行仿真测试，包括 64b/66b编解码、扰解码以及变速箱功能。
a.RX通道
RX通道将激励源接收进行位宽转换、解扰以及解码处理后，发送到XGMII接
58 口中。如图5-4所示，高亮显示的pma_data_i信号表示由PMA子层发送过来的64bit 数据；gearbox_data信号表示经过RX变速箱的位宽转换后变为66bit的数据； descram_result信号表示经过解扰后的数据，此信号表明解扰器功能正确； pcs_data_out信号表示RX通道输出到XGMII接口的信号，是经过64b/66b解码后 输出的信号，在未同步之前，激励源发送IDLE字符，其已经经过XGMII接口转码 变成0x07,根据附图2,其代表含义为IDLE字符，此信号表明64b/66b解码器将数 据正确解码，功能正确。上述结果表明RX通道各模块的相关功能正确。

图5-4 RX通道整体仿真

b.TX通道
由于是回环路径，TX通道将由RX通道接收到的数据进行发送，再次发送到 PMA子层，在此过程需要完成64b/66b编码、加扰以及位宽转换处理。
如图5-5所示，xgmii_data信号表示XGMII接口发到PCS模块的发送数据，其 已经经过10GBase-R协议转码变成0x00,同样根据附图2,其代表IDLE字符。因 此，从激励源发到PCS模块的数据，被PCS模块准确接收，并形成回环由MAC层 发回了 PCS模块；data_scramble信号表示经过加扰后的数据，此信号表明扰码器功 能正确；gearbox_data_o信号表示经过TX变速箱进行位宽转换后，66bit变为64bit 的数据，此信号表明TX变速箱功能正确；pma data o表示发送到PMA子层的数 据；在上述过程中，data_encode_en信号每经过32个高电平的时钟周期后拉低1个 时钟周期，表示停顿编码一次，此信号表明64b/66b编码器功能正确。上述结果表明


59
5.3 MAC地址卸载加速模块仿真测试
在MAC地址卸载加速模块中，主要包括解析模块、配置模块、匹配查找模块以 及动作模块，其功能是按照用户自定义的MAC地址将满足匹配条件的数据包进行 卸载，从而判断是转发还是丢弃，其中匹配查找模块是本部分的核心模块。在匹配查 找模块中，本文将48bits的MAC地址分成6段分别进行匹配，即一次匹配查找操作 需要在6块CAM存储器中进行，只有都匹配成功才说明MAC地址匹配成功，匹配 成功的等待执行动作，匹配失败的则被丢弃。
为了测试这一部分内容，同样采用回环路径，即由10G Ethernet PCS/PMA (10GBASE-R/KR)的IP Example Design产生数据激励源，经过PCS模块处理后进入 此模块，其中主要对匹配查找模块进行仿真测试。
(1) 整体功能仿真测试
如图5-6所示，nic_smac信号表示需要匹配的MAC地址，这部分由驱动通过配 置模块进行配置，在这里将需要匹配的MAC目的地址设置为4c:e0:00:00:00:00,此 信号表明配置模块功能正确；real_rxd信号表示实际收到的MAC地址，是由解析模 块将需要匹配的字段即MAC目的地址发送过来，此信号表明解析模块功能正确。在 本文中被分成了 6段分别进行匹配查找，只有addr_mat_0~5信号全部置为1,才代 表匹配成功；否则代表匹配失败；rx_begin信号表示开始进行匹配；addr_mat信号表 示匹配成功的状态，当addr_mat_0~5信号置1时，此信号持续置1； dropjpkt信号 表示匹配失败时执行丢弃动作，与addr_mat信号极性相反。上述结果表明，MAC地 址卸载加速各模块功能正确。

图5-6匹配查找模块整体仿真
(2) 匹配查找模块仿真测试
如图5-7所示，在real_rxd信号中，收到了一整帧，但只对MAC目的地址进行 匹配。其中①表示以太网的前导码D5-55,由物理层封装好；②表示收到的MAC目 的地址，显示顺序为倒序00, e0, 4c, 00, 00, 00；③表示每个CAM存储器进行匹
60
配后的结果，可以看出，addr_mat_O负责匹配的字段为00,只要有00进入，则匹配 成功，addr_mat_l负责匹配的字段为e0, addr_mat_2负责匹配的字段为4c,
addr_mat_3~5负责匹配的字段为00,这些都由驱动在匹配之前进行配置；④表示匹 配状态，可以看出，当有addr_mat_0~5信号置1时，此信号则置1,当全部 addr_mat_0〜5信号匹配成功时，addr_mat信号置1,表示匹配成功。



如图5-8所示，在real_rxd信号中表示收到了另外一帧，①表示前导码；②表示 此帧的MAC目的地址，倒序为00, aa, 3c, 33, ee, 9b,显然并非预期的MAC目 的地址;因此在③中，只有addr_mat_0> addr_mat_3~5对00匹配成功;④表示drop_pkt 在匹配失败后将此帧执行丢弃动作。上述结果表明，匹配查找模块以及动作模块功 能正确。

图5-8匹配失败执行丢弃动作

61

5.4 PCIE DMA模块仿真测试
本部分功能由于无法通过仿真波形显示，本文利用Modelsim在根复合体的TX 和RX端抓取TLP包，通过对TLP包的分析显示功能的正确性。通过编写测试程序 在根复合体（以下简称RC）端模拟出真实驱动与PCIEDMA模块的数据交互过程。 下面以网卡接收数据过程为例介绍完整的测试流程，如图5-9所示。

图5-9接收数据包测试流程图
为了测试此部分内容，选择第二条路径，即由1OG Ethernet PCS/PMA （10GB ASE- R/KR）的IP Example Design产生数据激励源，经过PCS接收、MAC接收（卸载加速）、 路径转换、PCIE DMA模块，再到达主机侧中的RC,这一路径可测试PCIE DMA模 块的整体功能。
62
(1) PCIE事务层仿真测试
在测试程序中RC首先对PCIE配置空间进行初始化操作，主要对MSI中断进 行配置。如图5-10所示，在左图中RC对网卡PCIE配置空间进行了一次配置写请 求，向0x010寄存器地址写入了 FF,随后又对0x010寄存器进行了一次配置读请求 来判断是否成功写入。在右图中可看到，由于第一次为配置写请求，网卡返回不带数 据的CMPL包，第二次为配置读请求，网卡将要读取的数据FF返回给了 RC。结果

图5-10存储器读写请求(左)和CMPL包(右)

(2) DMA寄存器仿真测试
在对PCIE进行初始化操作后，RC开始通过PCIE总线对DMA寄存器进行配 置，主要对4.3.2小节中所设计的寄存器进行配置。如图5-11所示，根据寄存器地址 0x800F0000可知RC对RX_BASE_LOW进行存储器写请求，在此过程中，完成了 对 RX_BASE_HIGH ( Ox800F0200 )、TX_BASE_LOW ( 0x800F1000 )以及 TX_BASE_HIGH (Ox800F1200)等寄存器的配置，同时使能RX接收队列，为接收 数据做准备。结果表明DMA寄存器功能正确。
63


图5-11 DMA寄存器相关配置
(3) DMA读操作仿真测试
经过RC对DMA寄存器配置后以及使能接收队列后，RC再次发起存储器写请 求，对RX_BD_TAIL (Ox800F0800)寄存器写入了 0x10,表示预先定义好的16个 接收BD在内存中等待读取，如图5-12所示。随后DMA控制器发起DMA读操作， 从64位内存地址OxOOOOOOOSBBBBOOOO中将16个接收BD进行读取，RC以带数据 的CMPL包的形式将接收BD发送到网卡，如图5-13所示(仅贴出了部分接收BD 信息)。结果表明DMA读操作功能正确。



图5-12写使能RX队列和RX_BD_TAIL寄存器
64

EP： e
Attrib吐g£： 0x0
Length: 0x040
Requester Id: 0X01A0
Tag: 0x00
Last and First Byte Enables: 0xFF
Address High: 0x00000008:
Address Low: 0XBBBB0000
图5-13 DMA读操作（左）和CMPL包（右）
（4）	DMA写操作仿真测试
DMA控制器将读取回来的接收BD进行缓存，并随即发起DMA写操作，将接
收到的网络数据根据接收BD中具体的地址信息写入到指定内存。如图5-14所示， 具体内存地址为0x0000000AAAAOOOOO»左图中OxFBCXGMII接口中的start信号） 为以太网帧的起始标志，OxFD （XGMII接口中的terminate信号）为以太网帧的结束 标志，和右图中后半部分网卡要写入内存的数据一致，其中前半部分中的16bytes数 据为内部控制信息，主要是包长度等信息。此过程由10G Ethernet PCS/PMA （10GBASE-R/KR）发出激励数据，经过PCS模块以及MAC层后通过DMA方式转 发到根复合体结果也表明了这一路径上的模块功能正确，即系统整体功能正确。
[200844*417 ns ] : Memory Write-64 Frame；
Traffic Class: 0x0
TD: 0
EP: 0
Attributes: 0x0
Length: 0x014
Requester Id: 0X01A0
Tag: 0x00
Last and First Byte Enables: QxFF
Address High: 0X0006000A
Address Low: 0XAAA60000
0x00 0X00	0x01	0x00	0x38	0x80	0x01	0x30	8x65	0x3E
0x00 0x00	0x06	0x00	0x00	0x60	0xFB	0x02	6x03	0x04
0X05 0X06	0x02	0x02	0x03	0x64	0x05	0X06	0x00	0x2E	|
0XAA 0X55	0X5S	0xAA	0x55	€xAA	0xAA	0xS5	0xAA	0x55	:
|	0x55 0XAA 0x5S 0xAA 8xAA 0x55 0xAA 6x55 0x55 0xAA[ |
0x55 0xAA	0xAA	0x55	0xAA	0x55	8x55	0xAA	8x55	0xAA
0xAA 0x55	0xAA	0x55	0x55	8xAA	8x55	0xAA	0xAA	0x55:
0xAA 0x55	0x5S	0xAA	0x55	0xFD	0x08	0x00	0x00	0x80
图5・14测试程序中的激励数据（左）和网卡收到的数据（右）
65
5.5本章小结
在本章主要基于自主搭建的测试环境对各个部分进行了功能测试，其中对PCS 模块主要测试了 Sync_Block同步模块以及整体功能的测试；对MAC地址卸载加速 模块主要测试了匹配查找模块；对基于PCIE的DMA模块主要测试了 DMA读写操 作功能。仿真结果表明各部分功能正确，符合设计预期目标。
66
第六章工作总结和展望
在网络数据极速增长的今天，数据中心以及一些金融领域对于高速智能网卡的 高吞吐量和低时延需求越来越高，但网卡先进技术仍然掌握在国外的芯片制造商手 中，国内缺乏相关的技术研究实现。基于以上背景，本文基于实验室千兆以太网项 目，对其关键部分进行改进和优化，设计成可支持万兆以太网的高智能网卡。其中本 文的主要工作完成了基于64b/66b编码的PCS模块的设计实现；在MAC层完成了 基于CAM存储器技术的数据包头匹配查找转发功能的设计；在PCIE接口部分完成 了基于描述符的DMA的设计实现工作。本文为国内关于高速智能网卡的研究和实 现提供了一些新的思路。在文章最后，对工作进行总结，并提出一些未来工作展望。
6.1论文工作总结
本文主要完成的工作如下：
第一，本文首先对目前产业界高速智能网卡根据不同架构和应用场景进行功能 和指标的分析和总结，基于以上提出本网卡的设计重点，即支持lOGbps的以太网速 率、支持协议栈部分功能卸载以及支持数据包的快速搬移，并对以上的关键技术理 论进行研究分析，得出本文网卡的主要功能、技术指标，并提出网卡总体设计框图， 然后对网卡中关键模块进行研究分析，其中关键模块包括PCS模块、MAC地址卸载 加速模块以及PCIEDMA模块。最后深入网卡系统，对关键模块整体进行设计分析， 并总结出各模块中的重点和难点，并针对重点难点对目前现有技术解决方案进行分 析比较，提出本文的设计思路和技术解决方案。
第二，根据确定好的技术解决方案。首先，对基于64b/66b编码的PCS模块进 行设计，其中包括64b/66b编解码器、加解扰器、TX/RX变速箱、Sync_Block模块， 其中难点在于接收时数据的同步头锁定，本文提出一种滑动机制，可通过硬件处理 能力强的特点高效完成同步，其中减少了同步头锁定的时间，并且容易实现；然后， 本文基于CAM存储器在MAC层中完成了 MAC地址的解析、匹配以及转发模块的 设计；最后，在PCIE总线接口部分，其主要负责与主机侧进行数据交互，为减少中 断CPU次数，采用了基于描述符的DMA工作方式将数据在内存与网卡之间进行快 速搬移。本部分主要基于PCIE IP核对事务层进行开发设计，其中完成了包括PCIE IP核配置、PCIE发送引擎、PCIE接收引擎以及PCIE控制器模块，然后对DMA控 制器进行总体设计，其中包括DMA读写操作、DMA寄存器以及中断控制模块。
第三，本文对各部分的功能进行仿真测试工作。其中对PCS模块、MAC地址卸 载加速模块以及PCIE DMA模块进行仿真测试。主要工作包括搭建测试平台，测试
67
平台包括测试激励、测试验证、被测部分。采用回环测试法，构造完整的测试路径。 并借助Vivado和Modelsim联合仿真完成各部分的功能仿真，仿真结果表明部分的 功能正确。
6.2未来工作展望
本文对于高速智能网卡的关键部分的设计实现了其基本功能，相比于产业界关 于智能网卡成熟的解决方案还存在不足之处，但可以看到的是，本文对于国内关于 智能网卡的研究实现具有一定的指导意义。在完成高速智能网卡的基本功能后，对 于PCS模块，随着网络带宽的升级，现已迎来1 OOGbE时代，随着编码方式的改变 其PCS模块的实现也发生了改变，本文在这一部分缺少可扩展性，之后应补充更高 效编码方式128b/130b,同时向后兼容8b/10b编码方式。对于MAC层，在其中设计 了一种MAC地址的匹配查找方式，但尚未进行仿真测试，之后会完善这部分的仿真 工作。在PCIE总线接口部分中，采用基于描述符的DMA方式完成了数据在网卡与 内存中的快速搬移，实现了微秒级的转发时延，但整体性能与最高性能指标还有一 定的差距，之后会对代码进行优化。在对仿真测试环境的搭建中，本文采用lOGBase- R的IP核作为数据激励源，PCIE核作为根复合体来验证数据，本文设计的网卡作为 中间被测部分，具体测试流程为：数据激励源产生数据，经过网卡，发送到根复合体 做验证，这种仿真测试环境缺少第三方的权威认证，在后期应补充对本文仿真测试 环境的认证。
68
参考文献
[1] Singh, J. Ong, A. Agarwal, G. Anderson, A. Armistead,R. Bannon, S. Boving, G. Desai, B. Felderman, P. Germano,A. Kanagala, J. Provost, J. Simmons, E. Tanda, J. Wanderer,U. Holzle, S. Stuart, and A. Vahdat. Jupiter rising: A decade of "Clos topologies and centralized control in Googles datacentemetwork. In ACM Conference on SIGCOMM, 2015.
[2] FPGA 技术在沪深行情加速 的应用[EB/OL].https://www.sohu.eom/a/318623270 _1000061005Year=2019.
[3] Li Xiaoyao, Wang Xiuxiu, Liu Fangming, et al. DHL: Enabling flexible software network functions with FPGA acceleration[C]//Proc of the 38th Int Conf on Distributed Computing Systems. Piscataway, NJ: IEEE： 2018.
[4] Ma Xiaoxiao, Lu Gang, Fu Binzhang, et al. Implementation method and performance analysis of non-contiguous data communication in network[J].Chinese Journal of Computers, 2020, 43(6): 1123-1138 (in Chinese).
[5] Li Bojie,Ruan Zhenyuan, Xiao Wencong, et al.KV-Direct:High-perfoiTnance inmemory key・value store with programmable NIC[C]//Proc of the 26th Symp on Operating Systems Principles. New York: ACM,2017:137-152.
[6] Tokusashi Y? Matsutani H? Zilberman N・ LaKe: The power of in-network computing [C] //Proc of the Int Conf on ReConFigurable Computing and FPGAs. Piscataway, NJ: IEEE? 2018.
[7] Tokusashi Y, Dang T, Pedone F, et al. The case for in-network computing on demand [C] //Proc of the 14th EuroSys Conf. New York: ACM, 2019.
[8] Kaufinann A , Peter S ? Sharma N K , et al. High Performance Packet Processing with FlexNIC[J]. Computer Architecture News, 2016, 44(2):67-81.
[9] 马潇潇，杨帆，王展，元国军,安学军•智能网卡综述[J/OL].计算机研究与发展: 20[2021-0330].http://kns.cnki.net/kcms/detail/11.1777.TP,20210225.1903.00&htail.
[10] 张登科，王兴伟，贾杰，李婕•智能网卡研究新进展[J/OL].小型微型计算机系统:1・ 8[2021-03-30].http://kns.cnki.net/kcms/detail/21.1106.tp.20210319.0906.004.html.
[11] Putnam A, Caulfield A, Chung E, et al. A reconfigurable fabric for accelerating large- scale datacenter services [C] //Proc of the 41st ACM/IEEE Int Symp on Computer Architecture. Piscataway, NJ: IEEE,2014: 13-24
[12] Daniel F, Andrew P9 Sambhrama M, et al. Azure accelerated networking:SmartNICs in the public cloud [C] //Proc of the 15th Symp on Networked Systems Design and
69
Implementation. Berkeley, CA: USENIX Association,2018:51 -66
[13] Caulfield A, Chung E, Putnam A, et aL A cloud-scale acceleration architecture [C]//Proc of the 49th Annual IEEE/ACM Int Symp on Microarcliitecture. Piscataway, NJ: IEEE, 2016
[14] Caulfield A, Costa P, Ghobadi M. Beyond SmartNICs: Towards a fully programmable cloud [C]//Proc of the 19th Int Conf on High Perfbmiance Switching and Routing. Piscataway, NJ: IEEE, 2018
[15] Miano S?Doriguzzi-Corin R,Risso F,et al. Introducing SmartNICs in Server-based Data Plane Processing: the DDoS Mitigation Use Case[J]. IEEE Access, 2019, PP(99):1-1.
[16] Mellanox. Mellanox BlueField series SmartNIC white paper [EB/OL].[2020-08-10]
[17] Netronome. Agilio series SmartNIC [EB/OL], [2020-08-10].
https://www.netronome.com/products/smartnic/overview/
[18] Broadcom. Stingray series SmartNIC [EB/OL]. [2020-08-10]. https://www.broadcom.com/products/ethemet-connectivity/smartnic
[19] Cavium. LiquidlO SmartNIC family [EB/OL] . [2020-08-10]. https://www.marvell.com/products/ethemet-adapters-and-controners/liquidi o-smail-nics.html
[20] Maroun T, Lina M, Mark S. Lynx: A SmartNIC-driven accelerator-centric architecture for network servers[C] //Proc of the 25th Int Conf on Architectural Support for Programming Languages and Operating Systems. New York: ACM, 2020: 117-131.
[21] Ii T H . Advanced microservices : a hands-on approach to microservice infrastructure and tooling.2017.
[22] Mellanox. ConnextX family intelligent data-center network adapters [EB/OL]. [2020・ 08-11] .https://cn.mellanox.com/products/ethemet/connectx-smartnic
[23] Broadcom. BCM58800 NetXtreme S~series named linley groups best embedded processor [EB/OL]. 2018 [2020-08-11]. https://www.broadcom.com/blog/bcm58800- netxtreme-s-series-named-linley-group ■s-best-embedded-processor
[24] Cavium. Marvell FastLinQ Ethernet NICs [EB/OL]. [2020-08-11]. https://www.marvell.com/products/ethemet-adapteTs-and-controllers/fhstlinq- performance-nics/documents.html
[25] Intel. Stratix V Device Handbook [EB/OL]. 2011 [2020-08-10]・ https://www.intel.com/content/www/us/en/programmable/dociinientation/nikl409774 008946.html
[26] Microsoft. TCP/IP offload overview [EB/OL]. 2019 [2020-08-11].
70
https://docs.microsoft.com/en-us/windows-hardware/drivers/network/tcp-ip-ofHoad
[27] Microsoft. Introduction to receive side scaling [EB/OL]. 2017[2020-08-l 1]. https://docs.microsoft.com/en-us/windows-hardware/drivers/network/introduction-to- receive-side-scaling
[28] Microsoft. Network virtualization using generic routing encapsulation(NVGRE) task offload[EB/OL].2017[2020-08-l l].https://docs.microsoft.com/en-us/windows- hardware/drivers/network/network-virtualization ・using・genei*ic-routing ・ encapsulation—nvgre—task-offload
[29] Xilinx.Products.Alveo SN1000 SmartNIC Accelerator Card[EB/OL].2021 [2019-0347] https://china.xilinx.com/support/documentation/data_sheets/c_ds965-u50.pdf
[30] Xilinx.Products.Alveo U50 Data-Center Accelerator Card[EB/OL].2019[2021 -02- 18]https://china.xilinx.com/products/boards-and-kits/alveo/sn 1000.html
[31] H. Frazier, nThe 802.3z Gigabit Ethernet Standard," in IEEE Network, vol. 12, no. 3, pp. 6-7, May-June 1998, doi: 10.1109/65.690946.
[32] IEEE Std 802.3ae 2002 Edition
[33] B. Raahemi, nError correction on 64/66 bit encoded links,'1 Canadian Conference on
Electrical and Computer Engineering, 2005.,	2005， pp. 412-416, doi:
10.1109/CCECE.2005.1556959・
[34] 杨钞翔.多协议高速串口芯片的PCS层设计和实现[D]•清华大学,2018.
[35] 张琴.10G背板以太网物理编码子层的设计与验证[D].中国科学技术大学,2016.
[36] W. P. Ranjula, R. M. A. U. Senarath, D. P D. Senaratna, G. D. S. P. Senaratne and S. Thayaparan, "Implementation techniques for IEEE 802.3ba 40Gbps Ethernet Physical Coding Sublayer (PCS)J 2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON), 2015?pp. l-5?doi: 10.1109/ECTlCon.2015.7207093.
[37] Ruan W ? Hu Q . A 0.18 pin CMOS transmit physical coding sublayer IC for 100G Ethemet[J]. Journal of Semiconductors, 2016, 37(3):035005・
[3 8] Kurilenko L S. FPGA Development of an Emulator Framework and a High Speed I/O Core for the ITk Pixel Upgrade. 2018.
[39] Sawada J , Nishi H . DC-Balanced Perfect Classified Channel for Hiding Framing Signals[J]. IEEJ Transactions on Electrical and Electronic Engineering, 2013, 8(6).
[40] 章曹超.lOOGbps以太网PCS子层模块设计与验证[D].东南大学,2016.
[41] M. Ruiz, D. Sidler, G. Sutter, G. Alonso, and S. Lopez-Buedo, 4tLimago: An FPGA- based open-source 100 GbE TCP/IP stack/5 in Proc. IEEE Int.Conf. Field Program・
71
Log. Appl. (FPL), Sep. 2019, pp. 286-292
[42] Batmaz B? Dogan A. 1 Gbit/s UDP/IP Offload Engine IP Core with PCIe Interface^]. Journal of Circuits, Systems and Computers, 201& 27(04):1850053.
[43] Ding L.Kang P,Yin W, et al. Design and implementation of hardware-based low latency TCP offload engine for 10 Gbps Ethernet[C]//2016 13th IEEE International Conference on Solid-State and Integrated Circuit TeclinoIogy(ICSICT). IEEE? 2016.
[44] Shantharama P , Thyagaturu A S ; Reisslein M . Hardware-Accelerated Platforms and Infrastructures for Network Functions: A Survey of Enabling Technologies and Research Studies[J]. IEEE Access, 2020, PP(99):1-1.
[45] Li Bojie, Tan Kun, Luo Layong, et al. ClickNP: Highly flexible and high performance network processing with reconfigurable hardware [C]// /Proc of the ACM Special Interest Group on Data Communication. New York:ACM, 2016
[46] Bosshail P, Gibb G： Kim H.S, et al. Forwarding metamorphosis: Fast programmable match-action processing in hardware for SDN [C] //Proc of the Special Interest Group on Data Communication. New York: ACM,2013: 99-110
[47] P. Shinde, A. Kaufmann, T. Roscoe, and S. Kaestle・ We need to talk about NICs. In 14th Workshop on Hot Topics in Operating Systems, HOTOS? 2013.
[48] PCI-SIG.PCI Express 4.0 Base Specafication 1.0[M],2017911
[49] Xilinx. 7 Series FPGAs Integrated Block for PCI Express v3.3[M],2020,07
[50] J. F. Zazo5 S. Lopez-Buedo? Y. Audzevich and A. W. Moore, nA PCIe DMA engine to support the virtualization of 40 Gbps FPGA-accelerated network appliances/1 2015 International Conference on ReConFigurable Computing and FPGAs (ReConFig), 2015, pp. 1-6, doi: 10.1109/ReConFig.2015.7393334.
[51] K. Chandra, A. P. Jagtap, N. Ranjan and S. Srivastava, nDesign of PCIe-DMA bridge interface for High Speed Ethernet Applications/' 2019 Second International Conference on Advanced Computational and Communication Paradigms (ICACCP), 2019, pp. 1-5, doi: 10.1109/ICACCR2019.8882929.
[52] F. Shanehsazzadeh and M. S. Sadri, "Area and performance evaluation of central DMA controller in Xilinx embedded FPGA designs/' 2017 Iranian Conference on Electrical Engineering (ICEE), 2017, pp. 546-550, doi: 10.1109/IranianCEE.
[53] L. Rota, M. Caselie, S. Chilingaryan, A. Kopmann and M. Weber. nA PCIe DMA Architecture for Multi-Gigabyte Per Second Data Transmission/ in IEEE Transactions on Nuclear Science, vol. 62, no. 3, pp. 972-976, June 2015, doi: 10.1109/TNS.2015.2426877.
72
[54] L. Rota, M. Caselle, S. Chilingaryan, A. Kopmann and M. Weber,nA new DMAPCIe architecture for Gigabyte data transmission/ 2014 19th IEEE-NPSS Real Time Conference, 2014, pp. 1-2, doi: 10.1109/RTC.2014.7097561.
[55] H. Kavianipour and C. Bohm, "High performance FPGA-based scatter/gather DMA interface for PCIe," 2012 IEEE Nuclear Science Symposium and Medical Imaging Conference Record (NSS/MIC). 2012, pp. 1517-1520, doi: 10.1109/NSSMIC.2012.6551364.
[56] Huang C, Yu X, Luo H. Research on high-speed network data stream capture based on multi-queue NIC and multi-core processor[C]//2010 2nd IEEE International Conference on Information Management and Engineering. IEEE, 2010: 248-251.
[57] N. Bonelli, S. Giordano and G. Procissi, "Network Traffic Processing With PFQJ in IEEE Journal on Selected Areas in Communications, vol. 34, no. 6, pp. 1819-1833, June 2016, doi: 10.1109/JSAC.2016.2558998. Cole, Schlesinger, David, et al. P4: Programming Protocol-Independent Packet Processors [J]. Computer Communication Review: A Quarterly Publication of the Special Interest Group on Data Communication, 2014, 44(3):87-95.
[58] PCI-SIG.PCI Express 2.0 Base Specafication l・0[S],2006,09
[59] V R. Datti and P. V Sridevi, ”Performance Evaluation of Content Addressable Memories/ 2018 7th International Conference on Reliability, Infbcom Technologies and Optimization (Trends and Future Directions) (ICRITO), 2018, pp. 596-598, doi: 10.1109/ICRIT0.2018.8748808.
[60] R・ Alexey and R・ Mikhail, nFPGA based implementation of content-addressed memory based on using direct sigma-delta bitstream processing/1 2016 IEEE NW Russia Young Researchers in Electrical and Electronic Engineering Conference (EIConRusNW), 2016, pp. 320-324, doi: 10.1109/EIConRusNW.2016.7448184.
[61] M・ Geier, F. Pitzl and S. Chakraborty, "GigE vision data acquisition for visual servoing using SG/DMA proxying/* 2016 14th ACM/IEEE Symposium on Embedded Systems For Real-time Multimedia (ESTIMedia), 2016? pp. 1-10.
73


Input Data
Position:
Data FoniSLf^
Do Di Dg D3/D5 D5 O6 D?
附录
Block Payload
C1
10
6
0x26
0x33
10
□7
W
0x66
10
0x55
10
10
w
0x87
10
4
Control Block Formats:
CQC-； C2 C3/C4 C5 Cg C7
C{? 5 5	% D$ o?
Co C1 C? C3/S4 Dg Dp 6
Oo Di O2 D3/S4 Dij D&
QQ D1 D2 D*d D5 Dg £>7
ToggC心 6
Do®。侃/H
D° 6 DJJ D3/O4 D«, Tg Cy
DQ
BlocU Type Field
0x1 e
0x78



















附图1 64b/66b编码规则（摘自参考文献［26］）
Control Character
Notation
XGMII Control Code
iOGBASE-ROmtnil
(.'ode
KKJBASE-R (> Code
SB/10B
Code8
idle
/V
0x07
(UOO

K28.0or
K283 or
K2S.5
start
/$/
Oxfb
Enccxied by block type field

K27.7
termhiate
ZTJ
Oxfd
EnctMied by block type field

K29.7
emn-
⑥
Oxfe
Oxle

K3O.7
Se<4ticnce： onlcrcd^s^t

0x9c
Encoded by blwk type field plus C) c(xJe
0x0
K28.4
rescrvedO
/R/b
Oxk-
0x24

K2&0
reserved 1

0x3c
0x33

K2JM
reservcd2
/A/
0x7c
0x4b

53
re^erved3
/K/
Oxbu
0x55

K283
reserved4

Ox.dc
(Jx66

K28.6
reservedS

0x17
OK7H

K23.7
Signal aRleivd_seic
>Tstg/
0x5c
fSnccxied by block type field plus O ccxie

K28,2
附图2控制字符对应表（摘自参考文献［26］）
74
LogiCORE IP 7 Series FPGAs Integrated Block for PCI Express
PCI
Express
Fabric
User Logic
Clock and Reset
附图3 7系列PCIE IP核结构框图（摘自参考文献［49］）
75






































