
第一章绪论
1.1本文研究背景
伴随着深度学习技术的突破和高速通信技术的进步，各式各样的智能设备
(手机，平板，机器人，AI助手)逐渐在社会生活中普及。同时，如今社会生活 中每天都会产生数以亿计的数据量，如图像，视频，音频等。在高速通信技术的 加持下的移动互联网的生态背景下，人们习惯使用智能设备获取信息，分享信息， 其中视频数据的占比正在逐步提高，而且目前在总体上占据了相当大的部分。视 频数据相比其他的数据，携带了更加丰富的信息内容。对于智能设备来讲，通过 摄像头和录音机采集下来的环境数据，通过算法让智能设备进行自我反馈，有利 于进一步提高设备的智能程度。在这些背景下，智能设备如何有效利用视频数据 更好去进行环境的感知和理解，已经成为研究热点之一。如视频感知，视频理解 技术。视频感知技术可以赋予智能设备感知外部世界的能力，如视频的目标检测， 视频的语音分割等，视频理解技术能在感知认知的基础上，打通自然语言和视觉 感知上的沟壑，能够在在社会生产中具有广泛的用途。在安防领域，可以用于对 异常行为的检测；在人机交互领域，可以利用丰富的多模态信息去理解人的行为 语言和动作；在互联网业务中，视频理解技术可以用于服务短视频，长视频，直 播平台，为各类视频在线实时标注分类，从而进行推荐实现有效分发，进行舆论 监控，以及实现精准的广告投放；在自动驾驶领域，可以帮助车辆识别不同的物 体，利用其丰富的多模态信息实现算法预警。面对如今海量的需要人工进行浏览 和分析的视频数据，利用视频感知和视频理解技术可以很大程度上地去解放人力, 大大提高效率。基于以上的这些需求，研究者们需要结合智能设备的硬件条件设 计出更加鲁棒高效的环境感知和理解算法，去挖掘出海量视频数据里面所蕴含的 丰富内容信息，利用这些信息，更好改善人类社会生活。
常见的视频数据含有多种模态的信息载体，常见的为图像，音频，文本：
(1) 最为主要的为视觉信息，基于深度学习方法的计算机视觉技术近年来 已经取得巨大的进步，从图像分类，人脸识别到图像生成，图像分割，都已经 取得了不输于人类的成绩，己经在社会生活的方方面面得到了应用，包括监 控，无人售货，手机应用，智能驾驶等。以新零售为例，人们已经实现了通过 人工智能技术实现无人售货，人脸付款等功能。在各式的手机中，己经实现了 人脸解锁，AI拍照。在互联网业务中，计算机视觉已经帮助企业实现了图像的 检索分类，风险控制等。在无人驾驶领域，计算机视觉技术起到了中流砥柱的
1 作用，从视觉或者激光的建图到对外界复杂世界的感知，无一不依赖强大的计 算机视觉技术。在人类的大脑思考过程中，图像能够给予智能设备“感知”外 界实物的能力。同理，使用目标检测，定位，分割算法也可以给智能设备带来 环境感知的能力。视频中蕴含连续的图像信息（含有连续的时序信息），能够在 图像的基础上给予智能设备“理解”事物和事物关系以及变化的能力，因而分 析理解视频信息一直都是一个重要的研究领域［1〕。以监控视频为例，重点分析 的目标为人，车，事件等，利用图像或者视频的感知技术可以进行人脸识别， 重行人识别，车牌识别，目标跟踪。利用视频理解技术可以进行人行为动作分 析，违规行为识别，事件的分析包括事件检测和异常行为预测等。通过对一段 安防视频的分析，可以得到大量的显式的信息特征，可以再利用分析到的这些 信息，去分析各个信息的关系形成完成的闭环过程，从而大大提高监管部门的 效率，促进智慧城市的建设。
（2）	音频也是视频里面一种重要的模态信息，使用单一的视觉信息可能无 法涵盖到所有的场景，例如，在行为识别的时候，对于说话和唱歌这两种行 为，仅使用视觉信息很难进行判别，如果引入音频模态的信息，任务的难度将 会极大降低。引入音频模态，可以利用音频进行事件检测，也可以提取音频中 的ASR C自动语音识别技术）信息，将音频转化为文本，根据文本的关键词进 行筛选，从而进行更高级语义信息提取等。Google在大型音频事件数据集 AudioSet进行了训练尝试，研究出了 VGGish模型【刀。视频中的音频模态数据 往往冗余度很大，如何对音频信息进行分析处理对于整个模型的特征也会起到 很重要的作用。目前在一些智能助手里面，己经存在用人类的声音特征去分析 使用者的情绪，从而辅助智能设备的决策。
（3）	在一些道路行驶视频，某些视频图像中可能包含有道路的路标信息。 很多的影视类视频，会含有很多字幕信息，这些信息对于一些具体任务都是十 分有意义的。这些情况下，可以利用OCR （光学字符识别）技术，将视频中这 些图像上的含有文本的内容提取出来，利用提取出来的这些信息，进一步去辅 助智能设备对采集到的环境视频的理解。例如，互联网应用中，可以利用对影 视剧的字幕进行提取，再利用NLP （自然语言处理）技术生成计算机可理解的 特征进行有效的分类，推荐分发。


图1-1常见的视频感知和理解应用领域
虽然基于视频数据的算法拥有如此丰富的应用领域，但是视频数据处理难度 大的问题一直存在，如何利用深度学习技术去处理视频数据更具有难度。常规的 方法如图1-2所示,对视频数据进行分析和挖掘算法模型往往先进行数据的传输， 将视频数据传输至远程的服务器，利用远程服务器强大的计算资源，在远程服务 器上进行的深度学习算法的处理，之后再将处理的结果回传至本地智能设备。虽 然这种方式可以利用远程服务器强大的算力资源，加速神经网络算法的处理速度。 通过这种方式也可以使用一些性能较好，参数和计算量规模较大的深度学习算法, 但是视频数据的传输需要高速而且稳定的通信网络，处理效率的瓶颈在于网络传 输速度。
在一些公共作业工作的智能设备中，如无人车，无人机，如果通信网络出现 问题，将会产生一些不可控制的安全后果。在一些家居机器人设备上，采取远程 服务器进行算法处理方式将会造成严重的隐私泄露问题，存在严重的安全隐患。 因而在这些智能设备上，需要搭建能够本地端能够可靠运行的基于视频数据的感 知和理解算法，如图1-3所示。



图1-2常规视频理解算法流程
视频数据


图1-3离线视频理解算法流程
在智能设备中部署的基于视频理解算法面临了一个重要问题，大多数的智能 设备的计算资源有限，无法提供和服务器一样的强大的算力。因此，为了解决在 这些智能设备上的视频算法的部署的问题，需要对整体的算法模型进行轻量化的 加速处理。
随着模型压缩技术的发展，移动端设备的算法部署越来越普及。相比服务端, 将算法部署在移动端有以下优势：(1)减轻服务器的压力。在某些高并发场景下， 如“双^一"过多用户同时访问时，全部视频的处理算法都在服务端进行处理，将 造成很大的延时，同时远程服务端压力巨大，只能扩容服务器，服务器成本高， 甚至存在宕机的可能。(2)实时性好，响应速度快，采用合适的算法处理，可以 达到相对不错的响应时间。(3)稳定性好，可靠性高，本地处理不存在网络中断 等问题，尤其在无人驾驶场景下，可以保证在高速行驶或者山洞隧道中稳定运行。
(4) 安全性高，用户隐私高，将数据传输到远程服务端，免去会造成用户隐私 泄露的风险。

尽管神经网络在许多任务数据集上都取得了不俗表现，如果考虑到实际落地 应用并将其移植到低算力的智能设备上，会受到很多条件上的限制一些常见 的卷积神经网络的参数量和计算量如图1-4所示。
Model	Params(M)	Flops(G)
AlexNet

ResNet50
ResNetlOl
图1-4常见神经网络参数量和计算量的大小
1. 神经网络计算量成本大：首先，深度卷积神经网络模型中一般会拥有着很 多卷积类型操作，仅运行一次前向运算的计算成本相比一般应有都相差几个量级, 大多数的智能设备都无法承受高昂的计算成本⑴。ResNet50计算量约为 3.5Gflops , ResNetl52计算量甚至达到了 11.3 Gflops,这仅是主干网络进行特征 提取的计算量，为不同任务仍需要在此基础上，进行附加的拓展设计，因而实际 中的深度神经网络模型的计算量往往会更大。即使利用先进的并行计算技术如 GPU, TPU等进行运算加速，在很多的智能设备上仍然无法正常使用。
2. 神经网络的参数量巨大：如图1-4中所示，引起深度学习浪潮的图像分类 模型AlexNet参数量大小约为6L1M,计算量Flops约为0.71GFlops, VGG16参 数量超过130M,计算量高达15.62GFlops, ResNet50参数量仍有25M,计算量 仍有3.5Gflopso由此可见，神经网络模型对于在低算力设备中的实际应用仍然是 巨大挑战。
3. 能耗大：神经网络进行运算时，会进行持续的内存访问，同时消耗大量的 算力资源，这两个方面都导致了耗电量巨大。因此，对于计算和内存资源有限的 智能设备，神经网络模型的使用在时间与空间上都受到巨大约束。
综上所述，如何在保证算法的效果的基础上，对神经网络模型进行压缩加速 已经成为了一个的研究领域，这使得模型压缩领域快速地发展起来。模型压缩是 指对在保证神经网络具有一定准确率的情况下下对神经网络进行压缩加速的技 术。经过神经网络压缩后的模型参数量往往会减少，与此同时，神经网络的计算 量也会下降，这使得算法的计算成本与存储成本大幅度的减少，为算法模型的部 署在受限资源条件下提供了可能性山。
综合以上研究，面向智能设备的轻量化环境感知和理解算法研究仍然是一个 存在很多挑战的领域。本文围绕着在智能设备如无人车，机器人等，利用深度学习技术，进行图像感知和视频理解的任务，从深度学习算法模型的轻量化，图像 感知算法和视频理解算法三个方面进行探讨研究。
1.2国内外研究现状及发展趋势
1.2.1模型轻量化和压缩加速国内外研究现状
移动智能设备部署神经网络模型存在很大的挑战，主要表现在在移动智能设 备上，计算能力，存储资源，电池电量都是受限的，所以移动端智能设备必须满 足模型尺寸小，计算复杂度低，电池耗电量低等特点。
模型的压缩和加速并不是相辅相成的，有的时候压缩并不可以带来加速的效 果，有时候二者是相辅相成的，压缩的重点在于减少网络的参数数量，加速则是 侧重于减少计算的复杂度。可以主要分为三种层次：（1）算法层，大致可以分为 六种方式：网络剪枝，参数共享，量化，网络分解，知识蒸谯和精巧的结构设计。
（2）框架层，主要在于对于深度学习框架Tensorflow, Pytorch和依赖底层库 CUDA, CUDnn的优化加速。（3）硬件层，这个层次主要是对各种硬件GPU, FPGA, ASIC针对深度学习算法进行优化。下面主要介绍算法层次的方法：
神经网络剪枝：网络剪枝早期是用来删除网络中冗余参数，降低网络复杂度, 从而提高网络泛化能力，并防止过拟合⑴。1989年，Lecun发表（Optimalbrain damage》⑶，最早提出了剪枝的概念，将神经网络的任意权值参数看作是单个参 数并进行非结构化的剪枝，但是剪枝代价高，效果不明显。2015年，Han发表 论文⑷，研究发现可以根据神经元连接权值的大小确定训练后的神经网络的不重 要连接 o2016 年 Wen 发表了《Learning Structured Sparsity in Deep Neural Networks》
[5] ,使用Group Lasso （组岭回归），将权值参数看成组来看待，实现将成组的权 重参数约束到0附近，实现学习到的神经网络参数尽可能的结构化稀疏。2017 年，Molchanov发表论文回，研究发现可以从众多的网络参数之中选择一个最优 的组合，使得被剪枝的神经网络损失函数的值最小。2018年，Han发表新的论 文⑺，利用了强化学习的方法，舍弃了人工设计剪枝策略，利用算法来实现神经 网络的自动化压缩。综合各类方法，剪枝从剪枝的主要对象往往可划分为：修剪 单个权重参数、修剪卷积核、修剪神经网络通道数等。不同剪枝方法从不同的角 度出发，在实际训练操作过程中往往会产生一定的区别，对不同类型的神经网络 的最后效果也会产生差异⑴。神经网络剪枝的一般流程如图1-5所示。


网络参数共享：主要思想是让神经网络中多个参数共享同一值，如果能够让 一个m * n阶的矩阵能够只用少于m * n个参数来描述，通常这样的不仅能减少内 存消耗，同时通过一些特殊的运算方法显著推理和训练速度。2015年，Han在 Deep Compression中提出［勺，将神经网络的权值大小和梯度大小进行聚类，同时 用哈希图记录下来，从而提升深度卷积神经网络的计算效率，同时大大地降低参 数量的大小，具体操作如图1-6所示。

图1-6 一种权值共享压缩方法示意
权值量化：主要思想是降低单个参数所需要的比特位数来压缩原始网络。对 于卷积神经网络，神经网络参数一般使用float32或者float64表示。低精度方法 使用更低位数的float型或int型对模型进行训练、测试和存储。编码方法对原有 数据进行重新编码，采用更少的位数对原有数据进行表示，实现了模型的压缩［匕 2016年，Dettmers提出低比特参数量化理论同，将32比特的梯度和激活值压缩 到8比特，在保证模型精度情况下，提出的方法取得两倍的传输加速。2015年， Courbariaux发表论文切，认为可以更加极端的量化，以极端的策略对神经网络的 权值进行量化，限制权值大小为+1或者-1。2016年，Rastegari发表论文卩°】，研 究认为将神经网络的输入和权值均进行二值化的处理，从头进行训练一个二值化
7
神经网络，而不是在已有的训练的神经网络进行二值化。2017年〔⑴，Zhou发表 研究论文，认为给定任意结构的全精度神经网络模型，将其转化为无损失的低精 度二进制模型，增量式网络的量化方法，权值划分，分组量化，重训练。神经网 络权值量化一般如图1-7所示。


网络分解（张量分解）：深度卷积神经网络的组成成分，主要包括卷积层，f 全连接层以及池化层，近年来批量归一化层也逐渐普及。一般而言，卷积层往往 计算成本较大，同时参数量较少，全连接层与之相反。一般可以使用SVD分解， 将三维张量分解成二维张量。2016年，Denton发表《Exploiting linear structure within convolutional netowrks for efficient evalution》问，这篇文章中对多种张量 分解的方法进行了实验分析，例如奇异值SVD分解，三维张量转化为二维张量 分解。卷积层的大量参数存在冗余，作者通过这些网络分解的操作以求获得近似 原始卷积计算过程，较大地减少了计算成本的大小，在保持了精度了情况下，取 得了近两倍的加速。
知识蒸憾：2014年，Hinton首次引入知识蒸憾［⑶，研究发现使用教师模型
（预先训练完成的复杂模型）和学生模型（目标训练的小模型）。学生网络模型 分别学习类别输出和目标真值的交叉爛。同时，还要学习自身网络模型类别的概 率上的输出和教师网络模型类别的概率上的输出的交叉爛，使得教室网络模型信 息可以传递到学生网络模型中山。2015年，Romero发表《Fitnets:hints for thin deep nets》a】，针对了当神经网络层数较深的情况下，学生神经网络直接模拟教师神 经网络的输出比较困难的问题，文章提出在神经网络模型的中间部分，再次增加 有监督的信息，使得学生模型和教师模型的神经网络中间层部分的激活响应尽可 能的保持一致。因此，知识蒸憾的训练方式一般分成两步，首先利用教师网络中 间层输出信息训练学生模型的神经网络前面部分参数，再利用教师网络的最终输 出信息训练学生模型的全部网络的参数⑴。2018年，Saurabh Gupta发表《Cross Model Distillation for Supervision Transfer》【⑸，提出跨模态迁移知识的方法，即 在RGB数据集学习到的网络参数信息也能迁移到深度估计的场景中来。知识蒸 憾的一般流程如图1-8所示。


图1-8神经网络知识蒸馆流程
网络设计：包含两个子方向：（1）人工设计精妙的神经网络结构。（2）基于神经 网络搜索的方法。轻量级神经网络架构的设计已经取得了一定的成果。Google在 2017年提出了首代mobilenet轻量型卷积神经网络，通过深度卷积Depthwise Convolution和 逐点卷积Piontwise Convolution的组合"深度可分离卷积"代替 标准卷积，减少神经网络模型的参数量和计算量"I。随后，Mobilenet借鉴ResNet 系列[⑺的设计思想，将残差连接引入进来，提岀了含有逆向残差模块的 Mobilenetv2[18]o 2019年，又在前作的基础上，利用NAS神经网络搜索技术，同 时引入新的激活函数，提出了 Mobilenetv3。2017年，旷视发表论文〔⑼，采用了 channel shuffle （通道混洗），pointwise group convolutions （逐点分组卷积）以及 深度可分离卷积，提出了 Shufflenet模型，保证精度的情况下，大大提高了效率。 随后,又在Shufflenet的基础上，提出了 ShufflenetV2[20\ 2019年,谷歌发表了 EfficientNetl21h针对卷积神经网络中的网络深度，网络宽度，输入图像分辨率大 小，通过神经网络搜索的技术，获得针对于不同环境下的一组最优参数。2020年， Facebook 发表《Designing Network Design Spaces》[22],提出了一种新的网络设 计范式，在神经网络搜索设计网络架构更进一步。
122基于视频数据的感知和理解算法国内外研究现状
视频数据因其携带丰富的信息内容成为当前深度学习领域的研究热点之一，
视频感知算法可以看做是常见计算机视觉算法如目标检测，语义分割在视频数据
上的延伸，这种情况下往往需要考虑算法的时效性，基于视频数据的感知算法往
往要保证精度的同时追求更好的实时性。视频理解技术在社会生产中具有广泛的
用途。在安防领域，可用于对异常行为的检测；在人机交互领域，可以通过丰富
的模态的信息去理解人的行为语言和动作；在互联网业务中，视频理解技术可以
服务于短视频，长视频，直播平台，为视频在线实时打标，推荐进行有效分发，
舆论监控，广告投放；在自动驾驶领域，可以帮助车辆识别不同的物体，其丰富
9

的模态信息可以提供信息去预警。面对海量需要人类浏览和分析的视频数据，利 用视频理解技术可以最大限度地去解放人力。目前，仅理解图片内容已经无法满 足日益增长的需求，在二维的图片数据领域，神经网络模型取得达到不输甚至超 过人类的成绩，但是目前让计算机去理解视频内容的空间和时间信息仍然充满着 挑战，视频内容理解的概念比识别分割单个目标更加复杂，需要计算机去学习更 加高级的语义信息，如交互对象，场景环境，人类姿态，语义理解。理解视频不 仅仅要去分析图像的空间信息，更要去分析时间维度的信息。

图1-10单帧图片的感知（左）和多个连续帧的内容理解（右）
目前，对环境视频的内容的研究主要包含识别，分类，检测，预测以及理解。 主要可以概括为：1、动作检测，2、行为识别，3、时序动作检测，4、行为预测， 5、视频摘要。
2014年，Karpathy最早提出使用单流神经网络进行视频动作识别任务，探
10
索了多种方法使用预训练的二维卷积神经网络从连续帧中融合时间信息㈡]。 Simmoyan和Zisserman在2014年研究发表了双流网络〔约，考虑需要学习动作 特征，引入了使用光流去建模视频的时间特征。因此，论文中有两个单独的网络， 分别是学习空间特征的图像输入卷积神经网络，学习时间特征的光流输入卷积神 经网络，最终将两个神经网络的输出进行融合，取得了不错的精度，但是，计算 开销比较大。2014年，Du发表了基于3D卷积的方法的论文a®,首次将三维卷 积神经网络作为特征提取器，依靠3D卷积同时学习视频的空间和时间特征。 2016年，Wang发表了 TSN的论文㉔〕，文章对双流的架构进行了优化，采用稀 疏采样的方式获取更长时间信号的建模，解决了长时间建模困难的问题。同时， 针对视频级别的预测，探索了多种的组合策略。2017年，Girdhar发表了
《Action VLAD: Learning spatio-temporal aggregation for action classification》[27], 文章认为，可以通过VLAD的方法学习到视频级别的特征聚合，并通过视频的 聚合特征训练端到端的模型，用来捕捉长期的依赖。此后，为了解决3D卷积神 经网络计算开销大的问题，I3D【28】，P3D〔29], S3D[30], R(2+l)D【3i], MFfiet[32]等一 系列基于三维卷积方法的3D卷积神经网络算法提出，研究如何更好地节约计算 成本，同时达到更好的精度。
同时，音频也是视频里面一种重要内容数据，仅使用单一的图像模态，算法 无法学习到所有的视频场景。引入音频信息内容，可以基于音频进行事件检测， 也可以提取音频中的ASR信息。将音频转化为文本，根据文本的关键词进行筛 选，从而进行更高级语义信息提取等等。声音的分类可以根据任务目标的不同， 划分为不同的任务。对于一段长音频，目标可以是一个全局的标签，也可以是每 个音频段各自有一个标签。在音频信号处理领域，构建适当的特征表示和分类模 型通常被作为两个分离的问题。这种做法的一个缺点是人工构建的特征可能对于 目标任务可能不是最优的。深度神经网络具有自主提取特征的能力，因此可以将 上述两个问题进行联合优化。Google在大型音频事件数据集AudioSet进行了尝 试，利用log-mel音频特征，研究出了 VGGish模型Bl。视频中的音频信息往往 冗余度很大，如何对音频信息进行分析处理对于整个模型的特征也会起到很重要 的作用。
在获得视频内容中的多种模态的信息之后，如何对各个模态的信息进行特征 的提取，以及如何运用对提取出来的特征信息，在不同场景的任务中去使用，这 是多模态模型融合的关键问题，为了使模型进一步加强对内容的理解，它需要具 备解释多模态特征的能力。表征(Representation)：找到某种对多模态信息的统 一表示，每个模态各自映射，然后用用相关度距离来约束表示，或者是多个模态 一起进行映射。翻译(Translation): 一个模态映射到另一个模态，分example-based
11
（有候选集，如检索任务），generative （Decoder-Encoder）»对齐：寻找不同模态 子成份之间的关系，如某词对应某区域，分显式对齐和隐式对齐，例如注意力机 制。融合（Fusion）,指的是如何整合信息，常用的方法有早融合，晚融合。联合 学习（Co-learning）：通过利用丰富的模态的知识来辅助稀缺的模态，例如迁移学 习,zero shoto
常见的多模态融合方式有：Pixel level,对原始数据最小粒度进行融合；
Feature level,对抽象的特征进行融合，包括早融合和晚融合，代表融合发生在特 征抽取的早期和晚期，如图l-llo早融合是指先将特征融合后再输出模型，缺点 是无法充分利用多个模态数据间的互补性，且存在信息冗余问题（可用降维等方 法缓解）。晚融合分为融合和不融合两种形式，不融合类似集成学习，不同模态 各自得到的结果了之后再统一打分进行融合，好处是模型独立鲁棒性强。融合的 方式即在特征生成过程中（如多层神经网络的中间）进行自由的融合，灵活性比 较高，如特征金字塔融合。或者是Decision level对决策结果进行融合，类似于 集成学习。Hybrid level混合融合多种融合方法。
a.早融合

图1-11两种的不同的特征融合方式
目前基于视频的研究主要是对图像，音频，视频三种模态数据的处理的研究。 之所以要对不同的模态进行融合，是因为不同模态的数据表现方式不一样，看待 世界的角度也会存在一些差异，所以存在一些交叉（所以存在信息冗余），互补 （比单特征更优秀）的现象，甚至模态间可能还存在多种不同的信息交互，如果 能合理的处理多模态信息，就能得到丰富特征信息。即概括来说多模态的显著特 点是：冗余性和互补性。
一般多模态需要处理的任务主要如表1-1有：
12
表1-1多模态算法的应用
应用
表征
翻译
主要难点



对齐
融合
联合学习
音频-视觉语音识别

1.3本文研究内容和研究要点
随着无线通信技术的高速进步，视频类型数据已经成为当今社会中最为常见 数据种类，在各种不同的智能设备上均有视频数据的产生和传输。目前对视频数 据的算法处理大多数都还在在远程服务端进行，由于部分环境极度需要高稳定性, 低延时，如无人驾驶，特殊场景的监控，或者是较高的隐私性，如私人场所的智 能机器人助手。在这些场景下，将视频数据传输返回到远端服务器会存在上述所 说的问题。为了解决这些问题，本文主要针对于解决智能设备如机器人，无人车 等较智能设备设备上视频算法的部署问题，在保证一定精度的情况下进行探讨研 究。
本文利用通过使用先进的深度学习技术，搭建了面向智能设备的环境感知和 理解的系统，包括：
(1) 基于二维图像的环境感知神经网络模型，同时用于视频数据中单帧图像 模态的特征提取。本文基于目前深度神经网络轻量化的研究，构建出了轻量型神 经网络M-SOSANet,并且在该网络的基础上进行图像分类，语义分割任务的实 验分析，验证该环境感知算法的性能，分析算法的鲁棒性和泛化性，效率指标。
(2) 基于视频数据的3D神经网络模型用于视频动作，行为理解任务，同时 用于视频数据中视频(连续帧)模态数据的特征的提取。本文基于目前对视频行 为识别，动作分类的算法研究，结合3D神经网络网络的轻量化的方法，设计了 3D卷积神经网络RegNet3Do本文分别在行为识别和动作分类任务上进行实验， 分析该网络的特征提取能力，计算量和参数量。
(3) 基于视频数据的中音频模态的特征提取模块。音频作为视频数据中重要 的数据模态，如何去最大化地利用音频特征同样是一项重要的工作，本文将构建 出轻量化的音频分类算法，同时为了解决没有经过大规模数据集预训练精度下降
13
的问题，引入知识蒸憾的方法。论文在声音场景分类数据集上进行实验，分析轻 量化音频分类模型的性能，参数量和计算量。
(4) 最终，为了最大利用视频内容的信息，基于不同模态的特征，构建基于 多模态特征融合的视频描述算法。本文将会使用单帧图像，连续帧视频和音频三 种不同的数据形式，进行多模态的数据处理，特征融合，实现完整的视频描述算 法，实现基于视频数据的轻量化环境理解算法。
针对目前面向智能设备的环境感知和理解中存在的问题，本文提出了以下的 研究要点。
(1) 提出了本文的二维轻量型卷积神经网络结构，轻量化的神经网络模型 M-SOSANet,将会分别介绍自行搭建的MEESP模块(更为高效的深度可分离空 洞卷积金字塔)，以及Mobile SOSA模块(结合通道混洗的一次性聚合模块)，并 在多个常见的计算机视觉任务上进行实验分析，1•大规模二维图像分类数据集 ImageNet-lOOOK, 2.二维图像分类数据集CIFAR-10和CIFAR-100,进行二维图 像分类实验分析，图像语义分割数据集Pascal VOC 2012进行训练，实验分析算 法模型在语义分割上的性能。
(2) 介绍本文所构建的3D轻量型神经网络模型，并从多个角度分析实验了 影响3D神经网络参数量和计算量的因素，最终制定了合适的视频采样策略，并 在2D神经网络的基础上设计了轻量型3D神经网络，在多个基准数据集如动作 分类数据集SomethingSomethingV2, UCF101,手势识别数据集Jester上进行验 证。
(3) 介绍了本文构建的轻量型音频分类模型，为了解决未使用大规模数据集 进行预训练，神经网络模型精度衰减的问题。通过引入知识蒸憾的方法，解决以 上的问题。在音频分类数据集UrbanSound8K上面，实验分析了经过知识蒸惚改 善的轻量型音频卷积神经网络的性能。
(4) 构建了本文的跨模态轻量型注意力机制模型，依据多模态融合改进多头 注意力机制，构建了跨模态的自注意力机制，并和Transformer的变体DeLighT 结合，构建出了跨模态轻量型的encoder-decoder架构，通过在Video Caption数 据集上进行了实验，分析了多模态特征融合模型的性能。
14
1.4本文结构安排
整篇论文一共分为六章，重点研究如何在智能设备上部署视频理解算法 结构如图1-12下。

第一章为研究背景和意义，关键技术的介绍。主要包括了研究工作的目 标意义，视频数据的特性，神经网络轻量化和加速的关键技术，基于视频数 据的环境感知和理解算法关键技术，多模态算法的关键技术。
第二章将介绍本文的第一个研究点，轻量型神经网络模型M-SOSANet的 设计和搭建。首先，将重点介绍该网络的两大模块MEESP下采样模块和 SOSA模型的设计特点和优势，将会进行常见计算机视觉任务图像分类数据 集CIFAR-10, CIFAR-100,著名的图像分类基准数据集ImageNet-lOOOK上 进行多组实验分析验证。然后使用在ImageNet上的预训练模型，迁移学习 语义分割分割任务，在语义分割数据集PASCALVOC的进行实验分析，同时 也会在移动设备NvidiaTX2,常规CPU 15-8400,和GPU设备NvidiaTITANX 进行时效性能的验证。
第三章将介绍本文的第二个研究点，对2D型深度卷积神经网络进行改 进，将对轻量型2D的卷积神经网络进行改造，延展为轻量型3D的卷积神经 网络RegNet3Do从数据的输入形式到神经网络的设计，进行分析实验，在此 基础上设计了轻量型的视频行为识别算法，动作分类算法。本章节在行为识 别动作分类数据集UCF-101进行实验分析，大规模手势动作分类数据集 Jester,以及大规模行为分类SomethingSomethingV2进行实验分析，进行主 流行为动作识别算法和本文提出的轻量型动作识别算法的对比实验。
第四章将介绍对视频中音频数据的特性以及音频分类的常用方法。本文 为了解决大规模数据集训练困难的问题，将介绍本文使用的结合知识蒸傭的 声音分类模型的训练方法，将会声音场景分类数据集UrbanSound8K进行实
15
验，验证分析了音频卷积神经网络模型的性能，以及本文使用的训练方式的 可靠性。
第五章将介绍本文研究点四，基于多模态特征融合的视频描述神经网络, 本章节将会介绍如何利用不同模态的数据特征进行特征融合，从而更好地去 指导视频理解任务的执行，重点介绍跨模态的注意力机制，以及transformer 的轻量化，并在视频描述任务的数据集上进行实验，分析实验结果，验证模 型的准确度和效率，实现视频和语言的互通理解。
第六章将会总结论文的整体，涉及研究目标内容，对本文进行概括性的 描述，总结现有的研究成果，同时分析现有方案的不足之处和可改进的地方， 针对这些存在的问题进行思考总结。

图像	视频	音频
图1-13本文各个神经网络关系结构图

1.5本章小结
本章节首先介绍了面向于智能设备的环境感知和理解算法的研究背景和意 义。其次研究了近年来深度神经网络压缩和加速的国内和国外研究现状和未来趋 势。重点说明了本文的主要研究内容，面向智能设备的环境感知和理解算法，即 如何在智能设备上部署轻量化的深度学习算法，达到实现环境感知和理解的目标。 本章概述了本文的主要研究内容，轻量化的2D卷积神经网络，轻量化的3D卷 积神经网络，用于音频模态数据的轻量化卷积神经网络，跨模态注意力机制的特 征融合网络。在本章的最后，详细介绍了文章的结构安排。
16
第二章构建高效轻量型2D卷积神经网络
2.1引言
本章中将会介绍本文的研究点之一，面向智能设备的轻量型神经网络模型M- SOSANet,该模型综合考虑了在较低算力智能设备上的效率和精度。著名的神经 网络模型DenseNet&3］将神经网络中的每一层连接到另一层，这样的密集连接可 以有效地去学习中间层之间的信息，但是由于密集连接增加的通道数量会引起大 量的资源消耗，同时增加了神经网络的参数量和计算量，无法实现在低算力设备 上的使用需求。MobileNet系列［⑹勺采用深度可分卷积，减少了计算量和参数， 但是深度卷积的方式在一些并行设备上不具有的实际的高效性。ESPnetv2【34】使用 分组逐点和深度空洞的可分离卷积从一个大的感受野中学习信息表征，具有较少 的重复次数和参数，但精确度存在略微下降，如下图2-1所示。为了解决以上的 这些问题,本文提出了两个模块：（1）Mobile SOSA模块，（2）采用记分机制的 下采样模块MEESP,并引入了注意机制。与过去的一些方法相比，本文的M- SOSAnet模型具有较低的计算量，更高的精度，保持了相似的参数量。本文将会 二维图像领域的图像分类、语义分割等任务上对轻量化的神经网络模型进行了实 验分析，证明了本文所提出的模型。

2.2研究关键点
2.2.1 MEESP下采样模块
More Extremely Efficient Spatial Pyramid of Depth-wise Dilated Separable Convolutions （更为高效的深度可分离空洞卷积金字塔），如图2-2所示，本文首 先将传统的标准卷积核分解为两个低秩卷积核，同时将以池化代替步长进行下采 样，本文在MNIST手写数字分类数据集上验证了这种方式的有效性，该模块在 降低复杂度的同时，引入了一种动态的多分支卷积得分机制。本文发现这种方式 能够使神经网络更为有效的选择感受野，从而动态地关注更为有效的分支。
中间支路:在下采样阶段,本文首先使用了0 = 4的pointwise group convolution （逐点分组卷积），将较高维度的投射到较低的维度。在ESPnetv2中，下采样操 作由标准的步长为2的3 * 3卷积核完成［列。在本文的神经网络模型中，本文将标 准的N * N卷积核分解为N * 1, 1*N卷积核，同时将下采样工作由maxpool （最大 池化）和avgpool （平均池化）完成。和ESPnetv2中的EESP模块一样设计四条 支路，因为这种方式既可以扩大感受野的大小，也可以通过设置不同的空洞率来 消除dilated convolution （空洞卷积）所带来的The Gridding Effect （栅格效应）。 本文将第一，第三支路设置为 3*1 depth wise convolution + maxpool + 1 * 3 depthwise convolution （3 * 1深度卷积+最大池化+ 1*3深度卷积），将第 二， 第 四支路 设置为 3*1 depth wise convolution + avgpool + 1 * 3 depth wise convolution （3*1的深度卷积+平均池化+ 1*3的深度卷积）。 相比于原始的ESPunit,本文的MEESP模块可以将复杂度进一步下降。例如， 当M = 64, g = 4时，参数的数量下降了为原来的0.8。本文在神经网络的每个 group convolution （分组卷积）后面都加入了 channel shuffle （通道混洗）以减轻 分组卷积分割通道，产生的通道相关性的负影响。经过这些操作，整个下采样模 型的计算量将会下降，整体计算量比较如公式2-1所示。
原始计算量_ Md/g+（n2+d）dK
目前计算量 ~ Md/g+（2n+d）dK	2_1
在完成了下采样操作之后，本文没有选择将各个支路的特征图进行直接拼接, 而是在前面操作之后引入一种多分支打分机制。四条支路输出的特征图分别为支 路1的输出outl,支路2的输出out2,支路3的输出out3,支路4的输出out4, 并和输入的特征图进行相加操作，得到了fusion feature map （融合的特征图像） =分支1输出+分支2输出+分支3输出+分支4输出。然后本文使用 global avgpool （全局平均池化）和global maxpool （全局最大池化）分别生成两
18
种1*1*C的张量。然后本文使用一个特征转换分析模块，它由两部分组成分别 为F1和F2,它的目标在于将经过global pooling的生成的全局特征进行特征映 射，将每个通道的得分值映射出来。在F1代表ReLU激活，B代表BatchNorm Layer (批量归一化层)，X为全局的特征变量，W为1*1的常规卷积，将高维的 数据映射到低维。之后，再由F2从低维映射到和各个支路channel数目之和。在 经过前面的操作后，最后使用Softmax函数使数值归一化，得到了每个通道的相 应得分。整体流程如如下公式所示。
FM = Sti out：	(2-2)
耳core(FM)	=	a * Avgpool(FM) +	* Maxpool(FM)	(2-3)
Fi(x) = ReLU(B* %))	(2-4)
F2(X) =a)2*x	(2-5)
Score = F2(^IW)	(2-6)
在完成上述的操作之后，神经网络将会得到每条支路的归一化的分数值，假 设SI, S2, S3, S4分别代表各个支路的分数值，分别将各个支路的分数与各个 支路的输出相乘，代表将每条支路的输出根据这条支路的重要程度，乘上一个可 变的控制因子，用于衡量每条支路的重要程度。完成控制因子的相乘之后，最后 再将经过控制因子影响的各个支路的输出进行拼接。
Score = So/tmax (Score)	(2-7)
outi = Si®out[	(2-8)
s.t.	= l	(2-9)
左支路：在下采样模块的左边支路本文在卷积操作之后加入了通道注意力和 空间注意力机制，在基本不增加整体网络参数的大小的情况下，提升精度。
右支路：将原本的ESP模块中的平均池化替换为最大池化。整个下采样模块 将右支路与中间支路的输出结果进行拼接操作，再和左支路的特征图进行相加， 最后通过激活函数，这里的激活函数本文选用PReLU函数。
该部分的神经网络详细结构图如2-2所示。在进行完下采样的之后，本文采 用Mobile SOSA Module进行特征重组和映射。
19


图2-2 M-SOSANet中使用的MEESP模块
2.2.2 Mobile SOSA特征映射模块
2019年，Youngwan Lee发表了 VOVnet[36], 一种高效GPU计算的卷积神 经网络模型。相比于著名的DenseNet, DenseNet密集连接所有中间层卩‘〕。随着
20

层数的加深，每一层的通道数都会线性增加，这将不可避免地导致低效率，因此 VOVnet提出了 OSA模块，该模块只需将最终特征图中的所有特征聚合一次，就 可以解决密集连接导致的输入通道线性增加的问题。本文在OSA模块的基础上， 提出了 SOSAModule(Shuffle One Shot Aggregation)(通道混洗一次性聚合模块)。 如图2-3所示,本文在OSA模块的基础上，将上一层输入的特征图先进行channel
shuffle (通道混洗)，再将通道切分成三份，每次分割出的左支路都将在最后进行
Aggregate合)，中间支路和右支路采用类似与ResNet的结构，区别在于本文
的SOSA Module在最后并不是采用输入的特征图像相加，而是采用拼接的方式。

根据Shufflenetv2中提出的在卷积的过程中输入特征图的通道数目和输出特 征图的通道数目大小保持一致的情况下，能够使MAC (memory access cost)达 到最小，MAC数值越低硬件计算上效率达到越高。这样的设计保证了输入进
SOSA Module的特征图的通道数目和输出的通道数目保持一致。原始的OSA
Module对低算力设备并不友好，它是为了提高在高性能GPU上的效率而设计。 因此为了减少参数量，本文将右支路设计的输入先进行1 * 1分组卷积进行降低维 度(g=3)并进行通道混洗，同样为了在进行特征的重新映射时具有更大的感受野。 这里本文仍然使用三组不同的卷积核组合，分别是3*l + l*3(d = l), 3*1 + l*3(d = 2), 3*l + l*3(d = 3),再将三条支路的特征图进行相加，之后在使 用1 * 1分组卷积(g=3)进行升维度并通道混洗，升高维度之后，以求达到更高的性 能。SOSA模块的结构如图2-3所示。

图23左图是VOVNet中使用的OSA模块〔殉，右图是本文提出的SOSA模块
21
2.3 2D轻量化卷积神经网络整体框架
本文的卷积神经网络的结构如下图2-4所示:
In put
Operator
Out cha nnel
2242x3?x3
StemBlock
256
562x256
DownSample Block
512
28?x512
Mobile SOSA
512
282x512
DownSample Block
1024
142x1024
Mobile SOSA
1024
142x1024
DownSample Block
2048
72x2048
Mobile SOSA
2048
72x2048
GConvlxl + Shuffle
2048
72x2048
GConvlxl * Shuffle
1280
72x1280
7x7 global average pool woo-d fc, softmax
1000
图2-4本文所提出的M-SOSANet的结构在ImageNet数据集上的神经网络结构示意
针对图像分类任务中的大规模数据集ImageNet,以下采样模块（MEESP） + Mobile SOSA的组合重复叠加3次组成，在最后将会使用两次逐点卷积piontwise convolution +通道混洗channel shuffle进行特征的重组，最后送入全连接层对高 级特征进行分类。在进行CIFAR-10和CIFAR-100的图像分类的时候，由于图像 的分辨率小，将会调整下釆样的次数，将原有的5次下采样改为3次下采样，以 保证最后的特征图的w和h均大于1,保证下采样不损失过多的信息。
在图像分割的实验中，基于M-SOSANet的图像分割任务，将使用ASPP模 块和该网络模型结合卩7】。ASPP结构的全称是Atrous Spatial Pyramid Pooling,其 中采用4个并行，不同空洞率的空洞卷积对输入的特征图进行处理，具有不同的 空洞率的卷积核能够捕获到多尺度的信息。其中包含有1*1的普通卷积，3*3的 空洞率为6的卷积，3*3的空洞率12的卷积，3*3的空洞率为18的卷积，以及 一个全局的池化操作。在M-SOSAnet的基础上进行拓展，语义分割算法模型的 整体结构和语义分割算法的深度神经网络模型DeepLabV3相似⑶】。如下图2-5 所示。
22






图2-5基于M-SOSANet的图像语义分割算法模型
2.4实验验证与结果分析
2.4.1图像分类
本文在图像分类的任务上，分别在三个数据集CIFAR-10, CIFAR-100, ImageNet-lOOOk进行了实验。下面将就这些数据集分别进行实验细节介绍和分析。 • CIFAR10 和 CIFAR-100
CIFAR 数据集由 Alex Krizhevsky, Vinod Nair 和 Geoffrey Hinton 收集和整 理自8000万张微型图像数据集，其中CIFAR数据集又根据所涉及的分类对象数 量，可分为CIFAR-10和CIFAR-100。这两个数据集目前主要用于深度学习领域 的图像分类任务，目前已被广泛应用。

图2-6 CIFAR图像分类数据集内容示意

如上图2-6所示，其中CIFAR-10数据集包含了轮船、卡车、狗，鸟等10个
23 类别物体的32x32大小的彩色图片，在每个类别中有6000张图片，由此可得， 整个数据集一共拥有60000张图。其中，汽车和卡车尽管都属于车类，但二者在 分类时属于不同的类，不存在重叠。汽车主要包括轿车、越野车之类的车。
数据集包括常见训练集和测试集两种。训练集一共50000张图片，每10000 张作为一个批次，每个批次包含了来自10个类别的1000张图片。训练集的批次 是随机抽取的各个类中的图像，因此存在某一个类的图像数量多于另一个类的图 像数量。测试集一共10000张图，也包含了随机从每个类中抽取的1000张图。
在基于CIFAR数据集的实验中，采用标准的数据增强方法：在图像两侧填 充零，然后剪切生成一个随机的32x32图像，然后将图像的一半水平翻转，并用 平均通道进行归一化。为了提高准确率，本文还采用了 cutout的数据增强方法， cutout方法是在训练阶段，随机在训练图片上截取出一块空洞，以达到防止过拟 合的目的〔3%在训练阶段，本文使用SGD随机梯度下降算法，从训练开始本文 使用的所有网络和IGCV3［佝算法的相同的权值初始化方式，权值衰减值0.0001, 动量为0.9,本文将初始化学习率为0.05,总训练轮数为300,在第100, 180, 250轮，学习率乘以衰减率0.1, batchsize的大小设置为64。M-SOSAnet以只有 290M的计算量，在CIFAR-10上面达到95.56%的topi准确率，在CIFAR-100中 达到78.6%的topi准确率。具体性能对比如表2-1所示。
表2-1 M-SOSANet在CIFAR10, CIFAR-100数据集上的性能对比

#计算量Flops
CIFAR-10
CIFAR-100
IGCV3-D

MobileNetv2

ShuffleNetv2

M-SOSANet

• ImageNet-1000K
ImageNet图像数据集始于2009年，李飞飞教授等在CVPR上发表了一篇名 为《ImageNet: A Large-Scale Hierarchical Image Database》，它是目前计算机视觉， 甚至是人工智能领域最为重要的数据集之一厲】。ImageNet的出现改变了人工智 能领域中，研究者们对数据集的认识，研究者们开始由于该数据集的出现和实际 效果，真正开始意识到数据集在人工智能研究中的地位，数据和人工智能算法的 重要性是一样的。ImageNet是根据WordNet层次结构组织的图像数据集。在 ImageNet中，目标是为了说明每个synset提供平均1000幅图像。每个概念的图 像都是质量控制和人为标注的。在完成之后，希望ImageNet能够为WordNet层
24 次结构中的大多数概念一样提供数千万个干净整理的图像。ImageNet是一项持 续的研究工作，旨在为世界各地的研究人员提供易于访问的图像数据库。目前 ImageNet中总共有14197122幅图像，总共分为21841个类别。
在基于ImageNet的实验中,本文首先使用标准的ImageNet数摇集数据增强， 标准化操作，随机剪切到224*224,随机水平翻转。在此基础上同时引入了 mixup 的数据增强方法，以增强算法模型在测试集上鲁棒性。算法模型使用SGD优化 算法，同时使用4块GPUs进行训练,batchsize大小为256,权值衰减为0.00004, 动量值大小0.875,本文使用指数型的学习率计划，同时进行5轮的warmup开 启训练。本文实验细节如下，训练300个epoch模型，再加上50个epoch进行 再训练，每个阶段的学习率分别为0.045和0.98衰减率。由于ImageNet的数据 集数据量巨大，在训练时候，采用了混合精度训练的方法。混合精度训练是在尽 可能减少精度损失的情况下利用半精度浮点数加速训练。它使用FP16即半精度 浮点数存储权重和梯度。在减少占用内存的同时起到了加速训练的效果。基于这 种方法可将训练时间由平均2周减小一倍。具体性能对比如表2-2所示。
表2-2 M-SOSAnet和其他著名轻量型神经网络的在ImageNet的topi精度对比

Topi精度
IGCV-D

Mobilenetv2

Shufflenetv2

M-SOSANet

2.4.2语义分割
在语义分割的实验中，本文使用的是Pascal VOC 2012数据集⑷］。PASCAL VOC是计算机视觉领域一个物体分割著名数据集，拥有一套合理的评价准则。 PASCALVOC数据集包含20个不同的类别：一级类目：人类，主要包含人这一 对象。一级类目：动物，其中包含了狗、马、羊、猫、牛、鸟以上类别的动物。 一级类目交通工具，其中包含了公共汽车、小轿车、摩托车、火车、飞机、自行 车、船以上类目。一级类目室内物品，其中包含了瓶、椅子、桌子、植物、沙发、 电视以上类目。该数据集图像质量好，标注完备，非常适合用来测试算法性能。 PASCAL VOC2012数据集提供了 20种不同类型的对象，其中有1.4k训练集图 片、1.4k验证集图片和1.4k测试集图片。
在进行语义分割的实验中，实验根据ESPnetv2的训练过程，分两个阶段训 练的语义分割网络〔河。在第一阶段，本文使用小分辨率图像输入（256*256）,使 用SGD优化算法，初始学习率为0.007,训练100个epoch»在第二阶段，训练 过程中，将会图片分辨率提高（384*384）,在第一阶段的训练模型的基础上进行
25
再训练，同样使用SGD优化算法，训练100个epoch,此时调小学习率为0.003。 具体性能对比如表2-3所示。
表2-3 M-SOSANet在语义分割上的应用性能对比

#计算量Flops
mIOU


SegNet

MobileNetvl


MobileNetv2


ESPNet


ESPNetv2


M-SOSAseg

2.4.3效率对比
大多数的智能设备的计算资源十分有限。一个优秀的轻量级神经网络必须满 足合格的精度要求，并且在设备上具有高效率、低延迟。本文在不同的设备上测 试了 M-SOSAnet： (1) NvidiaTX2O 这款模块采用 NVIDIAPascal™GPU,高达 8GB内存、59.7GB/s内存带宽，提供丰富的标准硬件接口，完美适配各类产品 和外形规格，实现真正意义上的AI计算终端。
(2) Nvidia Titan Xo TITAN X 的 FP32 单精度浮点是 11TFLOPS(AMD 双芯 显卡Radeon Pro Duo是16.38TFLOPS),衡量深度学习的指标INT8达到44TOPS。 新TITAN X还拥有12GB GDDR5X显存，384bit位宽，带宽高达480GB/S。
(3) 15-8400o英特尔酷睿第八代CPU芯片，六核心六线程，制作工艺为14 纳米，2.8GHz最大睿频。
为了公平比较，本文都使用Pytorchl.O作为本文的深度学习框架，下表2-4 比较了各种轻量级网络在不同平台的效率对比，可以看出M-SOSnet在cpu设 备和较低算力的gpu设备TX2均具有匹敌目前其他主流方案的性能。
表2-4 M-SOSNet在不同的设备的效率对比

Nvidia TX2

2.5本章小结
本章主要介绍了本文的研究点之一的面向智能设备的轻量化的2D卷积神经
26
网络M-SOSANet的设计和搭建。其次重点介绍神经网络模型的两大主要构成的 模块MEESP下采样模块和SOSA特征映射模型，详细介绍了它们的设计特点和 优势。最后介绍了详细的实验分析，从常见计算机视觉任务图像分类数据集 CIFAR10, CIFAR100 和图像分类 benclunark 数据集 ImageNet-lOOOK 上进行多组 实验分析验证，同时验证算法模型的泛化性，使用语义分割数据集PASCAL VOC 进行实验分析。最后进行轻量化网络的效率分析，分别从移动设备Nvidia TX2, CPU 15-8400,和GPU设备Nvidia TiTanX进行效率的实验。
27

28


第三章 构建高效轻量型3D卷积神经网络
3.1引言
本章中将会介绍本文的研究点之一，高效型的神经网络模型用于行为识别, 动作分类等视频层级的语义理解型任务。近年来，2D的卷积神经网络已经在计 算机视觉领域取得了巨大的成功，但是2D神经网络在处理具有时序特性的视频 数据无法很好学习时序的相关性。常用的引入时间维度的方法，有双流法和2D CNN+时序神经网络的方法，但是双流法计算过于复杂，2DCNN+时序神经网络 效果有限。近几年3D卷积神经网络出现，3D卷积神经网络比2D神经网络多了 一个维度，这一个维度刚好可以用于时间维度T的学习。针对视频数据，相似结 构的3D卷积神经网络和2D卷积神经网络，3D卷积神经网络一般拥有比2D卷 积神经网络更好的性能，但是由于3D卷积神经网络本身多引入了一个新的维度， 神经网络本身的参数量将上升一个数量级，再加上本身视频数据的输入将是多张 图像帧，模型的计算量将爆炸性增长。因此，在常规的智能设备部署基于3D神 经网络的行为识别，动作分类算法将会变得困难，为了解决这个问题，本章节将 从网络的结构，网络的输入，视频的采样率三个角度进行说明阐述，引出面向智 能设备的高效型行为识别，动作分类3D卷积神经网络，用于视频中多帧图像数 据的特征提取。
3.2研究关键点
3.2.1视频数据的采样策略
视频数据是由连续帧的图像数据组成，对于长视频，往往包含大量的图像帧， 而短视频或者小视频中，所含帧数不及长视频。在长视频中，如果进行动作识别 任务，往往需要部分关键帧所提供的信息便可完成任务，若是将长视频的大部分 的图像数据都送入到神经网络模型中进行计算，计算成本将会达到在常规的智能 设备无法完成的量级，使用以往的视频深度学习算法在本文研究的智能设备中无 法适用，所以为了在智能设备的构建高效型行为识别，动作分类算法，需要制定 一些合理的视频采样策略，使计算成本和算法精度都实现可用。
•采样图像大小
29

在卷积神经网络算法模型中，输入的图像大小和神经网络模型的计算量息息 相关，在常见的二维图像分类中，往往使用更大的图像分辨率会取得更好的精度, 但是二者的相关性并不是线性增长的，如图3-1所示，在尸1.0的时候代表常见 的3*224*224的二维图像分辨率，随着r系数的增长，纵轴代表的top-1精度也 会逐步增长，但是top-1精度的增长是由代价的，横坐标轴代表着计算量的大小, 可见当取得最高的top-1精度的情况下，此时的计算量Flops将是取得76% top- 1精度的计算量Flops的大约4倍，但是精度只提高了约3个百分点，因此，制 定适合的视频图像分辨率对于本文的研究内容的意义尤为重要。

图3-1 Effcinentnet论文中对输入图像分辨率的研究，横坐标为图像大小的系数,
纵坐标为精度［42】
女口图3・2所示，在著名的ResNet卷积神经网络中，随着图像的大小的倍增， 神经网络模型的计算量将会呈现指数级别的增长，因此不能够盲目的增大图像分 辨率，尤其是在算力受限智能设备中［叫 图3-3列举出来了常见的卷积神经网络 输入图像分辨率和模型计算量的大小。
30



3-112*112	3*224-224	3«443-4^8
图3-2 ResNet系列神经网络输入图像分辨率和计算量Flops的关系





5luifHarh：v2«15	Mobtlcretv/-1.0	Rc-^^50
图3-3常见的2D卷积神经网络在不同的输入分辨率下的计算量Flops的大小 经过在实验研究尝试下，本课题采用单帧的图像分辨率为3*160*160,在 3*160*160的情况下，兼顾了模型的计算量和精度，具体实验结果将在后续实验 章节说明。下图3-4分析了常见的3D卷积神经网络的在不同的输入分辨率的情 况下，模型的计算量大小。



•关键帧的采集
视频相比于图像数据信息更加丰富，但是连续图像序列中，无用的数据过多, 所以以一种低计算成本的无监督方法去提取关键帧信息在研究内容中尤为重要, 本文采用基于帧间差分的关键帧提取方法，这种方法具有计算成本较低，方便部 署的特点。算法的大致思想如下：将相邻的两帧的图像进行差分计算，得到的差 值图像可以反映出图像变化的大小，如果前一帧和后一帧产生了较大的变化，则 认为是关键帧，这种方法在场景的变化差异大，或者是人物的动作变化大的情况 下，可以较好的提取出关键帧，去除冗余帧。在下图3-5中，出现在较高的波峰 处的为关键帧，由此可见，在一段视频中，虽然存在大量的图像帧，但是若是提 取出比较有用的关键帧，将会减少信息的冗余，对算法模型的精度帮助较大。

•视频帧采样数
在常见的以视频数据作为输入的神经网络中，采样帧数目是一个不可避免的 问题，常见的采样方法有密集采样和稀疏采样，密集采样对于短暂快速的动作行 为信息具有较好的提取效果，稀疏采样的方法通过减少神经网络的输入大小，这 样可以减少模型的计算量，由于本文研究内容主要面向于智能设备，由于密集采 样的方法将会造成算法模型的计算量大量增加，所以本文采样稀疏采样结合关键 帧采集的方法，在保证一定精度的情况下，对模型的计算量进行均衡。


图3-6轻量型卷积神经网络-3D网络输入帧数和计算量Flops的关系
3.2.2 3D神经网络的设计
大多数的3D卷积神经网络在视频识别任务上的设计都是由2D卷积神经网 络驱动设计的，如将2D的空间卷积核拓展到3D的时间空间卷积核，也有在2D 神经网络的基础上添加RNN,本文将借鉴的2D神经网络RegNet的设计，同时 结合参数量和计算量进行考虑进行调整修改，形成本文所使用的3D神经网络。
33

本文所使用3D卷积神经网络由三个部分组成，分别为stem, body, head, 其中stem部分设计由下图所示，原始的2D RegNet网络的stem部分是常见的 conv2d+BatchNorm2d+ReLU的组合，本文3D-CNN设计的stem中，将会增加一 个额外的时域的3D卷积，尝试在网络的最开始处进行时域信息的整合提取。如 图3-7所示。

图3-7 Regnet中所使用的stem部分(左图)，本文使用的3D卷积神经网络stem部分(右图)
在3D卷积神经网络的body部分主要分为两个种类，需要进行下采样的 ResBottleneckBlock,如图 3-8 所示，不需要进行下采样的 ResBottleneckBlock, 如图3-9所示，传统的Bottleneck系列设^一般是的(Conv2d + BatchNorm2d + ReLU + Conv2d + BatchNorm2d + Re LU + Conv2d + BatchNorm2d)的结构，在 这里本文将2D的卷积核进行拓展，将ReLU激活函数替换为Swish激活函数, Conv3d + BatchNorm3d + Swish + Conv3d + BatchNorm3d + Swish +
Conv3d + BatchNorm3d),其中 Swish 激活函数为 F(X) = X * Sigmoid(X),在 将2D的卷积核延展成3D卷积核时，第一个Conv3d卷积核和第三个Conv3d卷 积核T维度的T维度固定为1,在第二个卷积核上的T维度上将会在网络的不 同深度设置不同的参数，在后层一般都设置为3。在需要进行下采样的时候，另 一个分支会设置大小为t*l*l的卷积核，步长为2的卷积核，t的大小和另一路
34

图3-8 Regnet中所使用的ResBottleneckBlock-0部分（左图），本文使用的3D卷积神经网络
3D 型 ResBottleneckBlock-0 部分(右图)



图3-9 Regnet中所使用的ResBottleneckBlock-1部分（左图），本文使用的3D卷积神经网络
3D 型 ResBottleneckBlock-1 部分（右图）
35
3.3 3D卷积神经网络的整体框架
Layer
Repeat
Output Size
Input Clips


图3-10本文所使用3D神经网络结构示意图
本文所使用的3D卷积神经网络是在2D卷积神经网络RegNetX-400M〔22〕的 基础上进行调整设计过来的，整体设计结构如上图3-10所示，首先会进行 StemBlock3D进行第一次空间维度的下采样，随后特征图进入Stagel, Stagel由 ResBottleneckBlock* 1 组成，Stage2 由 ResBottleneckBlock*2 组成，之后进入 Stage3, Stage3 由 ResBottleneckBlock* 7 组成，Stage4 由 ResBottleneckBlock* 12 组成，最 后将输出的特征图进行全局池化，然后进行维度上的拉伸，向量再送入全连接层 进行分类。
3.4实验验证与结果分析
(1) SomethingSomethingV2
36

SomethingSomethingV2是一个著名的视频片段数据集，该数据集包含了大 量的预先定义的人类和日常物件进行互动的动作行为，该数据集能够让模型去学 习到对现实世界发生的动作行为进行细粒度的理解，该数据集包含220847个视 频样本，其中训练集168913个视频样本,验证集24777个视频样本,测试集27157 个视频样本，共分为175个类别，本文中，将会使用该数据集进行算法模型在视 频数据上的预训练，以方便在后续其他的任务数据集，进行迁移学习。

图3-11. Something SomethingV2数据集数据分布情况
由于Something SomethingV2数据集样本数量大，使用4块Nvidia 3090Ti进 行训练。采用了随机梯度下降优化方法，在进行Something SomethingV2的训练 时，batchsize大小为256。训练约40轮，当训练轮次为10, 25, 30的时候，学 习率衰减0.1,初始学习率大小为0.03o在训练时使用ImageNet的预训练模型 去填充2D部分的卷积核，T维度的卷积核参数进行初始化，并且使用Kinetic- 400进行少量轮数的训练，对于训练数据，采样常见的随机缩放，对每个视频进 行稀疏采样8帧进行训练，图像的大小为160*160,在Something Something V2 数据集的验证集上，实现了 top-1精度45.07%, top-5精度76.5%的精度。
37
表3-1.经典算法在Something Something V2的top-1和top-5精度以及计算量和参数量对比
Model
Top-1精度
Top-5精度
计算量
参数量
TRN-Multiscale

(2) Jester
Jester数据集〔即是目前最大的手势行为分类数据集，在这个数据集的每一个 视频数据中，一个人将会在摄像头前进行预先定义好的手势动作行为，视频背景 相对固定，数据集包含148092个视频样本，共分为27个类别，数据集同样由训 练集，验证集，测试集组成，分别包含118562个，14787个，14743个视频样本， 本次实验的测试结果将使用训练集数据训练，并在验证集上进行评估，验证。本 文将在Jester数据集上去验证3D卷积神经网络模型在局部动作上的捕捉能力。


图3-12. Jester数据集数据分布情况

数据集是在大量不同工作者的帮助下收集的。相比现有数据集的数量要大得 多。本文按&1：1的比例将测试集分为训练集、验证集和测试集。这么做的原因 是为了确保同一个人的视频不会同时出现在训练集和测试集中。视频片段持续时 间为3秒。每个片段都包含一个来自一组25个常用于人机界面的测试的真实标 签，包括一个无手势类和一个对比类。手势是动态的和运动的，在许多情况下不 能从单个帧中辨别出来。该数据集的目的是对现有的手势识别方法进行基准测试, 并帮助人们能够建立实时的端到端手势识别系统。有25个手势类和两个类。“无 手势"类别内容显示为一个人静止坐着或站着的视频。“做其他事情”类别是各种 活动的集合，如伸展、转头、咀嚼、玩头发等。数据集不含有给定手势类中所示
38
以外的动作。
在神经网络模型的训练过程中，采用了随机梯度下降优化方法。对于训练过 程中，参数设定如下：Batchsize大小设置为128,动量系数为0.9,权值衰减参 数为1x10-3。训练网络的时候使用上文SSV2数据集所得为预训练模型，学习率 初始值为0.1,每到达一定的轮次数目，学习率衰减0.1。
在进行训练的时候，将分别对视频单帧图像的大小，单个视频样本的采样数 进行变量控制实验。经过研究发现，在对单个视频进行8帧的抽样，算法模型的 精度与计算量之间的均衡可以达到最好，此时计算量达到2.3Gflops, top-1精度 达到90.9%, top-5精度达到99.10%。在抽取4帧的情况下，虽然模型的计算量 仅有0.6Gflops,伴随着代价是top-1精度下降了大约10个百分点。在采样16帧 的情况下，模型将会消耗3倍多的计算量，仅带来约4个点的top-1精度提升， 达到这种规模的计算量，在智能设备中部署是困难的。综上所述，本文选取单个 视频样本抽取8帧的采样策略，在进行推理时候，结合上述的关键帧提取技术， 可以将模型的计算量和精度达到较为理想的程度。单个视频样本上分别抽取4帧, 8帧，16帧实验的数据，如表3-2所示。
表3-2. 160*160图像大小下不同的采样率对3D-CNN模型精度核模型计算量量参数量的对
比

Top-1精度
Top-5精度
计算量Flops

在计算机视觉任务中，输入到卷积神经网络中的图像大小对于模型的计算量 也产生了重要的影响，输入的图像大小不论在2D神经网络还是3D神经网络都 是影响整个模型计算量的重要因素，神经网络的输入可以定义为B*C*T*H*W, 上述实验中，本文将算法模型的输入视频帧数，也即为T定为8,对于视频数据， 一般都为彩色图像通道数C定为3,下面本文将会探讨视频帧H和W对算法模 型的影响，本文分别在视频抽样数为4, 8, 16的三种情况下，实验了两种不同 的图像大小112*112, 160*160对3D卷积神经网络模型的精度和计算量产生的 影响。实验结果如图3-13所示，由图可发现，在增大采样数的时候，算法模型 将会出现精度的提升，但是随着计算量的指数型增长，在相同的采样数的情况下， 提升图像的大小可以带来极微小的精度提升，但是会带来计算量的成倍增长。因 而，结合上述的实验结果，本文将本文所使用的3D卷积神经网络的输入采样数
39

定义为8帧，图像的大小为112*112。


B top-1 Acc ; top-5 Acc 乩 Flops(G)
图3-13.不同的采样率和图像大小对算法模型精度和计算量的对比
表3-3.实验结孑
丘分析对比


top-1精度
计算量FIops(G)
参数量(M)
ResNetl8-3D
16*112*112

-
ResNet50-3D
16*112*112

-
SqueezeNet-3D
16*112*112

Mobilenetv2-3D
16*112*112

Shufflenetv2-3D
16*112*112

Ours
16^112*112


Ours
8*112*112


(3) UCF-101
UCF-101数据集⑷堤从大型视频网站YouTUBE收集的具有101个动作的真 实动作视频的行为动作识别数据集，包含13320个视频，采集的分辨率为320*240, 涉及运动，乐器，人物交互等行为，该数据集在数据的采集上具有很大的多样性， 包括相机运动，外观变化，物体比例变换等，是在视频动作行为识别领域著名的 基准数据集。本文将在该数据集验证3D卷积网络的迁移学习能力。图3-14. UCF-101部分类别数据分布情况
为了验证迁移学习的能力，在训练过程中，本文冻结了前面的一部分卷积层 的网络参数，然后对剩余的网络参数进行微调。本实验仍然采用随机梯度下降的 优化方法，训练以0.01的学习率开始，在训练轮数为30和45的时候，将学习 率降低，学习率衰减系数为0.1,再经过15个轮次就完成训练。
本文对比了一些其他的3D卷积神经网络，本文所使用的卷积神经网络在 UCF-101上表现出了极佳的性能，在能够以更少的计算量的情况下，仍实现较好 的算法性能。
表3-4. —些3D卷积神经网络的基于UCF-101的性能对比


top-1精度
计算量(G)
参数量(M)
ResNetl8-3D


ResNet50-3D
16*112*112


SqueezeNet-3D[47]
16*112*112

Mobilenetv2-3D1471
16*112*112


Shufflenetv2-3D1471
16*112*112

Ours


3.5本章小结
本章主要介绍本文的研究点二，面向智能设备的轻量化3D卷积神经网络。 本文从对2D的卷积神经网络进行改进的角度，将轻量型2D的卷积神经网络拓 展为轻量型3D的卷积神经网络。首先，实验分析了视频数据的输入形式对神经
41
网络计算量的影响，设计了合适的视频数据采样策略。其次，详细介绍了 3D卷 积神经网络的设计，在行为识别动作检测数据集UCF101进行实验分析，大规模 手势动作分类数据集Jester,以及大规模行为分类Something SomethingV2进行 实验分析。在此基础上设计了轻量型的视频行为识别算法，动作分类算法。最后， 本章节进行了主流算法和本文提出的轻量型3D-CNN模型的对比实验。

第四章结合知识蒸f留方法的轻量型音频分类网
络
4.1引言
在本章将介绍本文的第三个研究点，结合知识蒸馆方法的轻量型卷积神经网 络。音频分类，音频模式识别是机器学习领域一项重要的任务，包含声音场景分 类，音乐分类，语音情感分类和语音事件检测,其中音频分类是一项重要的任务。 随着深度学习技术的快速发展，基于大规模数据集的预训练模型可以很好的推广 到同一领域的其他任务上，例如图像分类领域的ImageNet预训练模型，视频分 类领域的ActivityNet预训练模型，自然语言处理领域基于维基百科的Bert的预 训练模型［佝，在音频分类领域，Google开源了 AudioSet大规模音频数据集 在相同的数据集任务上，使用预训练模型再进行迁移学习往往可以带来很高的目 标收益，但是从头进行这些大规模数据集的训练，训练成本和调参成本往往是巨 大的。本章节目标在于如何够构建出高效可用的轻量型音频特征提取网络，结合 知识蒸憾的方法，在UrbanSound8K数据集实验分析〔呦。
4.2研究关键点
•利用知识蒸憾的方法辅助声音场景分类进行训练
声音场景分类是音频模式识别任务中重要的一项任务，目标判断给出的一段 音频中的片段是否含有分类体系的音频标识。早期的音频分类算法使用人工设计 的特征作为输入，例如音频的能量，mfcc音频特征，过零率等。近年来，随着深 度学习的技术快速发展，尤其是卷积神经网络已经在计算机视觉领域取得了巨大 的突破。同时，在音频领域，过去的使用卷积神经网络CNN进行音频分类任务 分方法也使得精度性能取得了巨大的突破。在使用CNN进行音频分类的时候， 神经网络的输入往往使用log mel频谱特征，计算过程如下所述，使用短时傅立 叶变换去计算音频波形的频谱，再使用mel滤波器提取，最后取对数，便得到了 log mel 特征。
AudioSet是一个大规模的音频数据集，拥有528个类别，该数据集的音频片 段是从YouTube的视频片段中提取而来的。其中，训练集包括2063839个音频 片段，每个类别至少拥有50个音频片段，评估集拥有20371个音频片段，总大 小高达1.1T量级。同时，AudioSet数据集存在大量的数据类别不均衡的问题， 其中对话和音乐大类占比极高，如果不对数据进行采样平衡，将会造成在样本数 43
量大的类别的过拟合，和样本数量少的类别的欠拟合，许多的研究，例如有的研 究尝试了对训练数据进行balanced sampling,确保在每个mini-batch里面样本类 别分布平衡，因此在AudioSet数据集上进行调参的成本是巨大的。
UrbansoundSK是目前使用十分广泛的环境音频分类模型，共包含有8732条 标注的数据,共分为10个类别，数据采集来自城市的各种环境音，如空调声， 汽车轰鸣，儿童玩耍，犬吠等，是较为理想的验证算法模型迁移学习能力的数据 集⑷】。在实验中，本文将10%的数据划分为验证集，剩余的90%的数据用于训 练使用。在从头训练UrbanSound8K数据集时，本文使用Adam优化算法,batchsize 为64,初始学习率为0.01,训练8K次迭代。在使用AudioSet数据集的预训练 模型的情况下，本文将学习率调小为0.0001,只进行4K次的迭代。对于提取的 音频，采样率为32K,由于UrbanSound8K数据集中的大部分音频为短音频，因 而本文取每段音频的5s进行log mel特征的提取，超过5s的音频进行截取，少 于5s的音频进行零填充。
由于AudioSet数据集以上的特性，常规地从头初始化神经网络训练AudioSet 数据集的成本往往是很高的，而且若是考虑计算资源的问题，构建一个面向低算 力智能设备的神经网络，进行从头训练，将会造成大量的时间成本的浪费，而且 精度无法保证。但是缺少了 AudioSet模型的预训练，音频分类算法的精度将会 产生很大的衰退。本文使用现有存在的基于AudioSet的预训练的CNN模型，在 UrbanSound8K数据集进行微调，同时对比了不使用AudioSet预训练模型直接对 Urbansound8K进行训练，对比如下图4-1所示。由图中可知，在使用了基于 AudioSet数据集进行预训练之后，在小规模的数据集上进行微调将会有完全领先 的性能，规模较大的CNN,由于没有进行预训练，最终可能会造成过拟合现象， 验证集上面的精度下降。综上所述，说明使用AudioSet数据集预训练对于音频 分类算法必要性。

图使用大规模数据集预训练和未使用预训练在中规模数扌居集上进行训练的精度对比

表使用AudioSet数据集预训练和不使用AudioSet数据集预训练网络各项指标

without AudioSet pretrained
AudioSet pretrained Acc
计算量
Flops
参数量
Params
DaiNet


mobilenetv2　　本文为了解决以上问题,利用知识蒸憾KD的方法，巧妙地避开了对AudioSet 数据集的复杂训练过程，保证在音频分类模型在有一定的AudioSet数据集的预 训练模型的先验背景下，定制各类迁移学习的任务。整体的方法流程如下图4-2, 图4-3所示。
Transfer Learning


基于AudioSet数据集训练的模型	使用AudioSet预训练模型，在UrbanSound8K上进行finetune
图4・2.使用已经在AudioSet数据集预训练的模型在UrbanSound8K _h进行微调finetune
Knowledge Distill


在 UrbanSound8K 上进行 finetune 完成的 Teacher 模型	Student 模型
图使用已经在AudioSet数据集预训练的模型在UrbanSound8K上进行微调finetune
知识蒸憾KD目标是将将复杂或者更优的教师模型所学到参数分布，去让学
生模型进行适配学习，通常认为教师模型具有更大的容量和更好的性能，可以
容量更小，复杂度更低的学生模型提高一种类似于soft target的目标去学习。一 般情况下，在进行知识蒸憾的时候，为了得到更好的学生模型，在进行学生模型 的训练时候，一般保障学生模型的输出和数据的真是标签一致，也要保证学生模 型在最后softmax之前的输出logits,和教师模型的中相对应的logits尽量的逼 近，一般在训练学生模型的时候,loss的设计至少包含有两项，一般形式是LOSS = CE(y, p) + XCE(o(logitsstudent, T), (logitsteacher, T)) , CE 表示分类任务常用的 交叉爛函数，y, p分别为学生模型的输出和数据真实的标签，。代表softmax函 数，T是人为设计的temperature参数，用于平滑数值，入用于协调两项loss值 之间的权重比。
传统的知识蒸馄只关注了单个样本在教师模型和学生模型输出的特征的相 关性，可以认为这种相关性是低阶的，本文所使用的知识蒸憾的方法，在关注教 师模型和学生模型的输出的基础上，将会构建岀样本之间的相似性，会用cosine 函数进行度量，最后将其规范到一个概率上面。知识蒸憾整体流程如下图4-4所 示［49】。
这种方法的是为了能够让学生模型学习到教师模型的概率分布，这比仅建立 两个模型输出之间的关系更加具有鲁棒性，在训练过程中，对于每个batchsize的 数据样本之间成对的交互进行建模，使其可以学到描述高级特征空间的能力，利 用特征空间中任意两个数据点的联合概率密度，对两个数据点之间的距离进行概 率分布建模。通过最小化教师模型与学生模型的联合密度概率估计的差异，实现 概率分布学习。联合概率密度公式如下。

训练中每个batch的数据都是随机抽样的，无法使用全局的数据，因此需要 使用条件概率密度代替联合概率密度的分布。在计算了每个batch数据的条件概 率密度之后，通过最小化教师模型的条件概率分布和学生模型的条件概率分布的 KL散度，实现概率上的知识蒸憎。使得学生模型可以学习到这类更高阶的相关 性⑷］。KL散度的公式如下所示。



图4-4.知识蒸憎方法细节图[49]
4.3实验验证与结果分析
首先，本文使用RegNetX-400作为芦音场景分类的学生模型，RegNetX-400 M是优秀的卷积神经网络的特征提取网络，在计算机视觉领域著名的ImageNet 图像分类任务上，拥有不俗的成绩，如下表4-2所示。
表4-2.不同卷积神经网络的在图像分类上的性能对比

首先本文在UrbanSound8K数据集上从头开始训练RegNetX-400M,音频的 采样率为32K,在提取音频的log mel特征时，hop size设置为320, Mel bins设 置为32,以减少计算的复杂度，使用Adam优化算法，初始学习率设置为0.1, 每2000次迭代学习率衰减0.1,共迭代10K次达到收敛。未经过AudioSet数据 集的预训练，从头开始训练得到的UrbanSound8K的准确率为78.3%,计算量仅 只有0.8G,参数量为5.2M,作为知识蒸憾方法的对比基准baselineo
然后，本文使用经过AudioSet数据集预训练的CnnlO作为教师模型，在使 用CnnlO模型进行UrbanSound8K数据集的迁移学习训练时候，hop size仍为设 置为320, Mel bins设置为32,使用Adam优化算法，初始学习率设置为0.001, 每2000次迭代学习率衰减0.1,共迭代6K次达到收敛。CnnlO在UrbanSound8K 上达到91.4%的准确率，计算量约为11.2G,参数量约为5.22M。
47
最后，本文进行教师模型向学生模型的知识转移学习训练。训练时，batchsize 的大小设置为32,学习率设置为0.001,训练8K次迭代。实验结果如下表4-3所 示⑴。
表4-3.不同卷积神经网络的在音频场景分类上的性能对比

未进行知识蒸徭精度对比
进行知识蒸馆后精度对比
计算量
参数量
DaiNeta】
由图表可知，利用构建样本相似性的知识蒸憾的方法，可以有效的解决缺少 大规模数据集模型预训练的造成的精度下降问题。同时这种方法可以避过在大规 模数据集上训练卷积神经网络模型的复杂过程，使整个训练周期缩短了几十倍。 本文在构建轻量型的卷积神经网络模型用于声音场景分类任务时，通过利用这种 方法，顺利解决了精度和训练成本这两大重要的问题。
4.4本章小结
本章节介绍了对如何利用视频中音频模态的数据，以及音频分类的常用方法。 首先，介绍了本文为了在大规模数据集进行预训练的重要性，进行了详细的实验 测试。然后，引出本章的主要内容，如何解决在声音分类任务上大规模数据集训 练困难的问题。为了解决该问题，介绍了本文使用的结合知识蒸憎的音频分类模 型的训练方法，将会音频场景分类数据集UrbanSound8K进行实验分析，验证模 型和训练方法的可靠性。
48
第五章用于视频描述任务的轻量型多模态特征
融合网络
5.1引言
对于任意一段视频流数据，利用神经网络模型去自主地生成一段概括该视频 片段的语句，被称为视频描述任务。不同于对图像的检测和分割，视频描述任务 要求算法模型能够学习到视频内容的主体的行为，动作，以及主体和其他人或者 物体之间的关系和行为，即为让神经网络模型去理解视频内容的关键内容并利用 语言反馈回来，视频描述任务可以很好地把计算机视觉任务和自然语言处理处理 任务连接起来，从视觉任务的感知级别的语义信息提升到更深层的理解级别的语 义信息。
目前，医学领域已经证实人类的大脑不是单模态并行的，不同类型的信息载 大脑中的信息处理的时候是存在相互影响的。受这种启发，在人工智能领域，很 多任务效仿人脑的工作方式，进行多种模态的信息处理，并通过各种方式进行融 合，实现目标要达到的效果。以往地视频描述任务中，往往是仅仅利用视频的图 像信息，进行信息的提取，而人类在观察世界的时候，往往是眼睛和耳朵并用， 分别代表着视觉信息和听觉信息。因而，本文在视频描述任务中，将会利用视频 数据的三种模态信息，分别是图像模态，视频模态，音频模态，进行特征的提取， 提取出来的特征表征分别代表着视觉，动作，音频语义信息，并将各类的语义信 息进行融合，提取出最终需要的结果。
因而，利用在视频描述任务上的实验效果可以很好的反映算法模型的对视频 内容理解的程度，本章节主要在面向智能设备上，如何利用采集到的环境视频中 多种模态的信息提取到的特征表征，并将多种模态的特征进行融合，从而更好地 进行视频描述任务。
5.2跨模态注意力融合机制
近年来视频描述任务，可以看作是一种机器翻译任务的变种，输入是一系列 在视频数据中抽取的特征序列，输出结果是一段自然语言，过去大量的工作都是 基于视觉信息，但是大多数的视频数据都包含多种模态的信息，比如图像,音频， 文本。特别是，视频中主体产生音频信息，可能对内容产生重要的影响。比如， 当有人从另一边敲门时，视觉信息中只看到门，但音频信息可以帮助本文理解有 人在门后面，想要打开它进入。因此，单模态的信息的作用十分有限。此外，部
49

分类型的视频（如教学视频、体育视频或视频讲座）仅仅使用单模态对于视频描 述任务是具有挑战性。因此，本文考虑利用视频中的视觉信息和听觉信息，构建 出一种跨模态的自注意力机制模型进行更为有效的视频描述任务。
跨模态自注意力机制的编码器部分如下图5-1所示，共用3种模态的输入， 分别是经过位置编码的图像特征厂，动作特征JM,音频特征1/人，这些特征将会 经过N个encoder部分,encoder中包含三个部分，自注意力模块selfattention[53], 跨模态注意力模块cross-modal attention,以及前馈神经网络韶分。首先，各个模 态的特征会进行位置编码，各自进入各自模态的多头注意力机制模块，这里的多 头注意力MultiHeadAttention模块和原始tansformer的多头注意力模块形式一- 致，然后经过残差连接的结果进行拆分，一部分进入跨模态注意力机制，每个模
态的操作均是一致，最后在进入前馈神经网络部分。
Vfeif = MultiHeadAttention(V!, V1, V,y)	(5-1)
氏疔=MultiHeadAttentlon(VM, VM, VM)	(5-2)
黑if = MultiHeadAttention(yA, VA, VA)	(5-3)
Vcross = MultiHeadAttention(V'elf, V^,	(5-4)
V#oss = MultiH eadAttention(V^,	(5-5)
= MultiHeadAttention(y^f 如咯)	(5-6)
V}c = PC^ross)礙(喘 ss)	VA = FC(V40SS) (5-7) 在decoder部分，由encoder部分输出的特征序列和词向量来到decoder中的 进行cross modal attention的操作，三个部分输出的特征序列进行拼接降维，完成 decoder部分的操作，最后经过全连接层和softmax层进行最终结果的输出。


图5-2.跨模态注意力机制解码器decoder部分

5.3用于视频描述任务编码器-解码器结构
近年来，视频描述技术已经取得了很大的进步，但是主流的方法仍效率较低。 在视频描述领域，现在主流的方法是encoder-decoder框架，视频数据通常由编码 器部分，一般是由卷积神经网络构成，编码为序列向量，视频的描述结果通常通 过解码器部分，一般是由循环神经网络构成。当使用RNN的结构的情况下，单 个序列输入的当前的输出与过去的输出均有相关，确切的表现方式是神经网络模 型会对前面的信息进行记忆保存，保留在神经网络的内部状态中，并在计算当前 输出的时候被使用，即神经网络的隐藏层之间的节点不再是孤立的而是有相互的 关联性，并且隐藏层的输入不单单包含输入层的输出同时还包含了上一时刻隐藏 层的输出。近来兴起的Transformer模型，解决了 RNN时序依赖的问题， Transformer不依赖于过去的隐藏状态来捕获对先前单词的依赖性，而是整体上 处理一个句子，以便允许并行计算，减少训练时间，并减少由于长期依赖性而导 致的性能下降。
Transformer结构近年来在自然语言领域取得了巨大的成就，也有很多的其他 的领域采用transformer的方法去解决问题，但是Transformer在很多领域取得了 巨大的成果，但是弊端也存在。Transformer的参数量巨大，Transformer主要由 多头自注意力模块和前馈神经网络模块组成，其中多头自注意力机制的计算成本
51

是n的2次方，这种量级的计算量显然无法在低算力设备上部署，因此原始的 Transformer结构进行多模态的特征的融合是不现实的⑴］。为了解决这个问题， 本文将跨模态的自注意力机制和Transformer的轻量型变体DeLight52进行结合, 构建出跨模态的轻量型自注意力机制的结构用于视频描述。图5-3为原始的 Transformer架构，图5-3为Transfomer的轻量型变体DeLighB】。


图5-3.原始的Transformer结构°习
DeLighT的核心是DExTra转换，使用带有扩展-缩减策略的组线性转换来有 效地改变DeLighT块的宽度和深度。由于这些转换本质上是局部的，所以DExTra 使用特性变换（类似于卷积网络中的通道变换）来在不同组之间共享信息。这种广 泛而深入的表示方式有助于用单头注意力和轻量级前馈层替换Transformer中的 多头注意力和前馈层，从而减少了总网络参数。DExTra将一个％维的输入向量 映射到一个高维空间，然后使用N层的组变换将其缩减为一个D。维的输出向量 （缩减）。在这些扩展和缩减阶段，DExTra使用组线性变换，因为它们通过从输入 的特定部分导出输出来学习局部表示，并且比线性变换更有效。为了学习全局表 示，DExTra使用特征变换进行组线性变换，在不同组之间共享信息

提高Transformer的表现力和容量的标准方法是增加输入维度。机。但是，线 性增加％也会增加标准Transformer中多头注意力的操作n2Dm,其中n是序列 长度。相比之下，DeLighT为了增加的表现力和容量，通过扩展和缩小阶段来增 加中间层的深度和宽度。这使本文能够使用更小的维度来计算注意力，需要更少 的操作。

图5-4. DeLighT结构
假设本文有一个n个输入序列，每个序列的维数为。冲，这些n个Dm维的输 入首先被输入到DExTra变换以产生n个D。维的输出，其中D。＜ Dm.然后使用 三个线性层同时投影这些n个D。维的输出，以产生D。维的查询0、键K和值几 然后，使用缩放点积注意力为这n个序列之间的上下文关系建模。为了使用残差 连接，该注意力操作的D。维输出被线性投影到D讥维空间。DeLighTblock计算成 本比标准Transformer降低了響倍阿。
在前馈神经网络中，Transformer中的FFN相似，包含两个线性层。由于
53
DeLighT块已经使用DExTra变换合并了更广泛的表示形式，因此可以在变换中 反转FFN层的功能。第一层将输入的维数由降维到。机〃，第二层将输入的维 数Dm/r扩展到D机，其中r为降维因子，比如把维数减少了 4倍，相当于参数量 减少了 16倍呵。
本文中，将结合DeLighT和上述部分的跨模态的注意力机制，构成了本文使 用的结合跨模态自注意力机制的轻量化的编码器-解码器结构。具体操作如下所 述，本文将多头注意力机制模块和跨模态注意力机制模块在原有的Transformer 形式上进行改进，改成DeLighT版本的多头注意力机制模块和跨模态注意力机 制模块。首先，将输入的各个特征序列进行升维，然后利用N层的群变换进行降 维，之后进行自注意力操作，在将维度升为原始的特征维度,送入前馈神经网络， 前馈神经网络中进行降维再升维，以到达减少参数的目标。
5.4实验验证与结果分析
ActivityNet Captions dataset®],被认为是视频描述任务的重要数据集。不同 于传统的视频描述任务，该数据集用于密集视频描述任务，给予一段长视频，需 要模型去定位事件的发生并且进行视频片段描述，该数据集包含大约2万个来自 YouTube的视频，并分成50/25/25%的部分，分别用于训练、验证和测试。每个 视频平均包含3.65个视频片段的描述描述，每个大约13.65个单词。此外，验证 集中的每个视频都由不同的标注者进行了两次注释。本文使用验证集进行模型性 能的验证。大多数的视频包含众多活动。例如，在“人弹钢琴”的视频中，视频可 能还会包含另一个“人跳舞”或“人群鼓掌”。
本文中，本文仅使用该数据集进行视频描述任务，不进行事件检测任务，在 进行多模态融合模型的训练时，为了验证融合模型的有效性，需要保证特征的泛 化性和鲁棒性。为了进行更加公平的比较，本文使用的经过预训练的模型进行基 础特征的提取，本文使用的由M-SOSAnet提取的图像特征，动作特征上，本文 使用在kinetic上进行预训练I3D模型，音频特征上，使用在AudioSet ±进行预 训练的VGGish模型。在衡量视频描述性能的时候，本文使用常见的指标 METEOR, BLEU@3, BLEU@4。
训练细节如下：训练时，使用batchsize为64,采样正则化操作，本文使用 0.5的dropout和0.0005的权重衰减，使用Adam优化算法,初始学习率为0.005。 首先，由于不同的模态的特征大小不一致，为了解决这个问题，将各种特征进行 填充，以匹配特征大小最大的特征的维度。同时，由于英语单词可能存在多个同 义词或者标注存在错误，使用label smooth的数据增强方法缓解这种问题。当一
54 次输入整个特征序列，需要去预测下一个位置的单词，由于基于自注意力机制的 结构关注整体，为了防止在训练神经网络过程中从算法模型能够从整体看到下个 位置的信息，在解码器的t之后的位置进行掩码mask的操作。在视频描述生成 的训练中，为了形成一个批次，所有的特征被填充到同一批次中最长的序列。 Decoder部分采用1层解码器。
表5-1跨模态型视频描述算法性能对比


本文对比了仅仅使用自注意力机制和联合使用自注意力机制和跨模态注意 力机制的影响，证实了使用跨模态注意力机制的有效性。将跨模态注意力层替换 为自注意力层对比实验结果如下表5-2所示。
表5-2是否使用跨模态注意力层性能对比

BLEU@3
BLEU@4
METEOR
self-attention

self-attention+cross modal attention

本文还将对各种融合模型在不同设备上的效率进行对比，各个模型的基础特 征均相似，以下的效率对比，均指多模态融合模型的效率，不包含前置模型特征 提取的时间，具体如下表5-3所示。
表5-3.模型的实际效率对比

GTX3090ti
(Pytorch FP32)
Iashin [刃
Avg 90ms
Ours
Avg 50ms
由以上的图表可知，本文的跨模态自注意力机制结构的编码器-解码器的实验 结果在视频描述上取得了不错的性能，参数量要比原始的Transformer方法或者 是RNN方法减少一倍左右。更加有利于在智能设备的上的部署和使用，在效率 方面，也能够快于传统的RNN结构和Transformer结构。

5.5本章小结
本章将介绍本文研究点四，搭建面向智能设备的多模态神经网络融合模型, 去进行视频描述的任务。本章节首先介绍了如何利用不同模态的数据特征进行特 征融合，从而更好地去指导视频理解任务的执行。然后，介绍了本文构建的跨模 态轻量型注意力机制模型，依据多模态融合改进多头注意力机制，构建了跨模态 的多头注意力机制，并和Transformer的变体DeLighT结合，构建出了跨模态轻 量型的编码器-解码器架构。最后进行实验分析，通过，在视频描述数据集上进行 实验分析，验证模型的准确度和效率，说明跨模态轻量化融合模型的可靠性。

第六章总结和展望
6.1总结
随着深度学习技术和硬件算力设备的提升，以及各种类型视频数据爆炸性增 长，以往基于图像的感知技术已经无法满足智能设备的智能化增长需求。现有的 面向智能设备的环境感知和理解算法，往往是利用边缘侧设备，进行数据的采集， 然后将采集得到的视频数据打包发回服务器进行算法处理，得到结果之后进行返 回，这种方式往往无法实时处理视频信息，在数据传输延时大和一些实时性要求 高的场景下无法使用部署。基于这些因素，本文提出了一种面向智能设备的环境 感知和理解算法。该算法系统旨在利用环境中采集的视频数据多种模态的信息， 同时考虑到算法模型的使用效率，进而实现在智能设备部署的目标，同时保证精 度的可用性。具体言之，本文的主要工作和成果如下：
(1) 在处理环境采集的视频的单帧图像数据的特征，本文提出了面向低算力 设备的轻量型卷积神经网络M-SOSANet,介绍了该网络的两大模块分别是 MEESP下采样模块和SOSA模型的设计特点和优势，将会进行常见计算机视觉 任务图像分类数据集CIFAR-10, CIFAR-100,图像分类领域benchmark数据集 ImageNet-lOOOK上进行多组实验分析验证使用该网络模型进行图像分类的实验, 在 CIFAR-10 达到了 95.64%, CIFAR-100 达到了 78.6%, ImageNet-1000k 达到了 72.8%的精度，仅有290M的计算量消耗；为了验证网络的泛化性，使用该网络 进行语义分割的实验，在PASCAL VOC 2012上达到了 67.8%的mIOU,仅有 0.82BFlops的算力消耗；为了分析神经网络在真实设备上的实时性，分别在 TITANX, 15-8400,取得了不错性能。
(2) 针对环境中采集的视频数据的处理问题，本文对2D的卷积神经网络进 行改进，将轻量型2D的卷积神经网络拓展为轻量型3D的卷积神经网络特征提 取网络，从视频数据的输入策略到网络的设计，进行分析实验。在此基础上设计 了轻量型的视频行为识别算法，动作分类算法，在SomethingSomethingV2进行 实验，达到了 45.07%的精度，相比一些其他算法，计算量和参数量均下降至少8 倍；为了验证网络对局部动作信息的提取能力，在大规模手势分类数据Jester进 行实验验证，达到了 90.1%的精度；为了验证网络的迁移泛化能力，在UCF-101 进行实验，最终达到了 87.4%的top-1精度。
(3) 为了利用采集视频数据中的音频模态数据，同时为了解决大规模数据集 训练困难的问题，本文使用的结合知识蒸憾的音频分类模型的训练方法，将会音
57 频场景分类数据集UrbanSoundSK进行实验分析，验证模型和训练方法的可靠性。 最终神经网络精度达到了 92.3%,计算量仅由0.8GFlops,参数量5.2M。
(4) 针对采集的环境视频数据，搭建了多模态神经网络模型的视频描述框 架，研究了如何利用不同模态的数据特征进行特征融合，从而更好地去指导视频 理解任务的执行，重点介绍跨模态的注意力机制，以及transformer的轻量化，并 在视频描述数据集上3S行实验分析，验证模型的准确度和效率。其中BLEU@3 达到了 4.59, BLEU@4达至U 1.97, METEOR达到了&96,参数量仅有30M。证 明了使用本文的多模态特征提取网络和融合网络的性能。
6.2未来工作展望
由于本文的研究难点在于如何在保证精度情况下，减少模型的计算量和参数 量以方便模型更加容易部署在智能设备中，用于环境的感知和理解，因而在神经 网络模型的整体精度方面，还存在一定的落差。可从两个方面进行改进，在基础 模型的上面，图像模态数据经过了 ImageNet大规模数据集的预训练，可以抽取 到相对鲁棒的特征，但是如果在更大规模的视频类型的数据集上进行预训练，比 如Kinetic数据集，预估可以抽取到更加鲁棒的图像特征，由于实验中的算力资 源和时间有限，本文融合模型实验部分的视频特征和音频特征分别是使用了在 Kinetic数据集进行预训练的I3D模型抽取的特征，在AudioSet大规模数据集上 进行预训练的音频特征，因为在实际地使用中，仍然需要对本文的轻量型模型进 行大规模数据集的预训练，最终融合模型的精度可能稍微下降，但是整体的多模 态特征会互补辅助，防止性能的衰退。除了以上所述的内容之外，在由足够的算 力保证下，还可以结合目前先进的神经网络结构搜索技术，进行更加精细化的神 经网络模型的自动化设计。
在模型的效率上面，本文只对模型的结构方面进行了轻量型化的设计和改造, 但是神经网络模型优化和加速的方法仍有很多，在模型的结构设计改造完成之后, 可以进一步对网络进行剪枝和量化的操作，经过这些操作之后，神经网络模型能 够在计算量进一步减少，效率上大为提高。

参考文献
[1] 李江旳，赵义凯,薛卓尔，蔡铮，李擎.深度神经网络模型压缩综述[J].工程科学学 报,2019,41(10):1229-1239.
[2] Simonyan K , Zisserman A . Very Deep Convolutional Networks for Large-Scale Image Recognition [J]. Computer Science, 2014.
[3] YL Cun, JS Denker, SA Solla. Optimal brain damage[M]. Morgan Kaufmann Publishers Inc. 1990.
[4] Han S, Pool J, Tran J, et al. Learning both Weights and Connections for Efficient Neural NetworksfJ]. MIT Press, 2015.
[5] Wen W,Wu C ,Wang Y , et al. Learning Structured Sparsity in Deep Neural Networks[C] Advances in Neural Information Processing Systems (NIPS). 2016.
[6] Molchanov P , lyree S , Karras T , et al. Pruning Convolutional Neural Networks
'■A
for Resource Efficient Transfer Learning [J]. 2016.
[7] He Y, Lin J ? Liu Z , et al. AMC: AutoML for Model Compression and Acceleration on Mobile Devices [J]. 2018.
[8] Dettmers T . 8-Bit Approximations for Parallelism in Deep Learning [J]. 2015 ・
[9] Courbariaux M , Hubara 15 Soudry D ? et aL Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or-l[J].2016.
[10] Rastegari M 5 Ordonez V, Redmon J , et al. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks [J]. Springer, Cham, 2016.
[11] Zhou A, Yao A, Guo Y ? et al. Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights. 2017.
[12] Denton E, Zaremba W5 Bruna J,et al. Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation. MIT Press, 2014.
[13] Hinton G? Vinyals O, Dean J. Distilling the Knowledge in a Neural Network[J]. Computer Science, 2015, 14(7):38-39.
[14] Romero A ,Ballas N ,Kahou S E , et al. FitNets: Hints for Thin Deep Nets[J]. Computer ence, 2015.
[15] Gupta S, Hoffman J ,Malik J ・ Cross Modal Distillation for Supervision Transfer [J]. IEEE Computer Society, 2016:2827-2836.
[16] Howard A G , Zhu M 5 Chen B ? et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications [J]. 2017.
[17] He K, Zhang X , Ren S , et al. Deep Residual Learning for Image Recognition[J].
59
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[18] Sandler M，Howard A ? Zhu M, et al. MobileNetV2: Inverted Residuals and Linear Bottlenecks[J]. IEEE, 2018.
[19] Zhang X, Zhou X, Lin M, et al. ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices [J]. 2017.
[20] Ma N ? Zhang X , Zheng H T , et al. ShuffleNet V2: Practical Guidelines for Efficient CNN Arcliitecture Design [J]. European Conference on Computer Vision, 2018.
[21] Tan M, Le Q V. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks[J]・ 2019.
[22] Radosavovic I , Kosaraju R P ,Girshick R, et al. Designing Network Design Spaces[C]// 2020IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).IEEE, 2020.
[23] Karpathy A, Toderici G , Shetty S , et al. Large-Scale Video Classification with Convolutional Neural Networks[C]// Computer Vision & Pattern Recognition. IEEE, 2014.
[24] Simonyan K 5 Zisserman A . Two-Stream Convolutional Networks for Action Recognition in Videos [J]. Advances in neural information processing systems, 2014,1.
[25] Tran D ? Bourdev L , Fergus R , et al. Learning Spatiotemporal Features with 3D Convolutional Networks [J]. IEEE, 2015.
[26] Wang L ? Xiong Y ? Wang Z 5 et al. Temporal Segment Networks: Towards Good Practices for Deep Action Recognition[J]. Springer, Cham, 2016・
[27] Girdhar R , Ramanan D , Gupta A ? et al. ActionVLAD: Learning spatio-temporal aggregation for action classificationfJ]. IEEE, 2017.
[28] Carreira J , Zisserman A. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset[J]. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[29] Qiu Z , Yao T, Mei T ・ Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks[C]// 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.
[30] Xie S ? Sun C , Huang J, et al. Rethinking Spatiotemporal Feature Learning: SpeedAccuracy Trade-ofife in Video Classification[J]. 2017.
[31 ] Du T, Wang H, Torresani L, et al. A Closer Look at Spatiotemporal Convolutions for Action Recognition[C]// IEEE/CVF Conference on Computer Vision and Pattern
60
Recognition. 0.
[32] Ha Q 5 Watanabe K, Karasawa T, et aL MFNet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes[C]// 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017.
[33] Huang G, Liu Z, Laurens V, et al. Densely Connected Convolutional Networks]』]. IEEE Computer Society, 2016.
[34] Mehta S 3 Rastegari M ? Shapiro L ? et al. ESPNetv2: A Light-Weight, Power Efficient, and General Purpose Convolutional Neural Network[C]// 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.
[35] Zhang T , Jiao J , Zhang C , et al. M-Sosanet: An Efficient Convolution Network Backbone For Embedding DevicesfC]// 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 2020.
[36] Lee Y , Hwang J W , Lee S 5 et al. An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection [J]. IEEE, 2019.
[37] Chen L C , Papandreou G , Schroff F , et al. Rethinking Atrous Convolution for Semantic Image Segmentation [J]. 2017.
[38] Jia D, Wei D, Socher R, et aL ImageNet: A large-scale hierarchical image database [J]. Proc of IEEE Computer Vision & Pattern Recognition, 2009:248-255.
[39] Devries T , Taylor G W . Improved Regularization of Convolutional Neural Networks with Cutout[J]. 2017.
[40] Ke S . IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks. 2018.
[41] Hoiem D , Diwala S K , Hays J H . Pascal VOC 2008 Challenge, world literature today, 2009.
[42] Tan M ? Le Q V. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks[J]. 2019.
[43] Goyal R , Kahou S E , Michalski V , et al. The nSomething Something11 Video Database for Learning and Evaluating Visual Common Sense[C] 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.
[44] Materzynska J , Berger G , Bax I , et al. The Jester Dataset: A Large-Scale Video Dataset of Human GesturesfC]// 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). IEEE, 2019.
[45] Soomro K , Zamir A R, Shah M . UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild[J]. Computer Science, 2012.
61
[46] Feichtenhofer C .X3D: Expanding Architectures for Efficient Video Recognition [J]. IEEE, 2020.
[47] Kopuklu O , Kose N , Gunduz A , et al. Resource Efficient 3D Convolutional Neural Networks[C]// 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). IEEE. 2019.
[48] Devlin J , Chang M W , Lee K , et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding^]. 2018.
[49] Passalis Na Tefas A. Learning Deep Representations with Probabilistic Knowledge Transfer卩].Springer, Cham, 2018.
[50] Kong Q , Cao Y, Iqbal T , et al. PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition [J]. 2019.
[51] I ashin V , Rahtu E . A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer [J]. 2020.
[52] Mehta S , Ghazvininejad M , Iyer S , et al. DeLighT: Very Deep and Light-weight Transfbmie匚 2020.
[53] Vaswani A, Shazeer N, Parmar N, et aL Attention Is All You Need[J]. arXiv, 2017.
[54] Shah A, Kesavamoorthy H , Rane P , et al. Activity Recognition on a Large Scale in Short Videos - Moments in Time Dataset・ 2018.
62
