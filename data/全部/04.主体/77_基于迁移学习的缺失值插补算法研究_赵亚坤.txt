第一章绪论
1.1	研究背景与意义
当前随着信息的快速发展，互联网，大数据等新技术已经深入到人们生活的 方方面面。数据是信息时代的血液，已经成为了经济、国防、日常生活中最重要 最有价值的资源。在互联网的大环境下，数据获取越来越方便，数据量随着时间 呈指数型增长。然而数据的复杂化对数据管理造成了极大的威胁，尤其是在互联 网的大背景下，信息交流更加频繁，使得这一趋势愈发明显。例如多源数据融合 或者违法记录等问题，导致人们不能完全相信所拥有的数据。比如商家使用了错 误的消费信息会将错误的商品推荐给用户，导致了商铺收益的下降；医疗数据的 不规范记录会导致医疗数据不可靠，耽误了医学研究进展和患者治疗的进程；低 质量的客户数据仍然对美国造成大约6110亿美元的损失。在各个领域中，数据 不准确都浪费着大量的财力物力。如何利用好这些数据，使其为我们的生产做出 贡献，是我们作为大数据挖掘工作的重点。而想要做出有价值的成果必须采用高 质量的数据，数据预处理是数据挖掘数据分析中的第一步也是至关重要的一步。 数据预处理的好坏直接影响到算法实现的性能。
在数据预处理的过程中，对于缺失值的处理又至关重要。在实际生活中，数 据集有缺失非常常见。比如美国的Honeywell公司的设备维护和测试用数据库缺 失值高达50%,医疗领域为了保护患者隐私，或者由于记录的不规范，数据丢失 率能够高达60%以上。数据缺失意味着信息缺失和统计特征丢失，而数据分析是 建立在统计学、信息学上的学科，这些缺失值不仅意味着信息的空白，其对后续 数据挖掘等工作的进行也会造成无法挽回的损失。缺失值给算法会造成极大的影 响，比如在聚类算法中，缺失值会影响距离函数的度量，这会影响最终的聚类结 果；在决策树分类算法中，缺失值会影响叶子结点的划分；在关联规则算法中， 缺失值会影响置信度的计算结果，影响了算法挖掘关联规则的能力。可见，无论 是何种情况，一旦数据发生缺失，都会给数据分析带来诸多困难。因此，如何减 少因数据缺失对后续数据挖掘研究工作影响，并且我们如何避免数据缺失导致的 错误结论，是当下数据科学研究的重要方向之一。
数据缺失问题可以说是无处不在，当然数据分析工作者一直以来都在关注这 个问题，也在从各个方面积极的寻求方法解决。对于缺失值问题，前人有着非常 丰富的相关工作。有基于参数估计的方法，有基于统计的方法，还有基于数学矩 阵的方法等等。这些方法在不同领域不同场景能够相应的达到相对于不插补更加
优良的性能。当然，传统的方法依旧具有局限性。传统的缺失值插补都是基于数 据集中已有的信息，对缺失值所在的缺失信息进行插补。值得注意的是，依据香 农的信息论，丢失的信息是无法挽回的，传统的缺失值插补方法仅仅是针对后期 数据分析的步骤，减少缺失信息所造成的噪声影响，达到提升性能的效果，而并 没有真正的将缺失的信息给插补上。这种方法在大部分场景下都适用，然而当缺 失信息远大于已有信息时，这种方法的效果会急剧下降并逼近未插补缺失值的方 法。因为此时缺失值造成的噪声影响已经大于仅存数据包含的信息了，数据分析 模型无法学习到有效的信息，使得最终结果不好。对于此类问题，迁移学习提供 了一种独特的思路来解决。我们可以通过寻找一个与含有缺失值的数据集（下称 目标数据集）相似的源数据集，来帮助我们训练一个更好的缺失值插补模型，通 过引入源数据集中包含的信息来插补现有数据集中的缺失值，这样从根本上提高 了整个数据集的信息量，对于缺失值比例过大的数据集也适用。
本文的工作主要在于利用外部的源数据集，来提高缺失值比例过大的数据集 中缺失值插补的问题。并与传统缺失值插补方法进行比较，对结果进行评价和分 析。
1.2	缺失值处理的相关介绍
1.2.1	缺失值的定义及产生原因
缺失值指的是粗糙数据集中，由于信息缺少导致的数据不完整的部分。数据 缺失在多个领域都是一个棘手的问题，对数据分析来说，数据缺失常常会有以下 影响：系统丢失了大量的有用信息；不确定因素在系统中越来越显著，导致系统 中的确定性成分很难被利用；缺失的数据会使挖掘过程陷入混乱，导致无可信度 的输出。在实际操作过程中，数据分析算法并不能使得数据完全拟合所需模型， 这使得它没有办法处理不完整数据。因此，缺失值需要通过专门的方法进行填充， 以减少数据挖掘算法与实际应用之间的差距。
完整的数据集是最完美的，然而实际生产过程中这样的数据并不多见。无 论是在电脑磁盘里存放的普通数据，还是存在于如MySQL、Oracle等数据库中 的重要数据，都会存在不同程度的缺失现象。造成数据集包含缺失值的原因众 多，根据产生来源可分为人为产生的缺失值和非人为产生的缺失值。人为产生 的缺失值常常是由于录入人员未能够发现该数据的重要性，或者是录入错误导 致的无法读取，也有可能是录入人员并不知道该数据可以填入什么，导致的该 数据为空值。非人为因素产生的缺失值也有很多种可能，通常有以下几点：
2
1）硬盘、计算机等机器故障常常能够导致大批量的数据产生缺失，例如某大 型电商公司数据库由于云平台机器故障或者断电导致的硬盘磁头损坏，从 而进一步导致数据缺失；
2）某些信息暂时无法获取，比如电商想要知道用户未来的特征信息但无法提 前获取；
3）某些数据收集的代价过高所导致的未能够良好记录。例如想要知道深海中 的某一些属性，可能需要大量的人力物力来探索，导致该类数据只能暂时 缺失。
1.2.2	数据缺失的类型
数据缺失的机制与缺失值的处理策略有关，研究数据缺失的类型，就是在研 究缺失数据同数据集中其他属性变量的关系。Rubin臼首次对数据的缺失机制做 了详细的划分，他按照数据的缺失值是否是随机产生的而将缺失值分为三类：完 全随机缺失（Missing Completely at Random）、随机缺失（Missing at Random） 和完全非随机缺失（Not Missing at Random）。具体阐述如下：
1）完全随机缺失（MCAR）,指的是数据的缺失完全是随机的，完全不受其他 因素影响。这是一个理想的状态，实际生活中并不存在此类缺失情况，但是 我们可以将很多情况近似为此种情况，例如由于机器故障造成的数据缺失就 可以看作是完全随机缺失；
2）随机缺失（MAR）,基于该缺失机制的数据缺失是最常见的缺失方式，它指 的是观测数据的缺失与数据集本身有一定的关系，而并不是完全相关的关系, 数据缺失带有一定的随机性。例如由于患者身体原因记录的患者某身体特征 缺失；
3）完全非随机缺失（NMAR）,数据缺失仅与数据本身的属性有关，而不带有 随机性，例如高收入的人群不愿意透露自己的收入信息而导致的信息录入缺 失。
1.2.3	缺失值处理的必要性
数据缺失对于数据挖掘过程来说，会造成巨大的影响。如何有效的处理缺失 值，显得至关重要。缺失值对于数据挖掘模型造成的影响，可以分模型种类来讨 论：
1）对于逻辑回归，支持向量机等线性模型，数据缺失值相当于给数据引入了巨 大的噪声，有可能造成数据线性不可分，使得模型分类出现错误；
2）对于神经网络等深度学习模型来说，缺失值可能会导致参数迭代方向发生错 误，导致最终训练出的模型不能拟合真实的场景；
3）对于树模型例如GBDT或者随机森林来说，如果数据集中包含大量缺失值的 话，决策树在划分节点的时候往往不能找到合理的划分，随着缺失值比例的 上升，算法的准确性能快速下降。
文献⑵分析了缺失数据对于数据挖掘的影响：机器学习模型通常的任务是利 用数据中的信息来判断未知内容的，所以当输入的数据含有大量的缺失值的时候, 模型无法工作，不能够准确的把握数据真实的分布情况，也无法产生好的效果， 因而无法建立可靠的算法模型。
综上，如果我们不能采取一个有效的缺失值插补方法，将会对我们的数据挖 掘过程造成极大的影响。所以，研究一个更优良的缺失值插补算法是非常有必要 的。
1.3	国内外缺失值插补研究现状
对于缺失值处理，近数十年以来，国内外也是有大量深入的研究。由于在插 补完全非随机缺失类型的数据时，比较依赖主观因素与先验知识，导致所采取的 方法往往不能够通用，所以在前人的研究中对于缺失值插补算法的研究主要集中 在随机缺失和完全随机缺失数据类型上。由于随机缺失数据集的状态是介于完全 随机缺失和完全非随机缺失之间，所以随机缺失包括了二者的优点和缺点，没有 其典型特征。所以在接下来主要讨论完全随机缺失这种类型。传统的缺失值插补 算法中，删除法实现最简单，也使用范围最广。简单删除法的原理就是将数量较 少的缺失值直接删除，传统的缺失值插补算法中，删除法实现最简单，也使用范 围最广。简单删除法的原理就是将数量较少的缺失值直接删除，使缺失值不影响 数据分析算法在数据集上的整体效果。这种方法只适用于数据量比较大且缺失值 比例很小的情况下，一旦数据缺失的比例比较高或者整体数据集的数据量比较小 的话，简单删除法就会导致大量信息丢失，也得不到理想的效果。
几十年来，数据缺失值的填充已经发展出了相对成熟的理论和算法。总结文 献的工作可知，缺失值插补模型从对数据集分布的了解情况可以分为基于参数模 型的插补算法和基于非参数插补方法两类，根据插补的数量来看可以分为单一插 补法和多重插补方法两类。其中基于参数的缺失值插补算法是用在已知数据集分 布的情况下，根据已有的先验知识去构建缺失值与其他变量之间的关系。基于参 数估计的缺失值填充算法得满足数据集分布的先验知识已知的假设，但这种假设
4
并不是一直成立。实际生产过程中的数据集来源广泛，分布的构成就比较复杂， 经典的算法往往服从高斯分布，而现实的数据集往往使多个高斯分布的组合，所 以说经典的分布函数很难去拟合真实的场景。然而非参数缺失值插补方法可以在 未知数据集分布的情况下，利用机器学习模型等其他方法构建缺失值与其他变量 之间的关系。文献［3,4,5,6,7,8］是非参数填充方法在实际场景中的一些应用。
非参数缺失值插补方法由于其不需要对数据集的分布有先验知识，所以适用 范围更为广泛，所以目前大量使用的主要是非参数缺失值插补方法。也正因为如 此，现有的人们对缺失值插补方法研究时，主要都是在讨论基于非参数的缺失值 插补方法。故本文只对这两种类别进行简要陈述，不再详细展开。缺失值插补算 法中，还可以依据插补数据的数量分为单一插补算法和多重插补算法，接下来对 这两种类别进行详细解释。特别的，接下来要对机器学习缺失值插补算法进行特 别阐述，因为机器学习缺失值插补算法与本文提到的迁移学习缺失值插补算法相 关。从分类来说，机器学习缺失值插补算法属于单一缺失值插补和非参数缺失值 插补。
1.3.1	单一插补法
单一插补算法是利用某种算法针对每个缺失值都计算出一个值来插补，单一 插补法能够发挥作用的基本思路为最可能的值插补缺失值比删除这些缺失值信 息丢失要少。在某些场景，比如数据库规模比较大的时候，如果因为某个属性缺 失值比较多而大量删除，会导致其他属性中有用的信息也遭到破坏，所以在这类 场景下我们选用合适的插补方法相对来说更加有效。单一插补法常用的有如下几 种方法：
1）均值插补：均值插补就是利用该特征列的均值作为缺失值的填补对象，优点 是简单易操作，能够以最小成本来保存尽可能多的信息。其中均值的计算大 致分为两种。如果数据均匀分布的话去平均值，如果数据分布不均匀的话取 众数。
2）权重法：权重法可以通过对数据样本增加权重来控制缺失值信息的学习效率。 权重法可以先通过Logistic回归来计算缺失样本的权重个案的权重，机器学 习算法中训练的时候对这些已标记的缺失值进行低权重训练，减少缺失值带 来的偏差。权重法处理缺失值问题也具有较大的缺陷。因为权重的计算很依 赖数据所在的场景，如果权重计算与设定与场景数据分布不符合，或者说不 同类型的数据被赋予了相同的权重，会导致计算的难度增加，预测的精度下 降，这时权重法的效果并不理想。
3）同类均值插补：同类均值插补和均值插补类似，在其基础上，增加了一个类 别的属性。每一个缺失值可能还可以依据某种属性分类，在相同类别下，将 该类别的所有数据作均值，填充到缺失值中去。同类均值插补属于对均值插 补算法的一种改进。
4）数学方法：数学手段主要使用核函数的方法。程誉莹19〕提出了一种基于修正的 Sigmoid核函数的算法用来插补缺失值，相比于KNN和最小二乘法插补，取 得了不错的效果。Johnj"。］等人构建了一种混合核函数，使得核函数缺失值插 补方法可以用来填充离散数据和连续性数据，大大提高了核函数缺失值插补 算法的适用范围。Zhangs。1〕将多重缺失值插补方法和核函数缺失值插补融合 起来，使之能够适用于不同缺失机制下的缺失值插补场景。
5）极大似然估计:如果数据缺失类型为随机缺失,那么根据我们对数据的理解， 观测数据的边际分布，可以得到关于数据的极大似然估计。极大似然估计需 要大样本的数据量作为前提。数据量越大，计算出的似然估计就接近于无偏 估计。但是极大似然估计算法缺点就是容易陷入局部最小值，收敛速度慢。
6） EM算法：EM算法是一种利用极大似然估计或者后验分布在不完整数据情 况下计算的迭代算法。在每一迭代循环过程中交替执行两个步骤：E步和M 步，E步是在给定数据和之前计算的结果的情况下计算数据对应的似然函数 的期望；M步是利用极大化似然函数来计算参数的具体值，用于下一步的计 算。算法通过E步和M步不断地迭代直到参数变化小于一个预定的阈值。当 然EM算法也有一些问题，其收敛速度慢，算法高度依赖初始值的选择。对 于此类问题,孙华艳02］等人提出，在EM算法之前增加KNN算法,使用KNN 算法的分类结果作为EM算法的初始参数设置的参考依据，利用KNN算法 分类的特性，然后按照EM算法迭代反复求精，快速得到缺失值填充结果。 该算法由于集成了聚类算法的稳定性，能够从效率和性能上都能够得到较大 的提升。
7）随机效应模型：于力超和金勇进口刃通过研究，发现采用随机效应模型作为插 补模型时，结果更加准确，并且效应模型的固定对于插补缺失值模型来说相 对简便。固定效应模型适用于缺失比例较小、数据集内部相关性较大等情形 下，否则随机效应插补模型更能够胜任工作。
1.3.2多重插补法
多重插补的方法是Rubin在1978年提出的.后经过多次发展完善，已经是一 个比较完整的理论体系。多重插补基于的缺失机制是随机缺失，更加贴近真实数
据。多值插补缺失值的方法与极大似然类似，其基本假设是缺失数据是随机的。
多重插补算法是利用似然函数估计出待插补的值，再添加不同噪声形成多组插补 值，最终通过某种算法选择出最合适的值。董世杰口4］分别实验了线性回归多重 插补、贝叶斯多重回归和贝叶斯自助线性回归多重插补法，对缺失值插补的结果 进行分析。模拟结果显示，这三种多重插补法显著改善了插补结果。多重插补法 由下列几种常用的方法：随机填补法〔口,趋势得分法”旬，马尔可夫蒙特卡洛法口7］。 下例以随机填补法为例，介绍算法的细节。
随机填补法通过缺失变量与其他观测变量的回归关系来拟合回归方程。假设 为有缺失值的变量，则Lis同其他自变量七，＞2,…，无关系便可以建立回 归模型：
Ymis=WTxX	(1-1)
回归模型的参数W为回归方程的系数，令回归模型的协方差矩阵为0m则 每次采用随机填补法填充过程为：
1)从回归模型的参数W中选择新的参数/ew以及新的协方差矩阵巧nis匕nis，随 后抽取一个服从旌TT分布的随机变量g,计算新的方差：
g
其中~nis是变量bis已知观察到的个体数。然后从标准的正态分布中随机选 取K+1个元素向量S,那么新产生的回归系数为：
*S	(1-3)
2)那么对于缺失值来说，新的预测值为：
Ynew=Wnew*X	(1-4)
3)从缺失变量-is中找到同Ynew最为接近的K个观察值，并从这K个值中选取 一个值作为最终的插补值。
本例假定了数据样本满足正态分布。虽然该假设并不是真实情况，但是可
以通过实验来得出结论，该假设很接近真实情况。
如图1-1所示为多重插补法的流程，总结起来多重插补方法一般分为三个步
骤，填补、分析和综合。接下来分别介绍这三个步骤：
1)针对每个缺失值都采用一个算法预测一套可能的插补值。可以通过预测一个 再添加不同的噪声，也可以预测出一个插补值可能的域；
2)将所有的可能的插补值分别依次插补进去，构建不同的数据集，并在这多个 数据集分别使用同一个分析模型进行分析；
3)根据模型分析的结果，选取效果最好的那一套插补数据作为最终的插补值。
7
图1-1多重插补法的过程示意图
多重插补算法算是对极大似然估计缺失值插补的一种改进，弥补了其几个方 面的不足：
1）多重插补算法采用多次插补一个缺失值，其依据大数据的理论，先验概率对 最终结果的影响将会很小。而极大似然估计需要先验的模型必须正确，否则 将得到错误的结论，比较依赖先验的知识；
2）多重插补算法利用了参数之间的相互关系，比极大似然估计要更加准确。
1.3.3机器学习缺失值插补
机器学习插补缺失值是最近缺失值插补算法中比较新颖的方法。其主要原理 是利用机器学习算法，将缺失值所在变量作为标签列，选取其中的未缺失值作为 训练模型的label,其他变量作为训练集，来训练一个机器学习模型，并对缺失值 所在样本，进行预测缺失值，达到插补缺失值的目的。机器学习补缺失值的优点 是其可以深层次的挖掘数据集中包含的信息，并且由于机器学习算法种类很多， 对各类数据问题都有了成熟的理论基础，引入了机器学习方法，相当于引入一系 列机器学习中成熟的方法，所以在应用到缺失值插补领域中，能够具有非常良好 的泛化性能。
目前的机器学习缺失值插补方法，其主要代表为监督学习缺失值插补。监督 学习缺失值插补方法，是将缺失值作为label,其他非缺失的数据作为训练数据 集，以此来构建缺失值插补模型，并对缺失的数值进行回归或者分类计算，达到 缺失值插补的目的。基于监督学习的缺失值插补方法主要有CantorM081等人提 出的KNN缺失值填补算法，取得了很好的性能。JiangC, YangZ口刃提出的融合 了 K-means聚类的k-NN填充算法，提出利用K-means对数据预聚类，完善了
8
之前KNN插补算法的工作结果大大提升了 k-NN缺失值插补算法的精度。 ShukurOBM提出了基于AR-ANN的填补算法，将深度学习引入到缺失值插补 领域中去。LiuPi〕等人提出了可以自适应的缺失值填充方法，利用分类器判断缺 失值有无效果，最终算法只填充学习器认为有效的缺失值。
除了监督学习之外，基于无监督学习在缺失值处理中也有应用，EM缺失值 插补算法［22］是其典型代表。无监督学习由于缺少必要的标签，往往效果不如监督 学习。
随着机器学习的发展，诞生了很多新的技术，比如强化学习，迁移学习等等， 在各个领域都大放异彩。目前从缺失值插补算法来说，仅有强化学习两种机器学 习方法被用在缺失值插补领域中去。而本文所使用的迁移学习尚未用到缺失值插 补领域。
1.4	本文的创新点
综述所述，无论是从参数插补和非参数插补，或者是单一插补和多重插补， 都是利用本数据集中数据的相关性，来对缺失值做一个预测和估计。但是在真实 场景下，往往缺失值比未缺失的数量还多。对于这种情况，基于统计或者基于机 器学习等方法都不能满足要求。通过调研发现，针对目标数据集中信息过少的问 题，迁移学习早就有研究。所以本文提出，在机器学习缺失值插补算法的基础上， 利用迁移学习，将其他数据集中的信息与目标要插补的信息融合起来，训练缺失 值插补模型。相当于借助外来信息辅助本地计算缺失值，可以从根本上解决本地 信息不足的问题。本文的创新点主要有以下两方面： 1）提出了迁移学习插补缺失值的方法
缺失值插补是数据分析的重要部分，然而现有的缺失值插补算法，从均值插 补、贝叶斯插补到后来的多重插补方法，从主成分分析插补方法到压缩感知低秩 矩阵插补算法，都是基于现有数据集的信息对缺失值进行预测计算，达到填补缺 失值的目的。这些经典的缺失值插补算法有一个缺点，仅仅能够依靠数据集自身 的信息来构建插补模型，然而当数据集中的缺失数据比例过多，现有信息不足以 填充缺失值的时候，缺失值处理的效果大大衰减。本文为了给缺失值插补算法提 供更多的思路，创新地引入了迁移学习算法，针对数据集缺失过多的场景，利用 源数据的信息，帮助目标数据插补缺失值。在模型训练阶段，结合了集成学习的 思想，但不同于传统的多数投票制，而是根据不同视图分类性能的差异做加权处 理，增加了整个系统的鲁棒性。
9
2）改进迁移学习算法使之可以适应回归场景
现有的基于实例的迁移学习的方法，都是基于分类算法而设计的。我们在构 建缺失值插补模型的过程中，只能够处理缺失值为二分类的变量，对于非二分类 的变量，我们只能够通过设定阈值将其转化成二分类变量来做。这样不仅损失了 大量的信息，也使得操作过程变得繁琐。本文基于迁移学习算法TradaBoost,提 出了一种改进的迁移学习算法TradaBoostReg,区别于传统的迁移学习算法，该 算法可以直接对连续值标签训练，迭代计算误差并更新权重。改进后的算法可以 适用于连续值缺失的缺失值插补场景，大大提高了性能。
1.5	本文结构安排
本文主要研究基于填充策略的缺失值填补算法的研究，分别研究了基于机 器学习的缺失值填充算法和基于低秩矩阵的填充模型，再实现基于迁移学习的 缺失值插补算法，再已有的数据集上建立迁移学习缺失值插补模型，并于机器 学习方法和低秩矩阵方法作比较，评估模型性能全文共分为五章，主要内容如 下：
第一章，绪论。主要介绍课题的背景与意义和缺失值填充研究的历史和现 状，比较了各种缺失值插补的算法，分析其优缺点，对缺失值过多导致插补算 法性能退化的问题提出解决办法，引出本文的迁移学习缺失值插补算法。最后 阐述本文关于缺失值填充的主要工作内容及结果。
第二章，详细介绍实现迁移学习缺失值插补所需要的相关技术。包括迁移 学习的相关背景，本次实验用到的决策树算法的相关技术等等，其中着重提到 了本文采用的决策树算法。最后介绍了实验所需要的评价指标。
第三章，研究并实现基于迁移学习的缺失值填充算法，介绍了迁移学习算 法TradaBoost,并搭建了迁移学习缺失值插补模型。设计对照实验，与机器学 习插补算法、低秩插补算法和传统插补算法进行比较，评估迁移学习插补缺失 值算法的优劣，得出结论。
第四章，对迁移学习缺失值插补算法进行改进，提出可以适用于回归任务 的迁移学习算法TradaBoostReg。并对TradaBoostReg算法进行理论分析，从数 学角度证明，随着迭代次数的增加，TradaBoostReg算法的误差损失函数不断下 降直至收敛。然后设计实验，将TradaBoostReg算法相比于传统方法和改进之 前的迁移学习算法比较得出结果。在章节的最后，还对误差进行了约束，提出
10
了误差因子约束的方法，将误差标准化到合理的区间内，使得算法更能够区分 开不同的误差。设计实验验证结果，得出结论。
第五章，总结和展望。对本文提到的工作进行总结，论述了本文的研究价值 的同时，也指出了本文的研究工作存在的不足以及后续可以进一步改进或扩展的 地方，为迁移学习预测缺失值的领域的后续研究提供了一定的参考。
12
第二章迁移学习缺失值插补相关技术
本章将介绍之后几章涉及的相关知识。由于迁移学习在缺失值插补上的应用 尚无前人工作，所以没有直接的文献作为参考。本章将从迁移学习缺失值插补的 三个方面进行阐述，第一部分为阐述缺失值插补所涉及到的机器学习算法，包含 以k-NN和决策树模型为代表的分类模型；第二部分是迁移学习算法，阐述迁移 学习算法的几种类别，并重点对本文所提到的基于实例的迁移学习进行详细介绍; 第三部分是评估分类任务的常用指标。
2.1	迁移学习
传统的机器学习往往需要大量的标注数据来训练一个完成某项任务的学习 器，然而我们并不一定能够搜集到关于该任务的大量标注数据集。在缺少标注数 据集的情况下，无监督学习性能也尚未得到突破，使得机器学习在这种任务下遭 遇瓶颈。迁移学习给这种困境带来一种新的思路，我们可以通过找寻相似的标注 数据，来辅助我们想要的目标任务，共同训练一个学习器。例如，我们有一百张 加菲猫的图片，想要通过传统的机器学习来训练一个加菲猫识别器是很困难的， 然而我们可以通过五万张普通猫的图片，先训练一个关于猫的分类器，然后再利 用一百张加菲猫的图片进一步将模型训练成加菲猫的识别器。在这个例子当中， 五万张普通猫的照片，就是作为外来辅助的数据，来帮助我们训练加菲猫的识别 器。这样得到的效果要比仅仅用那一百张加菲猫图片直接训练好得多。迁移学习 主要解决的就是某一个场景下标注数据过少的问题。
2.1.1	符号和定义
机器学习是分类器训练的主要手段，其分类结果是强烈依赖标签label的。而 迁移学习不同于传统的机器学习，它致力于跨领域学习，通过在其他方面学习到 的信息运用到新的场景中去。迁移学习中提到了源（Source）和目标（Target）的 概念，其中与我们想要预测的部分相关被称之为目标，与外部的辅助相关的称之 为源。我们将迁移学习的内容分为两个维度：
1）域（Domain）:域由特征空间和边缘概率分布构成。域代表了该数据集整体 的空间和边缘概率分布。
2）任务（Task）:给定一个具体的域，一个任务由两部分组成：一个标签空间 和一个目标预测函数。任务不可被直观观测，但是可以通过训练数据学习得
13
来。简化来说，任务包含的两个部分分别为训练的机器学习模型和所具有的 标签。
由域和任务的定义，我们可以给出迁移学习的统一定义［23］：给定源域和源任 务，一个目标域和目标任务，迁移学习致力于用源域和目标域中的知识，帮助提 高的学习。
依据任务和领域的不同，归纳了传统机器学习和迁移学习的异同见表2-1:
表2-1:迁移学习和传统机器学习的异同
学习的设置		源域和目标域	源任务和目标任务
传统机器学习		相同	相同
迁移学习	归纳式迁移学习	相同	不同但是相关
无监督迁移学习	不同但是相关	不同但是相关
直推式迁移学习	不同但是相关	相同
由表2-1可知，在源域和目标域相同并且源任务和目标任务都相同的时候， 即是机器学习算法，除此类之外，都是迁移学习的范畴。根据表里的划分，迁移 学习由以下三种类别：
1）归纳式迁移学习（Inductive Transfer Learning）
人为忽略目标领域和源领域的差异，此时的迁移被称为归纳式迁移。在归纳 式迁移学习中，又可以依据样本有无标签，划分为如下两种：
a）源领域有标记样本。此类别是借助源任务将知识迁移出来，对目标任务 进行改善；
b）源领域没有标记样本。不能用源领域的诸多知识，只能学习源领域的一 些分布。
2）无监督迁移学习（Unsupervised Transfer Learning）
无监督迁移学习中源任务与目标任务是不同的，源域和目标域也不同。但是 他们是有一定相关的。
3）直推式迁移学习（Transductive Transfer Learning）（又叫转导式迁移学习）
源任务和目标任务相同，源域和目标域不同。这种情况下，目标域中无己标 注数据可用，源域中有大量已标注数据可用。除此之外，根据源域和目标域中的 不同状况，可以进一步将转导迁移学习分为两类：
a）源域和目标域中的特征空间不同；
b）源域和目标域间的特征空间相同，但输入数据的边缘概率分布不同。
14
2.1.2	迁移学习的迁移方式
在上一节中，我们讨论了关于迁移学习的基本定义，并重点研究了迁移学习 的两个基本维度域(Domain)和任务(Task)。迁移学习以从源领域向目标领域 迁移的方式可以分为以下不同的形式［24 1)基于实例的迁移学习
基于实例的迁移是迁移学习中最容易的一种，其基本原理是将源领域中与目 标领域中最接近的实例内容提取出来，将其加权与目标领域中的数据共同完成目 标领域学习模型的构建。基于实例的迁移学习最重要的地方是对源领域数据的加 权赋值问题，以保证将其协助目标领域构建学习模型时能够尽量避免负迁移现象 的产生。基于实例的迁移学习的缺点是要求源领域和目标领域尽可能的像，应用 范围比较窄，比如源领域如果是文本形式目标领域是图片的形式这样的迁移是不 能完成的。
定义源域数据分布为Ps(x, y),目标域数据分布为Pt(x, y),最优模型为6*, 则学习的过程如下：
0* = arg min j Q(x,y, 0)Ps(x,y)	(2-1)
2)基于特征的迁移学习
基于特征的迁移是通过将源领域比较能够代表目标领域的特征项协助目标 领域构建其特征空间。基于特征的迁移学习准确度会有相应的降低，但是应用范 围却很广泛，对源领域与目标领域数据的要求也没有那么严格，现今广泛的应用 为图片打标签并且在文本与图片、机器翻译等领域。
定义特征变换〃犷，对观察样本x的表示进行变换的函数为郎。 定义P(W,y)为变换后的数据分布。特征迁移学习与实例迁移学习最大的区别在 于是否需要估算在特征迁移学习中，如果我们能够找到一个合适 的变化〃，使得尸sG,y)=Pr(x,y),那么便无需估算尸/丁)分。/)。
3)基于模型的迁移学习
基于模型的迁移学习技术可以称之为参数迁移，应用范围也相对比较狭窄， 适合应用在源领域和目标领域中有相同的学习模型或者参数或者某些先验分布。 通过这样的模型将源领域的这部分模型或者参数迁移到目标领域。
定义P(8|D)是领域键共享的先验参数信息，它独立于领域数据，则各领域的 分布模型学习可以为：
N
尸(°) = 口。(瞬，匕洌	(2-2)
1=1
15
假设参数p（e|Ds）已经从源领域学习到参数p（e|Ds）,把他应用到目标领域可 得到：
N
尸（Ms） =1［p（x»匕（2-3） 7=7
4）基于关系知识的迁移学习
基于关系知识的迁移学习是基于在源领域与目标领域之间总会有某些相似 的关系这样的假设前提下［25］。这种学习模型的目的就是找出这样的相似关系，然 后应用这些关系进行迁移学习。关系知识的迁移学习基本假设是数据之间存在某 些关系是相似的，这种关系便是关系知识中的关系。例如，专业的赛车手与职业 飞行员之间具有某种相似性，都有着对于速度的反应力和对机器的操控能力。通 过这种关系的联结，能够实现不同领域的知识迁移。
总结以上提到的四种迁移学习方法，基于实例的迁移学习是通过直接对数据 进行分析，修改目标与源数据的权重来达到辅助训练的效果，然而其需要源数据 集和目标数据集具有相同的特征列，对数据集要求较高。这一点造成了多方面的 影响，积极的一面是高要求的数据集带来的迁移效果往往非常良好，消极的一面 是对数据集要求过高导致适用范围较窄，比如我们常见的自然语言处理和图像领 域就无法使用基于实例的迁移学习方法。基于特征的迁移学习是将数据的特征提 取出来，并利用提取的特征达到辅助训练目标数据集的目的。这种方法可以用在 文本和图像数据上，但是由于特征不易使用数学来表达，所以提取特征的工作往 往很复杂，这也是基于特征的迁移学习效果不那么好的原因，但其适用范围广的 优势使得基于特征的迁移学习被大家广泛关注。基于模型的迁移学习虽然对数据 集没有要求，但是他需要源域和目标域训练的模型共享先验参数信息，所以其需 要源域和目标域的分布和空间相同，对数据集的要求也很高。基于关系知识的迁 移学习，需要源域和目标域具有某种相似性，这种迁移学习可以针对某一个场景 进行深度优化，例如赛车手的赛车技能和飞行员的航空技能之间的关系，如果可 以挖掘出来，则会大大提高赛车技能和航空技能的迁移效率。
综上，不同种类的迁移学习方法，都存在各自的优势与缺点，选择不同的迁 移学习方法是依据使用场景来定的。
2.1.3	基于实例的迁移学习
上一小节阐述了迁移学习的四种迁移方式，可见基于实例的迁移学习方法是 四种迁移学习中最为简洁并且效果最好的。本文所针对的应用场景为缺失值插补, 使用的数据集为网格类数据，所以为了达到更好的性能，本文采用基于实例的迁
16
移学习方法。
基于实例的迁移学习算法，最为经典的为戴文渊于2008发表的TradaBoost 四。TradaBoost算法是基于AdaBoost算法改进而来，采用了 AdaBoost的权重迭 代机制。TradaBoost算法在每一轮迭代中，将源数据集中分类错误的样本降低权 重，而将目标数据集中的样本提高权重。那么，多次迭代以后，源数据集中不符 合目标数据集分布的样本权重越来越低，而目标数据集中的样本被越来越重视， 最终得到了一个更好的分类模型。经过多方实验证明，该算法在目标训练数据较 少的情况下可以取得不错的效果。
接下来，将阐述TradaBoost算法的原理和思想，如图2-1,不妨对数据集进 行一个定义。
图 2-1 TradaBoost 算法的机制［26J12
定义2.1 （数据集）
7>｛（引&引））｝,其中引仁天,当 i=l,2.....n	（2-4）
7>｛（#,。（印））｝,其中片6豆,当/=1,2,...,加	（2-5）
S=｛（x；）｝,其中若 E Xb,当 i=l,2，…,k	（2-6）
其中，C（x）是特征向量x的真实标签。Ta是源训练数据集，Tb是目标训练数 据集。Xa, Xb分别为数据集Ta和冗的特征向量。n和m分别是源训练数据集和目 标训练数据集的大小。数据集S是未标注的，且左是集合S中的元素个数。上和7b 的区别在于，荒与测试数据S的分布相同，而Ta与S的分布可能很不一样。
可以定义问题：给定一个很小的源训练数据集灭，大量的辅助训练数据二和 一些未标注的测试数据集S,我们的目标就是要训练一个分类器，尽可能减小S上 的分类误差。算法描述如表2-2所示：
17
________________表2-2 TradaBoost算法描述阳於_____________
输入：源数据集Ta和目标数据集冗，测试集S,弱学习器Learner
输出：一个假设hf：X—Y,针对目标数据集分类器且在S上错误率很小
初始化:
1）初始向量权重W】=（W3w4,	其中：
（1 , -当 2=1,2,...,〃 间=,：	（2-7）
—当 i=l,2,...,m
2）设置 0= 1/（1+J21n（n/N））.
For『=1,2,…,N:
a）设置式满足
（2-8）
b）调用Learner,根据合并后的训练数据7以及T上的权重分布必和未标 注数据S,得到一个在S的分类器上:X-Y
c）计算上在葛上的错误率：
奇端著	勿）
d）设置作=*/（17）
e）设置新的权重向量如下：
向 记洲（.双）|当/12,…,〃
记=S盛）m I	当. -	+	（2-W）
W海	， 港尸力+1,...,力+加
输出模型：
2.2决策树模型
在上一小节我们提到的TradaBoost算法，是基于决策树模型演化而来的。在 本小节中，我们重点阐述决策树模型。
18
2.2.1	经典决策树模型
决策树算法最早由Quinlan在1986年提出卬】。其是一种非参数算法，能够再 数据中建立一种规则，让数据流沿着规则来得到划分的结果，不需要得到数据样 本的分布情况。
图2-2决策树结构示意图[28]
接下来给出决策树的理论解释，设训练集为7 = {(而,弘),(马,%),…,(x“，Mv)}， 其中玉=(染,X?…H"))7'是特征向量，〃是特征的数量，乂 g{1,2,…,K}表示标 签。最终根据规则将所有样本划分到如图2-2所示的一个树形结构的叶节点上, 每一个叶节点就是一个单独的类别。
表2-3 CART算法描述 输入：数据集D,损失函数loss
输出：树小)
在训练数据集所在空间内，循环的将每个区域划分子区域，构建二叉树:
1)选择最优切分变量，与切分点s,求解:
2)
min [mi〃/。ss(x,c)]
遍历扫描j和s,使之能够最小化损失函数； 用选定的(M)对划分区域并决定相应的输出值：
RiO'，s) = {对X⑺ < s})	R2(j,s') = {x|xw > s}
2很=a W Fj xERm, 07=1,2
x,^Rm(j,s)
(2-12)
(2-13)
(2-14)
3)重复直到满足停止条件；
4)将输入空间划分为〃个区域R1,R2,…,Rm，生成决策树：
加)=〉	21nl(x e Rm)	(2-15)
19
决策树有回归树和分类树之分，在二者的区别中，回归树是采用最大均方误 差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树 是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投票决 定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。 回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好 地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更 好的从整体把握这个数据集的分类。
CART算法将回归树和分类树统一起来，通过更换损失函数的方法来调整决 策树是分类树还是回归树的属性。CART的算法如表2-3所示。
2.2.2	集成树模型XGBoost
XGBoost(eXtreme Gradient Boosting)网是 Boosting 算法的其中一种。Boosting 算法的思想是将许多弱分类器集成在一起形成一个强分类器。在XGBoost中， 弱学习器为CART树，弱学习器集成的强学习器相比于原本的弱学习器，具有较 小的偏差(Bias)。XGBoost原理可分为Boosting和Gradient两个部分。
图2-3 GBDT的残差拟合示意图
XGBoost算法的Boosting思想是不断在之前训练的树模型上增添新的树模 型，来逐渐逼近真实值。其每次训练一棵新树，目标都是上一次树模型预测值与 真实值的残差。最终这个残差会在树模型的不断增长过程中越来越小，我们得到 的结果是所有的树预测值的总和。以实例来阐述这一过程：令训练集为[A, B, C, D],集合中的每个元素是数据集中的一条样本，label为[14,16,24,26],分别对
20
应训练样本。我们需要训练一棵决策树，使之可以找到训练集与label的对应关 系，例如输入一个样本A,树模型可以输出结果14。
如图2-3所示，每个方框的第一行数字代表上一棵树预测的数值，第二行代 表该节点剩余的数据标签。假设我们的决策树生成方式是以数据的均值作为划分 点切分,在第一棵树生成过程中，我们可以使用均值20作为第一棵树的切分点， 将数据切分成两部分，第一部分是以15为均值的［14,16］,第二部分是以25为均 值的［24, 26］,在此基础上继续生成新树逐渐逼近真实值。在第二棵树生成之前， 我们将上一次计算的值和真实值做一次残差计算，将计算过的残差作为第二棵树 模型生成的依据。
表2~4梯度提升算法描述
输入：训练数据集 T= {（x1,j/!）, （x2,y2（x^y^},xiex^Rn,yiEYQR，,损失函数
L(y，Ax))
输出：回归树市） 1）初始化：
f0(^)=argmmL{y.,c)
2)对m = 1,2,…,M：
a)对i = 1,2,…,N,计算：
_"(力/U))
5- g)
」网=4_13
(2-16)
(2-17)
b）对Gi拟合一个回归树，得到第加棵树的叶结点区域Rmj，j = l,2，…,J
c）对j=l,2,…,J,计算:
Cmj=argmm	上供工」（匹）+c）	（2-18）
更就(x) =fm.x (x) + * 1 cmjI(xERmj)-,
3）得到回归树:
加）九（x）= W tW t为”烧"叨）	（2/9）
在第一棵树的生成过程中，我们生成了两个结点：均值为15的［14,16］和均值 为25的［24,26］,将此两个结点的真实值减去第一棵树预测的值，得出了第一棵
21
树计算的两个残差［-1,1］和［-1,1］,那么第二棵树的计算便按照这残差来进行。我 们第二棵树也是按照均值划分，以0作为第一棵树左结点残差的划分点，将左结 点切分成-1和1两个子结点；同理第三棵树将右结点切分。这样切分完之后便得 到了一个由三棵树组成的集成决策树模型。那么我们如果输入一个样本A,该模 型可以正确的计算出A的标签值为14,计算过程为每棵树的预测值之和。即 A:15+(-l)=14o
XGBoost的Gradient概念指的是将梯度下降的思想引入进决策树模型。通过 某种巧妙地方法，我们可以将Boosting和梯度下降结合起来，使得Boosting训 练器在迭代过程中沿着梯度下降的方向不断优化，这也从理论上证明了该模型是 收敛的。在XGBoost中，我们利用损失函数的负梯度作为当前模型需要拟合的 残差。梯度提升算法的流程如表2必所示。
对算法全过程作详细解释：第1)步为算法的初始化步骤，初始化一个损失函 数使其极小化，作为只有一个根节点的树；第2)a步骤计算第1)步初始化的损失 函数的梯度负值和当前模型的值，求出模型残差；第2)b步拟合回归树，得到回 归树叶结点区域；第2)c步使用优化算法使得损失函数极小化；第2(d)步更新回 归树。第3步得到最终的模型f(x)。
2.3评估指标
对学习器的泛化性能进行评估，不仅仅需要有效可行的实验估计方法，还需 要有衡量模型泛化能力的评价标准，这就是性能度量。我们通常会根据不同的业 务选出适合的业务指标。在分类任务中，我们通常会选择分类的精确度(Accuracy) 作为评价分类模型的好坏的标准。但是在数据中正负样本(正样本指的是标签为 正或者为1)分布比例差别较大的时候，Accuracy会带来诸多问题，例如：当一 簇样本中正样本比例为99%,负样本比例为1%。这样分类器如果将最终结果全 预测成正，则Accuracy依旧有99%,然而这个分类器我们认为是无效的。为了 避免这些问题，我们采用其他评估指标。其中AUC (Area Under Curves)和F1 score是这其中最常用的代表。以下为介绍这两种评估指标：
2.3.1	AUC
AUC的含义是曲线下面积，这里提到的曲线(Curves)指的是ROC曲线 (Receiver Operating Characteristic Curve) , ROC 曲线是由混淆矩阵得来的。混 淆矩阵是针对二分类问题来得出的。以下用一个例子来解释：
假设有一个任务，预测明天会否下雨，则明天下雨和不下雨为典型的二分类
22
问题。令y=l表示明天会下雨，y=0表示明天不会下雨，则可以绘制如下表格: 表2-5 混淆矩阵表格
真实情况	预测结果
y=1下雨	尸0不下雨
y=1下雨	TP （真正例）	FN （假反例）
y=0不下雨	FP （假正例）	TN （真反例）
如表格2-5, TP表示预测为下雨实际也是下雨的样例数，FN表示预测为不 下雨，实际确实下雨的样例数，FP表示预测为下雨，而实际为不下雨的样例数， TN表示预测为不下雨，而实际也是不下雨的样例数。这四个值就构成混淆矩阵， 我们定义两个变量：
FP
fpr=fp+tn
TP TPR=™
(2-20)
(2-21)
其中，FPR表示，在所有的不下雨中，被预测成下雨的比例，称为假阳率。 假阳率指的是真实为无雨的情况时，有多大概率被预测成有雨。显然，FPR越小 越好。TPR表示，在所有的下雨中，被预测成下雨的比例，称为真阳率。真阳率 指的是真实有雨的情况，有多大概率被预测成有雨。显然，我们希望TPR越大
越好。
0.9
0.8
0.6
二 0.5
0.4	<	——:一
0.3	0』	0.4
0.2 S->---e
0.1 02	0?
0
0	0.2	0.4
0.6	0.8	1
TPR
图2d ROC曲线示意图
二分类模型输出的是预测为正的概率值，其转化成类别标签时需要一个阈值 作为划分标准。阈值的选取会导致分类结果不同，进而导致混淆矩阵不同。在阈
23
值从0到1变化过程中，会形成很多(FPR, TPR)的值，将这些值在坐标系上表 示，所得曲线便为ROC曲线。ROC曲线示意图如图2-4。其中ROC曲线的横坐 标为FPR,纵坐标为TPR。AUC值便为ROC曲线与X轴围成的面积。显然， AUC值越高，ROC曲线更加靠近TPR值为1, FPR值为0的方向。可见AUC 值越高代表结果越好。
AUC计算方法和其定义一样，求ROC曲线与X轴围成的面积。我们计算 每个样本属于正样本的概率并定义为score,算法会将所有样本按照score排序, 则正样本排在负样本之前的概率为AUC值，可知AUC值越大，正样本在负样 本之前的概率越大，则正负样本区分程度就越大，分类算法的性能越好。
在有限样本环境中，我们以频率能够很好的估计出概率值，样本数量越大， 则估计的概率值越精准。令M为正样本的个数,N为负样本的个数。首先对score 从大到小排序，给每个样本定义一个rank值，则将score排名第一的样本对应的 rank值设为N,按照score排名，rank依次递减。正样本中的rank依次相加，最 后减去M-1中两两正样本组合的情况。得到的就是所有的样本中有多少对正类 样本的score大于负类样本的score,然后再除以MXN归一化。使用数学表达式
表达为:
AUC=
Eis positive class rankj-
MQl+M) ~~2-
MxN
(2-22)
23.2 F] score
Fx score,被定义为精确率(precision)和召回率(recall)的调和平均数。其 中精确率和召回率被定义如下：
TP
P-ecisi。片病百
TP
recall=…一丁
TP+FN
其中FP、TP、FN和TN沿用了表格2-3中的定义，则score为:
precision ^recall
F1=2 * -------
precision^-recall
(2-23)
(2-24)
(2-25)
更一般的，Fp score定义为:
F胃=。+乎)*
precision ^recall
6 ^precision) ^-recall
(2-26)
24
2.4本章小结
本章节详细介绍了本文所使用的迁移学习缺失值插补所需的相关技术。首先 介绍了迁移学习的分类和方法；接着介绍了本次实验用到的决策树算法的相关技 术，其中决策树相关技术深入阐述了经典的决策树模型，将经典的分类树和回归 树统一之后的CART模型，还有前沿的XGBoost算法；最后介绍了实验所需要 的评价指标AUC和Fi scoreo
25
26
第三章 基于迁移学习的缺失值插补模型
在真实场景下，往往缺失值比未缺失的数量还多。纵观现有的缺失值插补算 法，从均值插补、贝叶斯插补到后来的多重插补方法，从主成分分析插补方法到 压缩感知低秩矩阵插补算法，都是基于现有数据集的信息对缺失值进行预测计算, 达到填补缺失值的目的。对于这种情况，基于统计或者基于机器学习等方法都不 能满足要求。通过调研发现，针对目标数据集中信息过少的问题，迁移学习早就 有研究。所以本文提出，在机器学习缺失值插补算法的基础上，利用迁移学习， 寻找一个类似的数据集，利用外来的信息对本地数据进行填补，这样不仅仅填补 了缺失值，也从信息论的角度，对整个数据集的信息进行了一个扩充。迁移学习 插补缺失值算法还没有先例，本章所提到的迁移学习插补缺失值算法是基于机器 学习插补缺失值算法演变而来的。迁移学习缺失值插补相当于借助外来信息辅助 本地计算缺失值，可以从根本上解决本地信息不足的问题。
本章节主要内容是提出了一个基于迁移学习的缺失值插补模型。并从实验上 验证其有效性。
3.1	迁移学习缺失值插补算法的构建
在上一章中我们提到，迁移学习有基于实例的迁移学习、基于特征的迁移学 习、基于模型的迁移学习和基于关系知识的迁移学习四种。而相对于实例迁移学 习来说，其他几种迁移学习方法，就通过提取特征的方法，通常用于计算机视觉 和自然语言处理等领域。在本次课题所讨论的表格类型数据中，将外部源数据与 目标数据直接融合在一起训练迁移学习模型,最适合的是基于实例的迁移学习方 法。基于实例的迁移假定源领域的部分训练数据可以通过加权选择之后应用于目 标领域的学习中，源领域训练数据的加权策略是这种技术的关键。该算法主要思 想是，在机器学习插补缺失值算法的基础上，改进了缺失值过多导致的信息不足 的问题。利用迁移学习算法将源数据与数据集中未缺失的变量联合起来，训练一 个插补缺失值的模型，并利用此插补缺失值的模型来对缺失值进行预测填补。
基于实例的迁移学习的缺失值插补算法的主要思想是，引入源数据集来训练 出符合目标数据分布的缺失值插补模型0可以通过减少源数据中不符合本地数据 分布的部分，增强源数据中符合目标数据分布的部分，达到插补缺失值更能够贴 近目标数据集的分布的目的。例如可以去寻找胆固醇高的患者，以其医疗数据来 辅助血糖血脂高的患者填补缺失值。如图3-1为缺失值插补的过程。
27
图3-1迁移学习缺失值插补流程
1）若目标数据X中含有缺失值，则不妨设缺失值所在列为Feat列，则目标数 据X可以根据Feat列是否含有缺失值分为D和C,其中数据集D为Feat列 包含缺失值的部分，C为Feat列不包含缺失值的部分；
源数据s
相交数据si
相交数据C1
相交数据D1
图3-2数据集划分示意图
2）数据集划分如图3-2所示。F1-F5为其他特征列。对于输入源数据集，并查 找源数据集S和数据集D之间特征列相交的部分（图3-2中所示为F1.F3, F4, Feat）,其中对于相交的特征列，源数据集包含该特征列的部分为SI, C 中包含该特征列的为C1；
28
3）将S1和C1的缺失值所在特征列（即Feat列）进行离散化，以便可以适用 于 TradaBoost 分类；
4）使用离散化后的数据集S1和C1输入到迁移学习模型中去，以Feat列作为 标签训练模型；
5）将包含缺失值的数据集D取出相交数据集D1；
6）迁移学习模型TradaBoost预测相交数据集D1的Feat歹U,得到插补完成的数 据集D。此时D中的Feat列是离散值，需要按照离散化之前的标准，将离散 数值恢复成连续的数值。例如，训练时按照120作为血压的阈值将数据划分 为两类，恢复时则按照120以上其他数据的均值和120以下的其他数据均值, 作为两类的恢复后数值。
3.2	数据集与数据预处理
3.2.1	数据集介绍
本次实验的数据集来源有两个，均是来自于算法大赛或者是公开数据集：
1）目标数据集为人工智能辅助糖尿病风险预测大赛中的数据集，该大赛旨在通 过糖尿病人的临床数据和体检指标来预测人群的糖尿病程度，以是否患有糖 尿病为指标。其中主要数据字段如表3-1：
表3-1糖尿病遗传大赛数据集字段
字段	类型	解释
String	体检人id
体检日期	String	体检的日期
Sys	Bigint	收缩压
Dia	Bigint	舒张压
T1	Double	甘油三酯
Hdl	Double	高密度脂蛋白胆固醇
Ldl	Double	低密度脂蛋白胆固醇
白蛋白	Double	白蛋白浓度
血蛋白	Double	血蛋白浓度
该数据集一共有1000条数据，收缩压和舒张压两个字段的缺失值比例分别 为30.1%和27.6%。缺失比例较大，需要对其进行插补。
2）源数据集为双高疾病风险预测大赛中的数据集，要求参赛者通过双高人群的 体检数据来预测人群的高血压和高血脂程度。其中主要数据字段如表3-2所
29
示，该数据集有患者数据一共38200条，其原本是用来根据患者的生理特征 来预测患者的高血压和高血脂症状，然而由于其包含的数据特征列中包含目 标数据集的缺失值列，所以可以用来训练基于实例的迁移学习模型。
表3-2双高疾病风险大赛数据集字段
字段	类型	解释
Vid	String	体检人id
Sp	Bigint	收缩压
Dp	Bigint	舒张压
T1	Double	甘油三酯
Hdl	Double	高密度脂蛋白胆固醇
Ldl	Double	低密度脂蛋白胆固醇
Snp	Double	基因序列特征
3.2.2	数据预处理
由于数据集是来自于真实的医疗场景，所以存在极少数由于人为因素导致的 数据录入错误等问题，我们在做数据清洗的时候，由于数量极少，我们可以将这 些录入错误的数据删除，经实验验证，直接删除此类数据对结果的影响很小，可 忽略。
表3-3源数据与目标数据集交集字段
字段	类型	解释
Vid	String	体检人id
Sp	Bigint	收缩压
Dp	Bigint	舒张压
T1	Double	甘油三酯
Hdl	Double	高密度脂蛋白胆固醇
Ldl	Double	低密度脂蛋白胆固醇
对于基于实例的迁移学习算法，我们要求源数据集和带有缺失值的目标数据
集的关系为：源数据集中包含目标数据集中缺失值所在属性，并且与目标数据集 有共同的若干个属性特征。所以我们为了构建这种形式，在目标数据和源数据的 特征列选取交集，这在上一节中也有具体阐述，这里不再赘述具体过程，只给出 针对糖尿病数据集具体的相交字段。实际操作中，源数据集和目标数据集中包含 字段意义相同但是名称不一的字段，将其统一。取完交集之后的数据如表3-3所 示，将这些字段在源数据的部分和目标数据集中的部分输入到模型进行后面的研
30
究。
3.3	设计实验
本次我们主要探究迁移学习缺失值插补算法与其他缺失值插补算法的性能 比较。传统的TradaBoost算法的弱学习器，是CART树模型。弱学习器的性能 会直接影响到整个TradaBoost算法的效果，由于CART模型结构较为简单，属 于性能较弱的决策树模型。这里我们可以思考如果将CART模型换成其他的树 模型，是否可以提高TradaBoost的性能。故在设计实验之前，我们可以对迁移学 习的弱学习器选用设计一组预实验，观察在迁移学习中，不同决策树算法作为迁 移学习的弱学习器对最终结果的影响。在选用最佳的弱学习器之后，我们再设计 实验观察在不同缺失值比例下，迁移学习缺失值插补算法相比于其他缺失值算法 的效果对比。
3.3.1	选取弱学习器
TradaBoost算法是基于决策树模型作为弱学习器的算法，因此我们选取的不 同弱学习器，都是决策树模型。弱学习器供选择如下：
1)	CART (Classification And Regression Tree)模型，最早的统一了分类树和回 归树的模型。也是基础决策树算法之一；
2)	RF (RandomForest)模型，随机森林模型是集成学习的一种，将多棵决策树 的结果投票，选取投票的结果作为最终结果；
3)	GBDT (Gradient Boost Decision Tree)模型，GBDT 是集成学习的一种，每棵 树都在学习上一轮的残差，使得迭代结果越来越逼近真实值。其代表作主要 是 XGBoost o
决策树模型还有一种是AdaBoost, TradaBoost的原理也是基于AdaBoost修 改而来，所以两种算法的原理非常类似。在对相同的样本进行权重迭代时，可能 会出现AdaBoost计算的权重和TradaBoost迭代计算的权重相同，在AdaBoost下 不能正确分类的样本，TradaBoost同样不能正确分类。这会导致最终结果泛化性 能下降，对结果造成干扰，故不选用。
现实中数据集的缺失值是随机的，为了更好地模拟这一特性，我们将待实验 的数据集中随机产生一定比例的缺失值。在本次数据集中，由于数据集的收缩压 特征本身具有30%的缺失数据，在设计实验的时候不能提前对这30%的缺失值 进行处理。所以我们设置缺失值比例时，从30%的缺失比例开始，按照30%至 100%的区间分成5等份，所以我们随机设置30%, 44%, 58%, 72%, 86%, 100%
31
比例的缺失值。接着我们按照相同比例缺失值的数据集中，采用不同弱学习器构 建的迁移学习方法来作为实验组，实验组相互对照。我们可以根据最终结果选用 最佳的弱学习器。
表35几种树模型对缺失值插补的影响
短失百分比	GBDT	RF	CART
30%	0.632	0.635	0.630
44%	0.622	0.616	0.612
58%	0.580	0.578	0.578
72%	0.547	0.546	0.550
86%	0.516	0.501	0.508
100%	0.514	0.503	0.505
如表3-4是不同算法在是数据集上的缺失值插补表现。其中第一列是数据集 特列的缺失值比例，表格中数据是插补缺失值之后整个数据集的AUC指标。
由图中分析可知，不同的弱学习器对最终的缺失值插补效果是有影响的。在 不同缺失值比例下，不同算法作为弱学习器的性能排名是不一样的。但是综合来 看，XGBoost算法效果最好，CART次之，随机森林(RandomForest)最差。通 过本次实验可以得出结论，在本糖尿病数据集下，弱学习器选用XGBoost效果 最优。并选用以XGBoost算法作为弱学习器的TradaBoost算法作为本次实验缺 失值插补算法。
3.3.2	实验对照组设计
由上一小节可知，我们通过实验来选取了合适的弱学习器，接下来便是由这 个弱学习器来构建迁移学习，与其他缺失值插补算法的对比。
在本次实验中，我们从30%到100%分成10等份，进行实验处理。我们对于 收缩压这一特征列随机设置30%, 37%, 44%, 51%, 58%, 65%, 72%, 79%, 86%, 93%, 100%比例的缺失值。接着我们按照相同比例缺失值的数据集中，采 用不同缺失值插补方法来作为对照组。本次实验设置对照组和实验组，其中对照 组为三份，分别为0值插补、机器学习插补算法和低秩矩阵压缩感知缺失值插补 算法；实验组为一份，为迁移学习缺失值插补算法。缺失值插补完成之后，将填 充完成的数据集交给同一个训练模型训练并验证结果，证明算法的有效性。实验 中采用的迁移学习算法为TradaBoost。插补完缺失值之后，衡量插补效果，我们 将插补完成的数据输入到同一个机器学习算法中验证效果，验证效果的机器学习 算法为XGBoosto
32
3.4	对比的缺失值插补模型
本次实验采用了 0值插补，k-NN缺失值插补算法，低秩矩阵缺失值插补算 法作为对比，本小节重点阐述了 k-NN算法和低秩矩阵算法。
3.4.1	k-NN缺失值插补算法
k-NN算法是由Cover和Hart网于1968年提出的基于实例的分类算法，其主 要思路是定义样本之间的距离，选取一定距离内的样本对待分类的样本进行投票, 投票结果为待预测样本最终的预测结果。如图3-3所示，为缺失值数据的划分示 意图。将待插补数据集分为两个部分，其中0声2曲声4声5｝为不含缺失值的训练集 数据，值6声7｝为含缺失值的待预测数据，缺失值所在特征列为标签值。确定好训 练数据和待预测数据，则k-NN缺失值插补算法具体的算法步骤如下：
fl	f2	f3	f4	f5	label
al
a2
a3
a4
a5
a6				nan
a7				nan
fl	f2	f3	f4	f5
al
a2
a3
a4
a5
a6				nan
a7				nan
图3-3：机器学习缺失值插补数据集划分示意图
1)选定计算样本距离的度量函数，通常选用欧式距离，表达式如式(3-1)：
d=].2-巧)2 + —2少1)2	(3-1)
2)按照第1步的距离度量指标，在训练集中寻找与待分类样本距离最近的左个 样本；
3)在上个距离最近的样本中，依次计算属于每个类别的权重，待分类的样本最 终属于权重最高的那个样本。
k-NN算法为基于实例的无参数模型，其不需要计算样本在空间中的概率分 布，然而由于需要计算每个待分类样本与训练集样本的距离，所以复杂度较高。
33
3.4.2	低秩矩阵缺失值插补算法
矩阵方法是数据集插补算法中较为重要的方法。马友等人即提出一种算法, 将卫星遥测数据建模为二维矩阵，然后将矩阵进行分解，求得其成分矩阵，通过 成分矩阵对原矩阵进行重构，重构之后矩阵中就包含了原始矩阵中对预测缺失值 的信息。矩阵低秩填充是当矩阵不完整时，通过分解矩阵来找到矩阵各元素之间 的相关性，进而填充样本矩阵。如果矩阵X不含缺失值，那么依据矩阵奇异值分 解，我们可以将矩阵X分解成。(大小为m*k)和/(大小为m*k),其中k< min{m, n］,贝(］：
X=UVr	(3-2)
因为k < min{m, nJ,所以rank(U) < k, rank(V) < k,该矩阵分解又称低秩 矩阵分解，这样的分解时建立在矩阵X是完整的情况下成立的。我们的目标是
找到矩阵X的近似矩阵又，通过文中对应的值来填充X中缺失的部分。而想要找 到文，就是要找到矩阵。和兀这里我们希望又尽可能与X相近，用优化数学表 达式表示成：min||X —X||fO展开表达式为：
J=||X-田
(3-3)
其中，i, j分别表示矩阵X的行和列，要求XjjHnan,表达式最终求解目标为
Uii，Vj］。
随机初始化矩阵。和匕则损失函数J便可以计算出误差，利用梯度下降方
法，更新。和匕按照梯度下降法，uu,收更新公式为: dJ
Uii=uu-a — =〃〃+2ae 2
dJ
为7=8厂 a 或 =uu+laeyun
(3-4)
(3-5)
每次迭代都会更新。和V,使得又不断接近X。
3.5	实验结果分析
本节我们使用表格列举了改进后的缺失值插补算法与改进之前的缺失值插 补算法在全部数据集上的表现。本次实验我们采用AUC (Area Under Curves)作 为算法的评估指标。我们将重复实验5次，并取均值作为最终结果。
如表3-5与表3-6是不同算法在是数据集上的缺失值插补表现。本次实验使
34
用了 AUC和F］值作为最终的评估指标。从表中可以看出，在数据集的指标测试 中，AUC和也得出的结论基本一致。根据算法的表现，可以从缺失比例较低和 较高两个方面进行分析。
在缺失比例相对较小的情况下，迁移学习缺失值插补算法并没有发挥出其优 势，反而被低秩矩阵缺失值插补算法反超了。这是因为在缺失值比例不高的情况 下，传统的机器学习方法可以利用的数据集信息较为充足，所以迁移学习的优势 并不能完全发挥出来。
表3-5不同算法在数据集上的缺失值插补表现（AUC值）
缺失百分比	0值插补	k-NN插未卜	低秩矩阵插补	迁移学习插补
30%	0.640	0.632	0.641	0.630
37%	0.610	0.614	0.635	0.615
44%	0.570	0.585	0.591	0.567
51%	0.532	0.523	0.572	0.544
58%	0.528	0.515	0.558	0.513
65%	0.500	0.503	0.525	0.508
72%	0.500	0.508	0.524	0.507
79%	0.500	0.500	0.502	0.510
86%	0.500	0.500	0.500	0.504
93%	0.500	0.500	0.500	0310
100%	0.500	0.500	0.500	0.507
当缺失值比例进一步加大，其他缺失值插补算法的效果迅速退化，尤其是 AUC指标甚至逐渐逼近最终到了 0.5,这表示此时的缺失值插补效果随着缺失比 例的增加而逐渐失效。而迁移学习缺失值插补，由于插补模型与目标数据有关， 所以在缺失值比例不断增大直至超过72%的时候保持平稳，AUC未退化成0.5, 表示迁移学习缺失值插补在此时依旧是有效的插补模型。在数据集中缺失值比例 过高，传统的缺失值插补算法不能从中学习到足够的信息，而迁移学习由于有源 数据集的信息作为辅助，所以依旧能够发挥作用。
综上从实验结果来看，迁移学习缺失值插补方法在缺失值比例较高的场景下 适用。但是实验结果也表明，迁移学习缺失值插补方法的效果并不显著，在大部 分情况下都弱于传统方法。分析其原因，在TradaBoost算法使用之前，对缺失值 进行了连续转化为离散的处理，使之可以适用于TradaBoost算法。这导致大量的 信息被丢失，使得最终结果也不理想。
35
表3-6不同算法在数据集上的缺失值插补表现（Fi值）
缺失百分比	0值插补	k-NN插补	低秩矩阵插补	迁移学习插补
30%	0.265	0.302	0302	0.256
37%	0.233	0.279	0.295	0.222
44%	0.196	0.254	0.265	0.201
51%	0.097	0.150	0.234	0.189
58%	0.073	0.132	0.196	0.156
65%	0.052	0.098	0.102	0.134
72%	0.021	0.067	0.078	0.112
79%	0.013	0.042	0.032	0.096
86%	0.024	0.021	0.013	0.045
93%	0.011	0.011	0.015	0.034
100%	0.005	0.005	0.005	0.032
3.6	本章小结
本章节研究并实现基于迁移学习的缺失值填充算法，介绍了迁移学习算法 TradaBoost,并搭建了迁移学习缺失值插补模型。设计对照实验,与机器学习插 补算法、低秩插补算法和传统插补算法进行比较，评估迁移学习插补缺失值算法 的优劣，得出结论。结果显示，由TradaBoost算法构建的迁移学习缺失值插补算 法并不能取得较于其他算法更好的性能，但是在缺失值比例较大，缺失比例超过 50%的时候，迁移学习缺失值插补算法还能够起到插补的效果，而相比之下其他 传统的缺失值插补算法的性能都退化到最低点。
36
第四章 基于回归的TradaBoostReg算法
由上一章我们提到，基于迁移学习的缺失值填充算法效果评估中，与传统的 缺失值填充算法优势并不明显，其有部分原因在于传统的Tradaboost是基于分类 模型构建的，要想利用TradaBoost模型来构建缺失值插补算法，那么我们必须保 证确实的数据是一个离散的变量，所以使用传统的Tradaboost模型之前，必须将 所有连续的缺失值转化成离散的值，这样会导致大量信息在转化过程中丢失。然 而在缺失值填充领域，我们并不能保证缺失的数据都是离散的变量，因此改进算 法使之能够胜任回归问题，是接下来工作的重点。我们从这个方向来着手优化迁 移学习缺失值填充算法。
在本小节中，我们将从损失函数的角度改进传统的TradaBoost模型，提出基 于回归的TradaBoostReg算法，使之具有学习回归问题的能力。
4.1	改进传统的Tradaboost模型
传统的Tradaboost模型中提到：在源数据集中，被错误分类的样本权重是随 着误分类率的升高而升高，而在目标数据集中，被错误分类的样本权重是随着误 分类率的升高而降低。分类问题和回归问题的核心就在于这个权重的更新是依据 分类误差还是回归误差。据此思路，我提出了基于回归的TradaBoost算法 TradaBoostReg o
在传统的TradaBoost模型中，误差计算公式如3-6所示，其中，3为权重， M%)为计算的标签，c(x)为真实的标签。由于采用的是分类模型，这里有九(均€ {0,1}, c(x) G {0,1}。我们的思路是：当采用回归模型时,h(x)和c(x)都是连续值， 那么h(x)-c(x)CR。由于牲的区间(0,1),我们可以将h(x)和c(x)的值域转化到(0,1)。 所以寻找一个合适的激活函数成了改进算法的过程中最重要的步骤之一。
4.1.1	激活函数sigmoid
满足分类函数回归化的函数，需要满足两个条件：取值范围在0〜1之间和函 数平滑可导。前者是为了将回归的数值归一化到［0,1］区间，后者是为了损失函数 迭代的时候可以顺利进行下去。由此引出本方法所提及的sigmoid函数。
sigmoid函数由其形状为S型而得名。其满足我们需要的两个条件：函数的 取值在(0,1),且平滑可导。如图4-1所示为sigmoid函数的图像。
37
图4-1: sigmoid函数的图像
sigmoid函数的数学表达式为：
sigmoid(x)=百羡	(4-1)
sigmoid在数学上的一个优势就是：
sigmoid (x) =sigmoid(x) (1 -sigmoid(x)')	(4-2)
这个优势导致sigmoid函数的导数可以降阶计算，由于在机器学习过程之中 对于梯度和导数的依赖比较多，所以这一点优势大大降低了算法计算的复杂度。 另外sigmoid函数的微分形式是一个“钟形”图像，类似于正态分布的函数。由 于正态分布的函数计算积分复杂度较高，所以我们可以使用sigmoid函数从一定 程度上模拟正态分布的积分形式。
从烯的角度解释，帽表达了一个分布中的混乱程度和不确定性，熠越大分布 越混乱。所以，均匀分布嫡最大，因为基本新数据是任何值的概率都均等。而我 们现在关心的是，给定某些假设之后，熠最大的分布。也就是说这个分布应该在 满足我假设的前提下越均匀越好。我们如果设计函数使得服从伯努利分布的变量 烯最大化，那么这个函数就是sigmoid函数。
4.1.2	TradaBoostReg 算法
由本章节开头可知，我们改进TradaBoost算法的思路为将损失函数中的误差 通过一个激活函数，使之值域缩放在［0,1］之间，又在上一节中介绍了合适的激活 函数sigmoid,至此我们可以参照TradaBoost算法，给出我们的TradaBoostReg 算法，TradaBoostReg算法描述如表4-1所示：
38
表 4-1 TradaBoostReg 算法描述
输入：源数据集Ta和目标数据集Tb，测试集S,弱学习器Learner,还有迭代次 数N。
输出：训练好的回归树。
初始化：
初始向量权重W1 = (W±W%...,wR+m),其中：
(1	I
-当 n
-当 i=l,2,…,a
设置忏1/(1+同而),
For t=l,2,…，N：
1)设置pt满足
2)调用Learner,根据合并后的训练数据T以及T上的权重分布/和未标注数 据S,得到一个在S的分类器上:X - Y
3)计算也在篇上的错误率：
v^jSigmoid\ ht(x^)-c(x^) |
4)
5)
设置叫=./(17)
设置新的权重向量如下:
州右的酒肉(珀一代)1),当
刖温(同底8)|),	当+1, tn+m
(4-6)
输出最终回归器:
值得注意的是，这里的的我们需要保证其在(0, 1)范围之间。因为Bt=Et/(l-J)， 如果4大于1,会导致比小于1,由于傥是计算权重的指数底数部分，所以凡小于 o会导致权重在o附近震荡，不能满足权重更新的需要。所以我们只需要将误差 的值域转换到(0,1)之间即可。
由表4-1我们得到了基于回归的Tradaboost算法TradaBoostReg,可以将其直 接运用到缺失值填充算法中去，由于TradaBoostReg算法适用于连续缺失值，所 以基于回归TradaBoostReg缺失值插补算法相比于基于TradaBoost的缺失值插补 算法减少了离散化的步骤。完整步骤如图4-2所示，步骤阐述如下：
1）数据集划分与上一章的基于TradaBoost的缺失值插补算法相同，我们由源数 据S和目标数据X得到了相交数据集SI、C1和D1；
2）将S1和C1直接输入到TradaBoostReg数据集中去，训练缺失值插补模型;
3）将训练好的模型在D1上预测，得到插补好的数据集D。
图4-2基于TradaBoostReg算法的缺失值插补流程
4.2	TradaBoostReg算法实验结果分析
在上一节中，我们给出了我们的基于回归的TradaBoost框架TradaBoostReg, TradaBoostReg利用源数据来帮助目标的回归学习，在这一部分里，我们通过设 计实验，来验证效果。
4.2.1	实验设计
本节我们使用表格列举了五种缺失值插补算法在全部数据集上的表现。在本 次数据集中，我们对于收缩压这一特征列随机设置30%, 37%, 44%, 51%, 58%, 65%, 72%, 79%, 86%, 93%, 100%比例的缺失值。接着我们按照相同比例缺失 值的数据集中，采用不同缺失值插补方法来作为对照组。本次实验设置对照组和 实验组，其中对照组为四份，分别为0值插补、机器学习插补算法、低秩矩阵压
40
缩感知缺失值插补算法和上一章提到的改进之前的迁移学习缺失值插补法；实验 组为一份，为改进后的迁移学习缺失值插补算法。缺失值插补完成之后，将填充 完成的数据集交给同一个训练模型训练并验证结果，证明算法的有效性。实验中 采用的机器学习插补缺失值的模型为k-NN,迁移学习算法为TradaBoost。插补 完缺失值之后，衡量插补效果，我们将插补完成的数据输入到同一个机器学习算 法中验证效果，验证效果的机器学习算法为XGBoost。
本次实验我们采用AUC和Fi值作为算法的评估指标。为了尽量减少随机性 带来的误差，再度同一个数据集上相同缺失比例的缺失值填充时重复实验5次, 将5次得到的AUC与Fi值均值作为最终的评价指标。
4.2.2	结果分析
如表4-2和表4-3是不同算法在是数据集上的缺失值插补表现。其中第一列 是数据集特征列的缺失值比例，表格中的数值是插补缺失值之后整个数据集的指 标，两个表分别为AUC值和Fi值。
从表中可以看出，在数据集的指标测试中，我们的改进之后的算法在多数情 况下明显好于其他三种算法。下面从缺失值比例较低和较高两个方面进行具体分 析。在缺失比例相对较小（51%及以下）的情况下，迁移学习缺失值插补算法并 没有发挥出其优势，这一点与改进之前类似，数据集缺失比例不高时，传统缺失 值算法能够从数据集中获取到足够信息来构建缺失值插补模型。
表4-2不同算法在数据集上的缺失值插补表现（AUC值）
缺失百分比	0值填充	k-NN填充	低秩矩阵 填充	迁移学习填充 （分类模型）	迁移学习填充 （回归模型）
30%	0.640	0.632	0.641	0.630	0.645
37%	0.610	0.614	0.635	0.615	0.624
44%	0.570	0.585	0.591	0.567	0.588
51%	0.532	0.523	0.572	0.544	0.564
58%	0.528	0.515	0.558	0.513	0.572
65%	0.500	0.503	0.525	0.508	0.544
72%	0.500	0.508	0.524	0.507	0.544
79%	0.500	0.500	0.502	0.510	0.557
86%	0.500	0.500	0.500	0.504	0.542
93%	0.500	0.500	0.500	0.510	0.524
100%	0.500	0.500	0.500	0.507	0.522
41
当缺失值比例进一步增大的过程中，其他缺失值插补算法迅速退化，AUC 最终逐渐逼近到了 05 而迁移学习缺失值插补，在缺失值比例大于51%之后， 超过了实验的所有的缺失值插补算法。并在缺失值比例进一步增大的过程中保持 稳定，性能明显好于在其他算法中效果最好的低秩矩阵分解算法。我们可以从图 中得出，改进后的迁移学习缺失值插补算法较于改进之前的提升很大，其原因主 要是改进之后的迁移学习缺失值插补算法能够避免了将数据集中的连续值转化 成离散值这一步，节约了大量的信息损失。
表4-3不同算法在数据集上的缺失值插补表现（Fi值）
缺失百分 比	0值填充	k-NN填充	低秩矩阵填 充	迁移学习填充 （分类模型）	迁移学习填 充（回归模 型）
30%	0.265	0.302	0.302	0.256	0.303
37%	0.233	0.279	0.295	0.222	0.297
44%	0.196	0.254	0.265	0.201	0.246
51%	0.097	0.150	0.234	0.189	0.231
58%	0.073	0.132	0.196	0.156	0.201
65%	0.052	0.098	0.102	0.134	0.185
72%	0.021	0.067	0.078	0.112	0.135
79%	0.013	0.042	0.032	0.096	0.101
86%	0.024	0.021	0.013	0.045	0.084
93%	0.011	0.011	0.015	0.034	0.083
100%	0.005	0.005	0.005	0.032	0.076
4.3	误差的标准化
在上一节的工作中，我们将基于分类的TradaBoost算法进行了推广，将算法 适用于回归的场景中，并取得了初步的成果，在缺失值比例较高的情况下，基于 回归的迁移学习缺失值插补算法相较于传统的缺失值插补算法存在优势o然而在 分析过程中，我们可以对于上一节提到的算法进行进一步改进。
4.3.1	误差标准化的思路
sigmoid函数图像如图4-1所示,可见在x=0附近,sigmoid函数的斜率较高， 易得最大值为0.25,而在x趋于正无穷或者是负无穷的区间内，sigmoid函数的
42
斜率非常低。在TradaBoostReg算法中,我们有计算误差的函数如公式4-5所示， 分析可得，若取值在0附近时，在通过sigmoid函数之后，其缩放关系约等于斜 率为0.25的线性缩放，数值之间的区别依旧可以在sigmoid函数之后体现并区 分。若表达式4-8取值远大于0时（绝对值不可能小于0）,其缩放关系便逐渐逼 近斜率为0的线性缩放，数值之间的差距通过sigmoid函数之后变得非常小，导 致了不同大小的误差在通过该sigmoid函数之后几乎相等。这样对后续的误差计 算和权重计算都带来不小的困难，使其难以收敛并容易受噪声的干扰。
n+m t
ZW他(於。(修)|
(4-8)
我们采用误差标准化的方法来解决这个问题，即在误差计算之前，减去一个 常数将其标准化，使误差均值为0,再通过sigmoid函数，计算最终误差。分析 其优点，误差标准化之后，误差会在0附近，通过sigmoid函数之后的误差将更 具有区分度，使得权重迭代更具有效率，不会在趋近于1或者趋近于0的范围反 复震荡而得不到最优值。最终的误差计算函数如4-9所示：
(n+m
f+1
-v^jSigmoidfSht(x^-cix^-constanf)
(4-9)
疆小记
4.3.2	实验结果分析
对于常数的选择，我们可以通过求均值的方法来定位，再使用实验的方法找 出来最合适的常数。我们迭代第一轮之后，将弱学习器在目标数据的1000条样 本上学习的误差值，得到误差的均值为6.537,于是将误差标准化的常数设置成 6.5,设计实验来研究标准化之后的性能。
沿用之前的实验方法，我们使用表格列举了改进后的缺失值插补算法与改进 之前的缺失值插补算法在全部数据集上的表现。本次实验我们采用AUC和Fi作 为算法的评估指标为了尽量减少随机性带来的误差，在同一个数据集上相同缺失 比例的缺失值填充时重复实验5次，将5次得到的AUC和Fi均值作为最终的评 价指标。
如图4-4和4-5是不同算法在是数据集上的缺失值插补表现。其中第一列是 数据集特征列的缺失值比例，表格中的数据是插补缺失值之后整个数据集的指标, 两个表分别表示的是AUC和取值。
43
表44不同算法在数据集上的缺失值插补表现（AUC值）
缺失百分比	0值插 补	k-NN 插 补	低秩矩阵 插补	迁移学习插 补（分类模 型）	迁移学习插 补（回归模 型）	迁移学习 （误差标 准化）
30%	0.640	0.632	0.641	0.630	0.645	0.645
37%	0.610	0.614	0.635	0.615	0.624	0.624
44%	0.570	0.585	0.591	0.567	0.588	0.588
51%	0.532	0.523	0.572	0.544	0.564	0.S64
58%	0.528	0.515	0.558	0.513	0.572	0.558
65%	0.500	0.503	0.525	0.508	0.544	0.535
72%	0.500	0.508	0.524	0.507	0.544	0.541
79%	0.500	0.500	0.502	0.510	0.557	0.550
86%	0.500	0.500	0.500	0.504	0.542	0.541
93%	0.500	0.500	0.500	0.510	0.524	0.511
100%	0.500	0.500	0.500	0.507	0.522	0.510
表4-5不同算法在数据集上的缺失值插补表现（琦值）
缺失百分 比	0值插补	k-NN 插 补	低秩矩 阵插补	迁移学习插 补（分类模 型）	迁移学习插 补（回归模 型）	迁移学习 （误差标 准化）
30%	0.265	0.302	0.302	0.256	0.303	0303
37%	0.233	0.279	0.295	0.222	0.297	0.297
44%	0.196	0.254	0.265	0.201	0.246	0.246
51%	0.097	0.150	0.234	0.189	0.231	0.223
58%	0.073	0.132	0.196	0.156	0.201	0.189
65%	0.052	0.098	0.102	0.134	0.185	0.184
72%	0.021	0.067	0.078	0.112	0.135	0.123
79%	0.013	0.042	0.032	0.096	0.101	0.092
86%	0.024	0.021	0.013	0.045	0.084	0.083
93%	0.011	0.011	0.015	0.034	0.083	0.062
100%	0.005	0.005	0.005	0.032	0.076	0.053
从表中可以看出，在数据集的指标测试中，我们的改进之后的算法在性能上 相较于误差标准化之前性能并未提升。在缺失比例相对较小的情况下，误差因子
44
改进之后的迁移学习缺失值插补算法要和改进之前保持一致，而在缺失值比例进 一步加大，误差因子改进之后的迁移学习缺失值插补算法迅速退化，AUC要比 标准化之前略有差距。然而，即使增加误差因子的算法相比于之前的算法效果稍 有下降，但是整体较于传统的缺失值插补算法还是有一些提升，尤其是缺失比例 在50%到90%之间。分析其原因，误差标准化可能在训练过程中对误差信息造成 了一些损失，导致最终效果从准确指标上来看并不突出。
在实验过程中，可以发现虽然误差标准化之后的性能相较于误差标准化之前 并未提升，但是迭代速度有明显提升，对此，本文对误差标准化增加了一组实验 来研究误差标准化前后的迭代速率问题，记录了算法收敛所需的迭代轮次，记录 数据如表4-6所示：
表4-6误差标准化前后对于缺失值插补算法的迭代次数影响
缺失百分比	误差标准化前	误差标准化后
30%	47	21
37%	45	25
44%	56	16
51%	45	29
58%	34	23
65%	67	43
72%	76	54
79%	54	34
86%	53	27
93%	65	31
100%	46	41
表4-6中，第一列为缺失值的缺失比例，表中的数据为误差标准化的迭代次 数可见误差标准化之后，算法的迭代次数均得到了明显的下降。分析其原因， 误差在被修正之后一直在合理的区间内迭代运行，误差在sigmoid函数上更具 有区分度，梯度下降迭代权重时，可以将梯度较为平缓的区域缩放，使得权重 迭代更加有效率。
综上，对于误差标准化的改进，我们得出结论为误差标准化改进之后，迭 代效率较于标准化之前有明显提升，然而性能角度来看，误差标准化之后性能 比标准化之前略有下降，但是比其他算法有一些提升。
45
4.4	本章小结
本章对迁移学习缺失值插补算法进行改进，提出可以适用于回归任务的迁移 学习算法TradaBoostRego并对TradaBoostReg算法进行理论分析，从数学角度 证明，随着迭代次数的增加，TradaBoostReg算法的误差损失函数不断下降直至 收敛。然后设计实验，实验结果证明TradaBoostReg算法在缺失值比例大于51% 的时候，能够取得明显优于传统方法和改进之前的迁移学习算法的性能。最后提 出了误差标准化的方法，将误差标准化到一个合理的区间，使之能够更具有区分 度。设计实验验证该想法，结果显示，增加了误差标准化之后的算法，在保证整 体性能变化不大的情况下，相较于未增加误差标准化的算法，具有更快的迭代次 数。
46
第五章总结与展望
5.1工作总结
在互联网的大环境下，数据获取越来越方便，数据量随着时间呈指数型增长。 然而庞大的数据量无疑增加了数据管理的难度，尤其是在互联网的大背景下，信 息流动更加频繁，使得这一趋势愈发明显，缺失值插补一直是大数据分析的热点 问题。本文为了给缺失值插补算法提供更多的思路，创新地引入了迁移学习算法, 针对数据集缺失过多的场景，利用源数据的信息，帮助目标数据插补缺失值。然 后为进一步提高迁移学习缺失值插补的准确性，本文提出了一种基于回归的 TradaBoostReg算法。本文主要研究内容和主要工作如下：
1）寻找数据集，在kaggle、UCL等公开数据库中寻找与目标数据相匹配的数据 集，对数据集进行数据清洗，数据切分等数据预处理。
2）调研了传统的缺失值插补算法，针对数据集场景，复现经典算法，对比几种 经典算法的优缺点，分析了将其应用在数据缺失的场景下存在的主要劣势。
3）重点针对基于实例的数据集展开讨论，分析了采取哪一种迁移学习算法适合 作为基于实例数据的缺失值插补算法，并对这一过程做出解释。本文提出了 在数据缺失量比较大的情况下，传统的缺失值插补算法可能不能够创造足够 的信息来插补缺失值，迁移学习如何能够利用源数据集中的信息，辅助模型 插补目标数据集的缺失值。本算法将在糖尿病预测的数据集上进行测试，在 AUC指标上，本算法相较于经典的缺失值算法，在缺失值比例大于51%的时 候能够得到提升。这种思想可用于医疗数据等数据缺失严重的场景中去。
4）研究了迁移学习算法，在迁移学习算法TradaBoost只能够解决类别型缺失值 的前提下，提出了基于回归的TradaBoostReg算法。从理论上证明 TradaBoostReg算法是随着迭代次数的增加而趋于收敛的，从而推导出该算 法是可行的。并在数据集上与改进前的迁移学习做对比，可以发现改进后相 比于改进之前的性能得到大幅度提升。在此基础上，对TradaBoostReg算法 进行了进一步改进。将TradaBoostReg算法的损失函数做了标准化处理，使 得误差损失一直工作在最佳范围内。在数据集上实验并与TradaBoostReg算 法做对比，没有获得较好的提升。.
47
5.2对本文技术的展望
本文提出了基于迁移学习的缺失值插补算法，又对迁移学习算法进行改进, 使之可以胜任回归的缺失值插补问题，取得了一定的效果提升，但由于个人水平 与时间精力都有限，仍有很多需要进一步优化的地方：
1）本文提到的迁移学习缺失值插补算法利用的是基于实例的迁移学习方法， 其对目标数据集和源数据集的要求比较高，需要能够找到与缺失值所在列 相同的特征列的源数据集。然而寻找这种数据集往往需要花费较大的成 本，导致最终没能够取得较好的效果。接下来可以考虑基于特征的迁移学 习，从源数据集中提取类似的特征，来与目标数据集共同训练缺失值插补 模型。基于特征的迁移学习，不需要源数据集与目标数据集具有相同的特 征列，从而扩大了源数据集的寻找范围。
2）本文使用的分类问题转化成回归问题，采用的sigmoid函数转化，然而我们 还可以选择更多的激活函数，或者选择其他的方法来将分类问题转化成回 归问题。
总体而言，本文所提出的算法和思路仍有很多不周之处，但是其在缺失值插 补领域提出了一种新的思想，希望后续的工作中，仍旧能够对其进行完善和开发, 挖掘出该思想的最大价值。
48
参考文献
[1]	Rubin DB. Inference and Missing Data [ J J . Biometrika, 1976(3) : 581 -590.
[2]胡红晓，谢佳，韩冰.缺失值处理方法比较研究[J].商场现代化,2007(15)： 352-353.
[3]	N. A. Zainuri, A. A. Jemain. A comparison of various imputation methods for missing values in air quality data[J]. Sains Malaysiana, 2015, 44:449-456.
[4]	J. V. Hulse, T. M. Khoshgoftaar. A comprehensive empirical evaluation of missing value imputation in noisy software measurement data[J], Journal of Systems and Software, 2008, 5: 691-708.
[5]	J. Huang, Y. F. Li, and M. Xie. An empirical analysis of data preprocessing for machine learning-based software cost estimation[J]. Information and Software Technology. 2015, 11:108-127.
[6]	K. Raja, G.T. Arasu, C. S. Nair. Imputation Framework for Missing Values [J]. International Journal of Computer Trends & Technology. 2012, 3.
[7]	C. Jiang, Z. Yang. An improved knn-based missing value handing technique[C], International Conference on Intelligent Computing. 2015, 9227: 441-452.
[8]	R. Thirumahal, Patil D A. Knn and arl based imputation to estimate missingvalues[ J]. Indonesian Journal of Electrical Engineering & Informatics. 2014,2:119-124.
[9]程誉莹.一种基于修正的Sigmoid核的成分数据缺失值填补法.云南民 族大学学报：自然科学版，2016, 25(6).
[10]	John J, Vinodhini B, Karthik S. Estimating Missing Values Using Mixture Kernel Regression. International Journal of Computer Applications in Engineering Sc,天津大学硕士学位论文
[ll]Zhang S, Qin Y, ZhuX. Kernel-Based Multi-Imputation fbr Missing Data[C]. International Conference on Advances in Intelligent It-active Media Technology, 2006:106-111.
[12]孙华艳，李业丽，字云飞.基于分类的加速EM缺失数据填充算法[J].北京 印刷学院学报,2018,26(09):984 02.
[13]于力超，金勇进.基于分层模型的缺失数据插补方法研究[J].统计研
49
究,20 电35(11):93-104.
[14]董世杰.三种线性回归多重插补法的模拟比较[D].天津财经大学,2017.
[15]赵俊康，王彤，荣慧英等.不同缺失机制并存时偏倚矫正的模拟研究[J]. 中国卫生统计，2014, 31(4)： 570-574.
[16]	Pearson R.K. The Problem of Disguised Missing Data ACM SIGKDD Explorations Newsletter archive Volume8,2006.
[17]	Takahashi M. Statistical Inference in Missing Data by MCMC and Non-MCMC Multiple Imputation Algorithms: Assessing the Effects of Between Imputation Iterations [J]. Data Science Journal, 2017:16:37.
[18]	Batista G, M C Monard, A study of K-nearest neighbor as an imputation method[C], in: Proceedings of the Second International Conference on Hybrid Intelligent Systems, vol.7, IOS Press, Amsterdam, Netherlands, 2002, 251260.
[19]	Jiang C, Yang Z. An Improved KNN-Based Missing Value Handling Technique[J]. Springer International Publishing, 2015, 9227:441-452.
[20]	Shukur OB, Lee MH. Imputation of Missing Values in Daily Wind Speed Data Using Hybrid AR-ANN Methond[J]. Modem Applied Science, 2015, 9(11):1.
[21]	Liu Z G, Pan Q, Dezert J. Adaptive imputation of missing values for incomplete pattern classification[J]. Pattern Recognition, 2016, 52(C):85-95.
[22]	Dempster A, Laird N, Rubin D. Maximun likelihood from incomplete data via the EM aIgorithm[J]. Journal of the Royal Statistical Society, 1977, 39(1):1-38.
[23]	Pan S J , Yang Q . A Survey on Transfer LeamingfJ]. IEEE Transactions on Knowledge and Data Engineering, 2010, 22(10):1345-1359.
[24]夏禹.迁移学习在文本分类中的应用研究[D].哈尔滨工程大学,2014.
[25]Evgeniou A, Pontil M. Multi-task feature leaming//Advances in neural information processing systems: Proceedings of the 2006 conference. The MIT Press, 2007,19:41.
[26]戴文渊.基于实例和特征的迁移学习算法研究[D].上海交通大学,2009.
[27]	Quinlan J R. Induction of decision trees [J]. Machine Learning. 1986 (1):3-7.
[28]王涛.基于多标签信息的特征向量映射算法研究[D].北京邮电大学,2018.
[29]	Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System[J]. 2016.
50
[30]	Cover T, Hart P. Nearest neighbor pattern classification[J]. IEEE Trans Inf Theory,1967, 13(1):21-27.
[31]马友，蔡长海,王强.基于矩阵分解的卫星遥测缺失数据预测算法[J].科技 与创新,2018(21):67-68.
51
52
