第一章绪论
1.1研究背景及意义
深度学习是一个多层而复杂的神经网络，从2006年Hinton等人提出深度学 习［1］概念开始，到2012年Geoffrey Hinton等人利用AlexNet^】在大规模视觉识别 挑战赛中获得了第一名，经历了几年的蓄势待发，深度学习终于如雨后春笋般脱 颖而出。在深度学习的背景下，计算机视觉中的图像分类、目标检测、图像分割 等常见的任务已显示岀前所未有的大发展。计算机视觉技术大大减少了人力成本 的消耗，其现实意义十分深远。其中，目标检测的理论及其应用研究是近年来学 者和工程师们的研究热点。它不仅是计算机视觉学科和图像处理的重要分支，更 是智能监控系统的核心部分。目标检测技术也已应用于许多领域，包括智能视频 监控、机器人导航、数码相机中自动定位和聚焦人脸、飞机航拍或卫星图像中道 路的检测、车载摄像机图像中的障碍物检测、医学影像中的病灶检测等。目标检 测的任务是找出图像中所有感兴趣的目标实例，并确定每个目标实例的空间位置 和覆盖范围o基于深度学习的目标检测算法也将目标检测的性能推向了一个新的 高度。而小目标检测通常被视为目标检测的特例，目标检测算法的性能能否再上 一个高度，很大程度上取决于小目标检测性能的提升。
一般来说，目标的尺寸相对于原图尺寸较小时，称为小目标。但是不同数据 集对小目标的定义不一样：CityPerson⑶数据集上，都是1024x2048像素大小的 图像，只要高度小于75像素即为小目标；MSCOCOR］数据集上，对于图像小于 32x32像素的物体，就认为它是一个小目标；有的数据集定义“物体-图像比⑸”， 即图像中待检测目标物体面积占图像总面积的比值，当物体-图像比小于0.2时 即认为是一个小目标。小目标检测的难度一般为：
（1）	小目标特征信息匮乏。小目标包含的特征比较浅（如边缘信息、灰度 信息等），更高级的语义信息较少，经过多层卷积特征提取之后，小目标的特征 信息所剩无几。在卷积神经网络中，网络速度不能太慢，也就要求下采样倍率不 能太低。然而下采样倍数比较高的情况下小目标会因为经过多次卷积造成过多的 特征信息丢失，导致检测效果差。
（2）	小目标内容信息匮乏。小目标内容信息的匮乏不仅体现在自身数量的 稀少、在数据集中所占比例稀少，而且还体现在训练过程中的样本类别不平衡、 锚框（anchor）回空间不平衡。一方面，传统的基于anchor的检测算法，会引入 相当的噪声以及在划分类别过程中有严重的正负样本不平衡、简单困难样本不平 衡，这样训练出的网络模型往往不是最优的。另一方面，因为训练数据集太小， 训练数据中小目标数量太少，训练出的网络往往会过拟合，这样，即使在训练集 上有较好的训练效果体现，但在测试集上往往检测性能极差。
诸如图像语义理解、目标识别等的高级计算机视觉任务的效果往往取决于目 标检测算法的具体性能，而小目标检测的效果在很大程度上决定了通用目标检测 算法的性能。因此，小目标检测的研究具有很强的理论价值。日常普通图像、由 X光探测的工业和医疗影像、通信基站摄像机等安装高度高的图像捕获设备采集 到的图像和军用无人机摄像头拍摄的图像中都存在着许多小目标。对图像信息少、 特征不明显的小目标检测的研究，在通信、交通、安全救援等方面具有重要的应 用价值。所以，不管是从理论价值还是从应用价值来看，小目标检测都将是计算 机视觉领域重点关注的研究方向之一。
1.2研究现状与发展趋势
目标检测已经被广泛研究多年。近年来，由于卷积神经网络(Convolutional Neural Networks, CNN)⑺在图像分类中的巨大成功，学者们已经提出了许多基 于CNN的目标检测框架。通常，主流的基于CNN的目标检测框架可以分为两 类：第一类是基于区域提名(regionproposal)的两步检测方法，其将检测问题视 为先定位候选区域，再对候选区域进行分类的两个子问题，主要代表方法有Faster R-CNN (Faster Region Convolutional Neural Networks)回和 R-FCN (Region-based Fully Convolutional Networks)卩〕。第二类是基于端到端(end to end)的一步检测 方法，其在一个步骤中完成定位和分类任务，代表方法有YOLO (You Only Look Once)〔9]和 SSD (Single Shot MultiBox Detector)〔⑼。
2014 年,Ross Girshick 提出的 R-CNN (Region Convolutional Neural Networks ) ["I,奠定了两步检测算法的发展基础。相较于传统的手工滑动窗口，R-CNN提 出区域提名的方法来提取候选区域，然后将每个候选区域缩放到一个固定的大小, 依次对这些区域进行特征提取，然后再对各个区域的特征进行分类。虽然R-CNN 在精度上达到了史无前例的效果，但其冗余的计算导致检测速度太慢。在R-CNN 之后不久，何凯明团队提出的SPP-NET ( Spatial Pyramid Pooling Networks )问不 固定图像的大小，直接将各种尺度的图像输入给卷积层处理，最后经过SPP层处 理的特征保证了固定尺寸的输出，这不仅减少了大量的重复计算，还为多尺度输 入到单尺度输出创造了条件。同年，Ross提出的Fast R-CNN (Fast Region Convolutional Neural Networks)⑴】，是在 SPP-NET 的基础上对 R-CNN 的改造。 不再类似于R-CNN那样使用均匀的随机采样，FastR-CNN采用分层采样，这样 正反向训练都会有所加速。FastR-CNN的网络结构也作了改变，在SPP-NET的 基础上引入了多任务损失函数，用于同时计算回归和分类的损失。2015年，任少 卿提出了一种基于深度学习的RPN (Region Proposal Networks)回全卷积网络, 专门用来生成候选区域。基于RPN的FasterR-CNN最终实现了深度学习算法支 持的端到端训练，大大提高了检测速度和精度。至今，主流的两步检测算法大多 是在Faster R-CNN进行改进的。
2016年，RossGirshick提出的YOLO^奠定了一步检测算法的发展基础。与 两步检测算法相比，YOLO创造性的将物体检测任务直接当作回归问题来处理, 将候选区的生成和分类两个阶段合二为一。它直接将输入图片划分成7x7=49个 网格，每个网格预测两个边界框，最后将这些边界特征作输入到分类网络中进行 预测。同年Liu W提出的的SSD采用CNN来直接进行检测，而不是像YOLO 那样在全连接层之后做检测。除此之外，SSD不仅提取了不同尺度的特征图来做 检测,还采用了不同尺度和长宽比的先验框(类似于FasterR-CNN中的anchor)。 随后，RossGirshick开始对YOLO进行一系列的改进，提出了 YOLOv2Z］和v3 版本。YOLOv2借鉴SSD使用多尺度的特征图做检测，提出跨越层(passthrough layer)的概念将高分辨率的特征图与低分辨率的特征图联结在一起，从而实现多 尺度检测。RossGirshick还自行设计了一个高效的网络结构Darknet-19〔⑷作为特 征提取网络，效果与VGG-16U5〕相当，但运算量缩减了 20%。YOLOv3［⑹又是在 YOLO v2的基础上进行的一大改进，YOLO v3将特征提取网络改为引入了残差 结构的Darknet-53【i6］,其网络结构更深，提取特征的能力更强，检测性能更优。
上面提到的主流目标检测算法主要都是针对通用目标数据集来设计的解决 方案，但是随着目标检测技术的高速发展，其任务难度也不断加大，越来越多的 侧重于小目标检测。因此，小目标检测的研究成为计算机视觉中一项极具挑战性 的任务。由于Pascal VOCU7］、MS COCO^等大型复杂数据集相继的出现，促使 小目标检测研究深入发展。下面简要介绍小目标检测的发展历史：
2017年,Lin等人在Faster R-CNN中嵌入了特征金字塔网络(Feature Pyramid Networks, FPN) 118］,它是一个集自底向上提取特征、自顶向下上采样、侧边融 合通道于一体的横向连接网络。通过这三个结构，网络将低分辨率但语义强的特 征与高分辨率但语义弱的特征融合在一起，获得高分辨率，强语义的特征，十分 有利于小目标的检测。同年，Fu等人分析了 SSD检测算法在小目标检测上的不 足，提出了 DSSD (Deconvolutional Single Shot Detector)【闵，为了进一步提升其 检测精度，Fu使用更强大的Resnet-100］作为模型的特征提取网络，然后在网 络的后端加入了多个反卷积层形成“宽-窄-宽”的“沙漏”结构，有效地扩展了低维 信息的上下文信息。在小目标的检测精度上,DSSD有了很大的提升。与此同时， Ren等人根据SSD检测算法在小目标被遮挡的问题，提出了循环滚动卷积
(Recurrent Rolling Convolution, RRC) ［21L 对于每个 RRC,都有一个单独的损 失函数对其进行指导学习，这可以确保逐渐导入相关特征，因此，可以利用上下 文信息中的深度来检测小目标。2018年，Bharat等人提出了图像金字塔的尺度归 一化(Scale Normalization for Image Pyramids, SNIP)〔九］网络。对于输入的三个 不同尺度的图像，SNIP只对尺寸在指定范围内的目标进行损失回传，这样不仅 减少了域移位(domain-shift,这里指因目标太大而超出检测范围)带来的影响， 还能保证至少有一个目标在指定的尺寸范围内。这种利用大目标的训练提升小目 标的信息表达的方法，将小目标检测算法的精度提升到一个新高度。
但是图像金字塔也有一个非常严重的缺点：即增加了模型的计算量，一个经 过三个尺度放大的图像金字塔要多处理14倍的像素点。所以，Bharat等人又在 同年提出基于高效重采样的图像金字塔尺度归一化(Scale Nonnalization for Image Pyramid with Efficient Resampling, SNIPER)〔如网络来解决这一问题， SNIPER并不是一个检测算法，而是对输入图像的一个采样策略，其采样的结果 将作为输入，输入到目标检测算法中。至此，小目标检测的精度和速度都有了极 大提升。
多分支图像训练的模式还是会带来不小的计算问题。2019年，Li等人提出 三叉戟网络(TridentNetworks)回〕来加快检测速度，但重点仍是提升小目标检测 的精度。Trident Network以单尺度图像作为输入，然后通过并行分支创建尺度固 定的特征图，并行分支中的卷积层共享网络参数但各个卷积层又具有不同的空洞 率。Trident Network从感受野的角度解决了小目标检测精度不高的问题。随后， Kisantal等人㈤提出了小目标检测的数据增广秘籍，从小目标数据本身出发，提 出数据增强策略，使得小目标特征在卷积网络中更好地表达。Pang等人PS根据 小目前检测过程存在的三个平衡问题，将采样均衡、空间均衡和损失均衡统一集 成在Faster R-CNN上，使其成为一个全面平衡的检测器。
以上针对小目标检测的优化算法及策略应用到MS COCO等通用数据集上， 都大大提高了目标检测的性能。但是在一些特定小目标检测上，如医疗影像、遥 感图像的特定应用中，这些优化算法和策略对于其检测结果性能的提升效果并不 显著。因此除了在通用目标检测中提升小目标的检测性能之外，还不乏一批学者 针对特定内容的小目标检测进行了研究。
2016年，Takeki等人〔27］在鸟类生态自动调查中针对天空中小鸟的检测，提 出了一种联合分割的小目标检测方法：分别训练不同尺度的CNN检测器、全卷 积网络(FCNs)和FCNs的变体，并通过支持向量机(Support Vector Machine, SVM)对其结果进行整合，最终获得较高的检测性能。在类似天空的简单大背景 下细小目标的检测上，该方法都有较好的表现。2018年，Ren等人［281针对光学遥 感图像中小目标的检测修正了 FasterR-CNN,采用自顶向下和跳层连接来生成一 个高分辨率特征图作为共享特征输出，这对于检测小的遥感对象非常关键，并且 在RPN、采样策略以及数据增强方面也作了相应改进，该方法在复杂场景中的遥 感目标和检测密集的小型光学遥感目标上都展示了极高的检测性能。2019年， Yi等人【29］针对交通标志图像中小图标的检测提出了一个基于知识的递归注意神 经网络(Knowledge-based RecurrentAttentive Neural Network, KB-RANN)。其采 用了一种新的递归神经网络以细粒度的方式提高检测精度并将自动驾驶领域特 定知识与注意力机制相结合，以提高检测效率。Yi团队最终将该方法部署到自动 驾驶汽车上，得到了成功的应用。
总的来说，不管是基于两步检测算法还是基于一步检测算法的小目标检测都 各有优势：两步检测算法的检测精度高但检测速度较慢，一步检测算法的检测精 度略逊一筹但可以用于实时检测项目中。就当前的研究趋势来看，不管是在通用 数据集上还是在特定场景下，针对小目标检测的研究，主要是对当前主流的目标 检测框架进行网络优化和训练过程优化，以此来获得更高的检测精度、更快的检 测速度。
1.3研究内容概述
本文针对小目标检测存在的难点，对小目标检测算法开展了多项工作。首先 从基于深度学习的目标检测算法着手，通过实践加强对其理论的理解，然后针对 小目标检测在尺度维度和内容维度上的具体难点，对两步检测算法Faster R-CNN 进行优化，具体的研究工作如下：
(1) 调研主流的目标检测模型，并对两步检测算法Faster R-CNN和一步检 测算法SSD进行复现，总结这两种主流的目标检测算法对于小目标检测的优缺 点，根据速度和精度两个指标选取合适的基础模型。
(2) 因为小目标包含的特征信息少，经卷积神经网络提取后特征图所包含 的特征弱，所以增强小目标的特征表达是关键，以此来缩小与大目标的特征表示 差异。本文在Faster R-CNN的基础上，创新性的提出了特征尺度融合和特征尺 度增强的优化结构。其中，特征尺度融合模块通过通道数叠加的方式实现了不同 尺度特征层的融合，并且，采用的超像素卷积并不会引入计算量。基于多尺度感 受野和通道注意力的特征尺度增强模块确保了特征图的稳健。优化网络顶部的特 征图既有底层的细节信息，又有高层的语义信息，有效的解决了小目标经过卷积 网络后特征表达不足的问题。
(3) 因为小目标本身数量稀缺，自身内容的不足以及在卷积神经网络训练 中的不均衡都会导致小目标的检测性能过低。本文在Faster R-CNN的基础上， 首先，引入清洁度的概念并采用梯度协调机制(Gradient Harmonized Mechanism Loss, GHM Loss)㈤啲思想改进损失函数，构成梯度均衡机制，不仅可以缩小 RPN中正负样本不均衡的问题，而且还能大大缩小简单样本与困难样本在数量 上的巨大差距。然后提出了小目标数据增强算法，包括基于旋转的数据集增强策 略和基于掩膜的小目标实例增强策略。不仅使得数据集和目标实例得到扩充，而 且保证训练样本有一定区别。进而达到提高小目标检测性能的效果。
1.4本文内容安排
论文围绕基于深度学习的小目标检测算法展开研究与应用工作，基本的组织 结构如下：
第一章是绪论，首先介绍了本课题的研究背景和意义，然后分别介绍了目标 检测领域两大主流算法的发展历史以及小目标检测领域的研究现状和发展趋势， 最后对本文的主要研究工作和组织结构做了简单的阐述和介绍。
第二章首先详细介绍了经典的目标检测算法Faster R-CNN和SSD,接着详 细叙述了小目标检测的主流优化思想:特征尺度上的FPN、图像尺度上的SNIP、 内容维度上的Focal Loss和Mate Anchor □
第三章介绍了小目标检测算法在特征尺度上的优化改进，基于FasterR-CNN, 分别对特征尺度融合和特征尺度增强进行了优化改进，应用优化后的算法大幅提 升了小目标的检测精度。
第四章介绍了小目标检测算法在内容维度上的优化改进，基于Faster R-CNN 中的RPN网络，引入清洁度概念并采用GHM Loss的思想改进损失函数。并提 出小目标数据增强算法：基于旋转的数据集增强和基于掩膜的小目标增强策略， 并将其应用到汽车轮毂X光数据集中，提高小缺陷检测的性能。
第五章对本文的研究工作进行了总结，并对未来的工作进行了展望。
第二章小目标检测算法的背景知识
2.1基于深度学习的目标检测算法
目标检测是计算机视觉的一个重要分支，其目的是准确判断图像或视频中的 物体类别并定位。传统的目标检测方法存在很多问题，难以满足检测时高精度和 高速度的要求，而基于深度学习的目标检测方法使检测模型在精度和速度方面都 有了很大的提升。目前，基于深度学习的目标检测方法主要分为两大类，第一类 是特定生成候选框后再进行置信度分类和边界框的位置精修的两步检测算法，本 节主要以Faster R-CNN为例进行介绍；第二类不用预先生成候选框，而是直接 将目标边框进行分类和位置回归的一步检测算法，本节主要以SSD为例进行介 绍。
2.1.1两步检测Faster R-CNN算法介绍
Faster R-CNN虽然是在R-CNN、FastR-CNN的基础上发展而来，但它却是 目标检测发展史上一个重要的里程碑：不仅是第一个所有检测阶段都以深度学习 技术为核心的目标检测算法，更是整个网络端到端进行训练的开端。Faster R- CNN在一个大网络中完成了特征提取、生成候选框、分类回归等步骤，使得综 合性能有极大提高。Faster R-CNN的基本结构如图2-1所示：
分类

图2-1 Faster R-CNN基本结构
从图2-1可以看出，Faster R-CNN可以分为四个主要内容：
（1）	特征提取网络。特征图的质量好坏严重影响检测器的性能，不变形和 同变性是图像特征表示中两个重要的属性。分类问题需要具有不变形的特征，因 为分类的目的是学习高层语义信息；而定位问题需要具有同变性的特征，因为它 需要辨别位置、尺度等变化因素。目标检测同时包含分类与定位两个任务，因此 检测器需要不变形和同变性共存的特征图。
特征提取网络要求输入的图像尺寸保持一致，所以先将尺寸不一的图像统一 重塑成MxN的图像。特征提取网络由卷积层、激活层和池化层组成：卷积操作 通过使用特定大小的卷积核，作用于图像的局部区域从而获得图像的局部特征； 激活操作是把卷积结果输入到一个特定的非线性激活函数中进行变换，以提高卷 积网络的表达能力；池化是将特定大小的池化核在图像上滑动从而聚合提取特征 的过程，能够极大程度上压缩特征的维度和降低网络的计算量。FasterR-CNN中 使用的特征提取网络是VGG-16,它由13个卷积层，13个激活层和4个池化层 组成。所有的卷积层和激活层不改变输入输出的大小，而池化层会使输出长宽都 变为输入的一半。那么，一个MxN大小的图像经过VGG-16网络输出（M/16） x （N/16）大小的特征图。该特征图被共享用于后续RPN网络和全连接层。
（2）	RPN网络。RPN用于生成候选框。在Faster R-CNN之前，目标检测 模型都是利用传统图像处理的方法生成候选区域，不仅速度很慢，而且生成的候 选区域质量也不够好o Faster R-CNN直接使用RPN生成候选框，是Faster R-CNN 的巨大优势，因为RPN开创性地引入锚框（anchor）的机制，完全利用卷积神 经网络的方式实现候选区域的生成，能极大提升检测框的生成速度。
首先介绍一下anchor这个概念：将特征图上的每一个点映射回原图，得到 这些点在原图上的坐标，然后在这些点周围取一些提前设定好的区域就是anchor, 这些选定的anchor是用来训练RPN的。假设对特征图上的每个点选取K个anchor （原论文中K=9）,特征图的大小为HxW,那么一共得到HxWxK个anchor.,
然后在生成了 HxWxK个anchor的特征图后并行接入两个卷积层分别做置 信度预测和边界框回归。这两条分支输出的张量大小分别是HXWX2K和 HxWx4K,分别对应每个点的每个anchor是否是目标物体的概率（是或者否，2 个分类结果）以及它所对应的物体的坐标（4个坐标回归系数）。
在训练的过程，RPN网络的损失包含一个用于二分类的交叉爛损失和一个 用于回归的smoothL1损失，对于每个anchor,如果它和图片中某个物体的IOU （面积的交/面积的并）大于某个阈值，就认为它属于前景类，否则认为是背景 类，对于是背景类的anchor回归损失就是零，最后利用边界框回归来修正anchor 以获得精确的候选框。
8

(3) ROI (Region of interest)池化层。通过RPN网络生成的候选区域， 是一系列尺寸位置不一的候选框。但是由于后续的分类回归层是全连接层，输入 神经元的个数固定。这就要求输入到分类回归层的候选区域尺寸一致，因此引入 ROI池化的方式，将不同大小的候选区域规范化到同样的大小。
ROI池化借鉴了 SPP-NET的思想：先将候选识别区域划分为相等大小的部 分(其数量与输出的维度相同，例如7x7),然后找到这些区域(一共49个子区 域)的最大值，最后将这些最大值直接输出。结果是，从具有不同大小的矩形列 表中，快速获得具有固定大小的相应特征映射的列表。但在这一过程中，ROI池 化层输出的维度并不取决于输入特征图的大小，也不取决于候选区域的大小，它 仅由区域划分的部分数量决定。
(4) 分类回归层。通过ROI池化层已经得到所有候选区域维度统一的特征 向量，然后接入全连接层进行多任务分类并输出分类置信度，同时利用每个候选 框的偏移量进行边界框修正，得到最终的目标检测框。
这里也采用了与RPN类似的并行分支，但不同之处在于，RPN网络是利用 卷积的方式对于所有的anchor同时生成分类和回归结果，而这里是只对单个候 选区域生成分类回归结果，因此使用的是全连接的方式。这一部分的损失也是多 任务损失，是分类和边框回归损失之和，分类损失依然是交叉燔，边框回归损失 是smooth L1。Faster R-CNN网络细节图如图2-2所示：

2.1.2 一步检测SSD算法介绍
一步检测算法的主要思路是均匀地在图片的不同位置进行密集抽样，抽样时 可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归, 整个过程只需要一步，所以其优势是速度快。但是均匀密集采样的一个重要缺点 是训练比较困难，主要原因是正样本与负样本的分布极度不均衡，导致模型准确 度比两步检测算法略低。SSD是一步检测算法的典型代表，其整体框架如图2-3 所示。


图2-3 SSD整体框架
SSD输入一幅图片(300x300),先将其输入到预训练好的分类网络(VGG- 16)中来获得不同大小的特征图。针对VGG-16,这里稍作了一些改动：图2-3 中Conv6和Conv7代替了 VGG-16的FC6和FC7,并且不再包含Dropout层和 FC8o 然后抽取 Conv4_3、Conv7、Conv8_2> Conv9_2> Convl0_2> Convll_2 层 的特征图，分别在这些特征层上面的每一个点构造K个不同尺度大小的先验框 并分别进行检测和分类，一步生成多个预测框。最后将不同特征层获得的预测框 结合起来，经过非极大值抑制(Non-Maximum Suppression, NMS)〔3口方法来抑 制掉一部分重叠或者不正确的预测框，生成最终的检测结果。
可以看出SSD采用CNN来直接进行检测的同时，增加了两点改进：
(1)、采用多尺度特征图用于检测。CNN网络一般前面的特征图比较大， 后面会逐渐采用stride-2的卷积或者池化来降低特征图大小，SSD采取的方法正 如图2-4(b)所示，底层比较大的特征图和高层比较小的特征图，都用来做检测。

图2-4单层特征图预测和特征金字塔预测对比图
在SSD中如果有多个目标框(ground truth),每个anchorC原文中称作default box)会选择对应到IOU最大的那个ground truth□ 一个anchor只会对应一个
10 ground truth,但一个ground truth都可以对应到大量anchor。原论文中认为仅仅 靠同一层上的多个anchor来回归，远远不够。因为有很大可能这层上所有anchor 的IOU都比较小，用这种anchor来训练误差会很大。原论文将VGG-16的最后 两层全连接改为卷积层并额外增加四个卷积层来构造网络，通过同时对六个层级 上的anchor计算IOU,就能找到与ground truth的尺寸、位置最接近(即IOU 最大)的一批anchor,高层特征anchor覆盖范围大可以检测大目标，低层特征 anchor覆盖范围小可以检测小目标。
(2)、Anchor尺寸设计。YOLO通过网格划分的方式，每个小格只预测一 个回归框，当目标物体存在重叠度较大，或者目标物体较为密集时，YOLO存在 较大的漏检。Faster R-CNN中的anchor思想能够很好地解决这一问题。与Faster R-CNN的不同的是，SSD在不同尺度的特征层上都生成anchor,每个层级上的 anchor尺寸又有相应的设计。anchor尺度(默认为正方形)的计算方法如公式(2- 1)所示：
Sk = smin + 警-響(k -l),ke [l.m]	(2-1)
m — 1
其中tn表示不同尺度的特征层数量，原论文中为6, s诚“是最底层的anchor 尺寸，这里使用的是0.2, Sjnax是最高层的anchor尺寸，这里使用的是0.95。
2.2小目标检测算法的相关设计与优化
在现有的目标检测文献中，大多数是针对通用目标数据集来设计解决方案， 因此对于图像中的小目标来说，检测效果不是很理想。小目标分辨率低，图像模 糊，携带的信息少，但是在许多下游任务中小目标检测又相当重要。因此有很多 学者提岀小目标检测的一些方法，这些方法大多是建立在现有的目标检测基础上, 对其进行的一些改进或者优化。这些改进和优化总体围绕着尺度、内容等维度进 行着，本节接下来将分别介绍小目标检测在尺度维度、内容维度上的相关设计与 优化。
2.2.1尺度维度的设计与优化
小目标检测难度之所以大，正是因为它太“小”了，如果将“小”目标变“大”了， 那么相应的检测效果自然上去了。如何将小目标变成大目标呢？围绕着目标的尺 度问题，许多学者相继在特征尺度和图像尺度提出了相应的优化方案。本节主要 介绍FPN和SNIP：
(1)在特征尺度上，以FPN为主要代表。它是一个集自底向上提取特征， 自顶向下上采样，侧边融合于一体的横向连接网络。通过这三个结构，网络将低 分辨率但语义强的低层特征与高分辨率但语义弱的高层特征融合在一起,最终获 得高分辨率，强语义的特征，有利于小目标的检测。图2-5展示了利用特征尺度
11
的四种常见方式结构。



图2-5利用特征尺度的四种常见方式结构图
图2-5 (a)是一个图像金字塔模型，它将图像缩放到不同尺寸，然后分别对 这些不同尺度的图像执行目标检测操作，最后将所有的检测结果结合到一起。这 种方法的缺点是计算量大，需要大量的内存。图2-5 (b)是对图像金字塔的一种 改进思路，学者们发现利用卷积网络本身的特性，可以获得不同尺寸的特征图， 这样其实就类似于在图像的特征空间中构造金字塔，最后利用最高卷积层上的特 征图进行预测。它的缺点是仅仅关注深层网络中最后一层的特征，却忽略了其它 层的特征。图2-5 (c)所示的架构就是同时利用低层特征和高层特征，分别在不 同的特征层，同时进行预测。它的缺点是获得的特征不具鲁棒性，都是一些弱特 征。而FPN的架构如图2-5 (d)所示，它在图(c)的基础上得到每一层的特征 图，之后采用自顶向下的方法将小的特征图上采样之后与上一个特征图融合，融 合之后再做预测，依次如此，即可得到多个预测结果。

FPN算法大致结构如图2-6所示，它包含了三个步骤：自底向上的特征提取、 自顶向下的上采样(upsampling)和横向连接。
在特征提取的过程中，特征图的大小在经过池化层后会变小，而在经过卷积 层的时候不改变，这里将特征大小相同的层统一为同一个stage,越往后的stage 中的特征较前一 stage小，这样就构成了特征金字塔。在上采样的过程中，原文 为了方便采用的是最近邻上采样，一般的上采样有三种方式：最近邻上采样、反 卷积、线性插值，可以根据网络模型的复杂度选取具有不同计算量的上采样方式。 在横向连接的过程中，先使用1x1的卷积核是为了减少特征图的个数而不改变 特征图的尺寸大小。然后将上采样的结果和自底向上生成的相同大小的特征图进 行融合。在融合之后还会再采用3x3的卷积核对每个融合结果进行卷积，目的是 消除上采样的混叠效应。
(2)在图像尺度上，以SNIP为主要代表。SNIP利用了多尺度训练(Multi- ScaleTraining, MST)的思想，即训练一个检测器时采用不同尺度的图像进行训 练。但在MST方法中，单纯的将小目标放大也会影响检测效果，因为随之变大 的还有大目标，大目标变得太大而超出检测范围，导致MST性能并不理想。因 此SNIP的做法就是只对尺寸在指定范围内的目标回传损失，这样就能减少 domain-shift带来的影响。又因为训练过程采用了类似MST的做法，所以每个目 标在训练时都会有几个不同的尺寸，那么总有一个尺寸在指定的尺寸范围内。


图2-7 SNIP算法示意图
SNIP的算法过程如图2-7所示，它采用三个尺度的图像输入，并通过RPN 生成相应的候选框，但每个分支的RPN只负责一个尺度范围的候选框生成，不 在范围内的候选框直接舍弃，这样的设计保证了每个CNN分支在判别候选框是 否为前景时，只需针对最易分类的中等尺寸的候选框进行训练。在anchor的选 取上，如果ground truth的大小在该分支指定的范围内，就被标记为有效，否则 就被标记为无效。在生成anchor并给anchor分配标签的时候，检查该anchor是 否和某个无效的ground truth的交并比超过0.3,若存在，则该anchor会被视作
13
无效；若不存在，则会被视为有效。这些无效anchor在训练的时候梯度并不会被 反向传播。这就相当于在每个分辨率上，只对大小合适的目标进行训练。最后将 多个分支上的预测结果进行重塑(rescale),并进行NMS,得到最后的预测结果。 不管是特征金字塔还是图像金字塔，都是从尺度维度去解决小目标信息匮乏 的问题，FPN、SNIP的思想都可以很好的嵌入到Faster R-CNN的检测模型中， 使得小目标检测性能提升很高。
2.2.2内容维度的设计与优化
在目标的内容维度上，小目标检测的难度不仅来自于自身数量的稀少、在数 据集中所占比例稀少，而且还与训练过程中的极度不平衡密切相关。目标检测任 务集分类与定位于一体，所以不平衡包含了类别不平衡和空间不平衡。本节主要 介绍解决分类不平衡的Focal Loss卩2〕和解决空间不平衡的MateAnchor[33]：
(1)在解决类别不平衡的问题上，以Focal Loss为主要代表。类别不平衡 主要包括检测目标中正负样本不平衡、训练过程中简单样本和困难样本数量不平 衡。
对于简单困难样本不平衡的问题，在线难分样本挖掘(Online Hard Example Mining, OHEM)印】进行了最早的尝试，它使用模型的输出概率，选出部分困 难样本，然后根据这些样本的权重，更新网络参数，在一定程度上缓解了样本不 平衡造成的问题。OHEM算法虽然增加了困难样本的权重，但是忽略了简单样 本。而何凯明团队提出的Focal Loss是现阶段解决正负样本、简单困难样本不平 衡的主流方法。
何凯明团队提出一种新的损失函数 Focal Loss,这个损失函数是在标准 交叉矯损失基础上修改得到的，可以通过减少简单样本的权重，使得模型在训练 时更专注于困难样本。
对于二分类来说，标准的交叉燔(crossentropy, CE)损失函数如公式(2-2)
所示:

这里P*是anchor的标签，取值为1或者0,分别代表正负样本，p是预测其 输出为1的概率，取值范围为0〜1。当p* = l时，损失L = -log(p),厶与预 测输出的关系如图2-8左图所示：很显然，对于正样本的预测，预测输出越接近 真实样本标签p* = l，损失函数Z越小；预测输出越接近0,厶越大。
14


图2-8 L与p的关系图
而当p* = 0时：损失厶=-2og(l-p),厶与预测输出的关系如上右图：同 样，预测输出越接近真实样本标签0 (p值越小)，损失函数厶越小；预测输出越 接近1,厶越大。函数的变化趋势也完全符合实际需要的情况。
为了简化交叉矯损失，用氏代替p得到公式(2-3)：
(p ifp* = 1
Pt — (1 — p otherwise	(-丿
便可知CE(p, p*) =	=-Zog(pt)。既然在训练的时候正负样本的数量
差距很大，一种常见的做法就是给正负样本加上权重，负样本出现的频次多，那 么就降低负样本的权重，正样本数量少，就相对提高正样本的权重。具体调整如 下：
CE(p』=-at Iog(pt)	(2-4)
at的定义与pt类似，当p* = 1时，at = a；当p* = 0时，CQ = 1 - a, a的 范围也是0到1。因此可以通过设定a的值来控制正负样本对总的损失的共享 权重。当a = 0.5时退化为标准交叉矯。
公式(2-4)虽然可以控制正负样本的权重，但是没法控制简单样本和困难样 本的权重。类似于添加权重因子a的思想，Focal Loss提出调制系数(1 - pt)Y来 处理简单、困难样本的问题：
FL(p』=-(1 - Pt)Y log(pt)	(2-5)
这里的y称作聚焦参数(focusing parameter), y > 0o当一个样本被分错的 时候，Pt是很小的(结合公式(2-3),当p* = 1时，p < 0.5才是错分类，此时,pt 就比较小，反之当p* = 0时，p>0.5是错分)，因此调制系数就趋于1。当s趋 于1时(此时分类正确而且是简单样本)，调制系数趋于0,也就是对于总的损 失的贡献很小。当y = 0时，Focal Loss就是传统的交叉爛损失，当y增加的时 候，调制系数也会增加。Focal Loss的核心就是用一个合适的函数去度量困难样 本和简单样本对总的损失的贡献。
(2)在解决空间不平衡的问题上，以M^eAnchor为主要代表。从2.1节的
15 介绍可以知道，现阶段不管两步检测还是一步检测，都应用了 anchor机制，现阶 段anchor机制主要的问题有以下两个：1) anchor需要特别的设计，并且无法覆 盖特殊情况，如果设计得不好，会降低最后的检测效果，尤其是小目标检测的效 果；2)为了保持较高的召回率，需要大量的anchor,而其中大多数都是负样本， 又会出现前后背景及其不平衡的问题，影响小目标的检测精度。
Anchor的设计与优化同样能够增强小目标的检测效果，其中最具代表的是 MetaAnchoro Anchor将框空间(包括位置，大小，类等)划分为离散区间，并通 过相应区间中定义的anchor函数生成每个对象框。X表示从输入图像提取的特 征，那么，第i个区域的anchor公式(2-6):
珂(X；E)=(呛 s(x；o 严)，矚 eg(x；e「g))	(2-6)
其中bi G B (即先验框，在之前的工作中可以理解为FasterR-CNN原论文中 固定尺寸的9个anchor),同时，沪：：(•)判别是否存在一个目标框与第i个区域 相关联，笛了(•)则把目标框(如果有)的相对位置回归到先验S； $表示与 anchor相关的参数。
而MetaAnchor函数由6动态生成，而不是通过枚举每一个可能的边界框。 其在分别建模相应的anchor函数时，可写为公式(2-7)：
咒产 ggw)	(2-7)
其中，§(•)称之为anchor函数生成器，它把任一边界框b：映射到相应的 anchor函数几：。“•)函数建模学习anchor的尺寸，w为网络学习的参数。虽然 6仍需要先验知识提前设定，但起码初始化的anchor尺寸不再是一成不变的， 并且这样生产的anchor具有多样性。在MetaAnchor框架中，如公式(2-7)所 示，anchor函数生成器把S映射到相应的anchor函数，从而扮演了关键角色。 为利用神经网络建模，首先要假设对于不同的0, anchor函数共享同一方程，但 是参数不同，这意味着公式(2-8)：
珂=F(X； 0b.)	(2-8)
接着，由于每一个anchor函数的区别仅在于其参数，生成器由此可预测，如 公式(2-9)所示：
。如=w) = 0* + 7?(bf; w)	(2-9)
其中0*代表共享参数(独立于0且同样可学习)，残差项欠(0; w)取决于先 验通过一个简单的二层神经网络实现：
欠(0； w) = W2a(W1bi)	(2-10)
这里，必和购皆是可学习参数，"(•)是激活函数,隐藏神经元的数量用m
16
表示，实际上m通常远小于0乩的维数，这导致预测的权重聚集在一个明显低阶 的子空间，这就是为什么在公式（2-9）中把方程化为一个残差项，而不是直接使 用。为应用MetaAnchor,需要重新设计原始的anchor函数，保证其参数生成于 自定义6。首先要考虑如何编码S，—个包含位置、尺寸、分类等信息的向量。 在实验中，6主要于anchor尺寸相关，并表示为：
ahs awt	、
6 =盹丽」昭丽）	Q-ll）
其中a加和aw：是相应anchor的高和宽，AH, AW是作为正则化项的“标准 锚点框"的尺寸。至此，MetaAnchor,其anchor函数可由任意自定义的先验框动 态生成。加上权重预测，MetaAnchor可与大多数基于anchor的目标检测系统协 同工作，相较于预定义anchor方法，MetaAnchor对于anchor设置和边界框分布 更为鲁棒，并在迁移任务上变现出潜力。
2.3本章小结
本章首先介绍了基于深度学习目标检测算法两步检测和一步检测算法的主 要思想和模型结构。然后介绍了小目标检测在尺度维度、内容维度上相关设计与 优化。其中，详细介绍了在尺度维度上基于特征尺度的FPN和基于图像尺度的 SNIP＞在内容维度上基于前后景平衡的Focal Loss以及在anchor机制中，基于 动态生成的MateAnchoro这些设计优化方案均能嵌入到一步检测和两步检测算 法中去，进而提升小目标检测的性能。
17
第三章基于特征尺度的小目标检测研究
为提高小目标检测算法的精度，特征尺度上的研究已经司空见惯，但基于特 征尺度的方法主要集中在特征融合，且融合方法各有利弊，本章提出一个新颖的 特征尺度融合模块，它能有效的将多个特征层融合在一起但几乎不增加计算量。 为更好的发挥特征层的表达能力，本章还提出一个全新的特征尺度增强模块，最 后将这两个模块嵌入到两步检测Faster R-CNN算法中，目标检测尤其是小目标 检测的性能提高显著。
3.1基于Faster R-CNN的特征尺度融合算法
基于低层特征语义信息少但空间信息丰富、高层特征分辨率低但语义信息较 强的特点，多尺度特征融合的方法层出不穷。但如何高效的将各层特征融合起来 且不增加计算量，是亟待解决的问题。本节为更好的解决该问题，提出了一个全 新的特征尺度融合模块(Feature Scale Fusion Module, FSFM)。
3.1.1特征尺度融合模块
前文提到的FPN虽然都通过自顶向下的方法将各层特征融合在一起，但因 在不同层进行独立预测，计算量大大增加。本节提出的FSFM高效的将高、低特 征层融合在一起的同时，几乎未增加计算量，相较于FPN,优势显著。


图3-1 FPN与FSFM融合过程示意图
图3-1分别展示了 FPN和FSFM的融合方法,FPN的具体实现方法已在2.2.1 中详细说明，这里不再赘述。而FSFM是通过低层大特征进行最大池化、高层小 特征进行超像素卷积，从而获得特征大小相同但通道数不同的几个特征层，最后 这些特征层通过通道数叠加的方式进行融合从而得到最终融合的特征图。
(1)最大池化。目标检测框架的骨干网络不管是采用VGG-16还是ResNet- 101,池化层必不可少，它既能保持旋转、平移、伸缩等的不变性，又能减少特 征，减少参数。最大池化，即对邻域内的特征点取最大值，这样能够有效的减少 卷积层参数误差造成估计均值的偏移，保留更多的纹理信息。最大池化需要满足
18
梯度之和不变的原则：前向传播是把邻域内的特征点中最大值传递给后一层，而 其它像素的值直接被舍弃掉；反向传播是把梯度直接传给前一层的某一个像素, 而其他像素不接受梯度，也就是为零。最大池化的前向与反向传播过程如图3-2：


图3-2最大池化的前向与反向传播过程示意图
(2)超像素卷积。低层特征图(128x128x64)分辨率高且尺寸较大，通过 stride=4的最大池化可以变为高分辨率的小特征图(32x32x64),特征图尺寸变 小，分辨率和通道数不变。高层特征图语义信息丰富且尺寸较小，如何将其变为 高分辨率的大尺寸特征图呢？在2.2.1中提到，通过上采样可以实现小尺寸变为 大尺寸，但不管是最近邻上采样还是反卷积，都会引入一定的计算量和一定的噪 声且不改变其为低分辨率特征的事实。众所周知，图像超分辨率重建技术是将低 分辨率(Low Resolution, LR)的模糊图像放大到高分辨率(High Resolution, HR)空间进行图像重建而获得高清图像的一种技术。图3-3展示了图像重建过


图3-3 LR到HR重建过程示意图
输入的低分辨率图像通过骨干网络的一系列常规卷积操作得到LR特征图, 该特征图的大小相比于原图像大小不变，但是特征通道数变为r2 (r为上采样因 子)。对于一个由L层组成的重建网络，前L-1层(即图3-3中常规卷积操作部 分)的实现过程可以表示为：
19


其中，必、bt, l e (1,L-1)分别是网络学习的权值和偏差，坷是一个大小 为nz_j xniXkiXki的2D卷积张量，其中血是L层的特征数量，俭是L层上 卷积核的大小，0是长度为％的向量偏置，0是激活函数。最后一层尸将特征 图lLR转化为HR图像IHR -
在L层，也就是超像素卷积层，将每个"R上的r2个通道通过周期性像素重 排函数得到一个［HR上的一个rxr区域，即大小为HxWx?的特征图被重新排列 成田xrWxl大小的高分辨率图像。其计算方式如下：
IHR = fiQLR) = PS(Wt * /'1(卩R) + bj	(3-3)
其中PS(-)是周期性像素重排函数，具体表示式为公式(3-4)所示:
PS(T)x,y,c = Tlx/ri,[y/r],C*r*mod(y,r)+C*mod(x,r)+c
其中x,y,c分别代表重建图像的长、宽和通道数。至此完成图像超分辨率重 建。重建过程的核心步骤是超像素卷积，该过程虽然叫做超像素卷积，但是未引 入卷积操作，不会引入计算量和噪声。
本文从图像超分辨率重建技术得到启发，但不是在图像尺度上进行重建，而 是在特征尺度上，将低分辨率的高层特征图通过超像素卷积，重建为高分辨率的 高层特征图，而且还能保留丰富的语义信息。假设超像素卷积层输入的特征图大 小为HxWxC,上采样因子为r。从图3-4中可以看出，超像素卷积层通过压缩特 征图中的通道数来扩展宽度和高度。



图3-4超像素卷积
其数学公式可展示为公式(3-5), HR是高分辨率特征图，ILR是低分辨率特 征图。
LR
[x /rj ,[y /r\,r,Tnod(ytr')+mod(x#r)+c-r2
其中x,y,c分别代表高分辨率特征图的长、宽和通道数，其与低分辨率特征
20 图上对应关系为公式（3-5）所示。与使用反卷积（在反卷积操作之前，池化步骤 中必须填充霎）相比，超像素卷积没有额外的参数和计算开销。最后，计算重建 的像素均方误差（MSE）作为训练网络的损失函数。损失函数数学公式可表示为 公式（3-6）：
rH rW
£ = r^HW 工》（醪° - 閲C2）	G-6）
X=1 X=1
高分辨率的低层特征通过最大池化、低分辨率的高层特征通过超像素卷积变 成特征大小统一而通道数不同的特征层，最后将这些特征层在通道上进行叠加， 完成最后的融合。最后得到的特征图既包含低层的位置信息，又包含高层语义信 息。FSFM结构理论上可以嵌入到各种骨干网络中，且几乎未增加计算量。
3.1.2 FSFM 嵌入 VGG-16
本章FasterR-CNN的backbone采用的是VGG-16。表3-1详细展示了 VGG-
16 网络结构。
表3-1 VGG-16网络结构表
层名
输出尺寸（输入为256x256x3）
参数

256x256x64
3x3
64
Black1
256x256x64
3x3
64

128x128x64
2x2
2

128x128x128
3x3
128
Black2
128x128x128
3x3
128

64x64x128
2x2
2

64x64x256
3x3
256
Black3
64x64x256
3x3
256

64x64x256
3x3
256

32x32x256
2x2
2

32x32x512
3x3
512
Black4
32x32x512
3x3
512

32x32x512
3x3
512

16x16x512
2x2
2

16x16x512
3x3
512
Black5
16x16x512
3x3
512

16x16x512
3x3
512
从表3-1中可以知道，卷积层均采用相同的卷积核参数，卷积核的尺寸（kernel size）是3,即宽（width）和高（height）均为3, 3x3是很小的卷积核尺寸，结 合其它参数（步长stride=l,填充方式padding=same）,这样就能够使得每一个卷 积层的张量与前一层保持相同的宽和高。池化层均采用相同的池化核参数，均为 2x2,模型是由若干卷积层和池化层堆叠的方式构成，所以VGG-16卷积神经网

21
络的设计思路简单，实际效果却十分有效。

图3-5 VGG-16网络简化流程图
VGG-16的简化网络流程如图3-5所示。网络模型的检测速度和检测精度一 般是相互约束的，主要是因为输入图像的尺寸——图像越大，计算量越大，检测 速度就越低，但检测精度会相应提高。本节在权衡了速度与精度后，选取图像重 塑大小为256x256。一张256x256的三通道彩色图像经过VGG-16的五个Block 块后，输出一个大小为16x16x512的特征图。
VGG-16模型的总参数为：14714688个，本文将FSFM结构嵌入到VGG-16 中去，没有改变VGG-16自身任何参数的同时，也未引入任何新参数，使得模型 本身没有增加计算量，FSFM网络流程如图3-6所示：

图3-6 FSFM网络流程图
FSFM舍弃Blackl的输出、对Black2的输出做2x2的最大池化、对Black3 的输出做1x1的卷积、对Black4的输出做2x2的超像素卷积、对Black5的输出 做4x4的超像素卷积，最后将这四层特征层按通道数叠加，融合成最终的特征 图。表3-2展示了 FSFM的输出细节。最后即可将32x32x544的特征图输入到 Faster R-CNN的后续网络结构中。
表3-2 FSFM输出细节
层名
VGG-16输出尺寸
FSFM输出尺寸
Blackl
128x128x64
None
Black2
64x64x256
32x32x128
Black3
32x32x512
32x32x256
Black4
16x16x512
32x32x128
Black5
16x16x512
32x32x32
Final
16x16x512
32x32x544
22
3.2基于Faster R-CNN的特征尺度增强算法
如前文部分所述，目标检测面临着两个挑战：分类和定位。然而，分类的要 求与定位问题自然是矛盾的。对于分类，模型需要对输入的变换保持不变。而对 于定位，模型需要对转换敏感，因为定位过程依赖于输入位置。调和这两方的矛 盾需要一个强有力的特征表达。人脑在观察事物的过程中会先关注注意力焦点， 通过模仿人脑这种机制，研究人员在目标检测中会更多的关注图像上含有丰富的 具有判别力的区域，例如在行人检测中更多的关注图片中的行人区域、行人的上 半身部分、行人的背包等。
本节从上述机制获得启发，不过不是在图像尺度上进行关键信息选择，而是 在特征尺度上，提出一个全新的特征尺度增强模块(Feature Scale Enhancement Module, FSEM),旨在增强特征图中对当前任务更有用的特征的表达，抑制无用 特征的表达。
3.2.1多尺度感受野增强
神经网络中，感受野用来表示网络每一层输出的特征图上的像素点在原始图 像上映射的区域大小。感受野越大意味着它可能蕴含更为全局、语义层次更高的 特征；相反，感受野越小则表示其所包含的特征越趋向局部和细节。因此，利用 多尺度感受野进行特征增强是十分重要的。
空洞卷积支持指数级扩张的感受野，而不丢失分辨率。在普通卷积中，感受 野大小和卷积过后特征图大小的计算方法如下所示：
= (k - 1) x I + 1	(3-7)
o = [^=4^] + 1	(3-8)
其中，k为卷积核大小，i为卷积的数层，尺表示经过i层卷积后，感受野的 大小；讪为输入图像的大小(只考虑输入宽、高相等情况)，p为填充的像素数， s为步长，o为卷积过后特征图的大小。使用常规的3x3卷积，三层卷积后F3=(3- l)x3+l=7 ,即感受野大小为7x7。常规卷积的感受野大小随卷积层数呈线性增长， 但空洞卷积的感受野大小随卷积层数是呈指数增长的：


图3-7空洞卷积感受野增长示意图
如图3-7所示，卷积核没有红点标记位置为零，红点标记位置同正常卷积核。 假设原始特征为F0,首先使用空洞率为1的空洞卷积生成Fl, F1上一点相对F0 的感受野为3x3 （如图3-7 （a））；然后使用空洞率为2的空洞卷积处理Fl生成 F2 （如图3-7 （b））,使第一次空洞卷积的卷积核大小等于第二次空洞卷积的一个 像素点的感受野，即F1上一个点综合了 F0上3x3区域的信息，则生成的F2感 受野为7x7,即整个图3-7 （在b）深色区域；第三次处理同上，第二次空洞卷积 的整个卷积核大小等于第三次空洞卷积的一个像素点的感受野，F2上每个点综 合了 F0上7x7的信息（感受野），则釆用空洞率为3的空洞卷积，生成的F3每 一个点感受野为15x15。可以看出，空洞卷积的感受野是呈指数的增长，其计算 公式如下所示：
Ft = 2i+k~2 一 1	（3-9）
同样地，k为卷积核大小，i为卷积的数层，尺表示经过i层卷积后，感受野 的大小。经过空洞卷积后的特征图大小为:
w — 上 _ (上一 1) x (r _ 1) + 2p
w为输入图像的大小（只考虑输入宽、高相等情况），p为填充的像素数，s 为步长，r为空洞率，o为卷积过后特征图的大小。
从空洞卷积的计算过程可以看到，它既带有常规卷积的滤波功能，还能以指 数形式扩张感受野大小；它同时具有池化的泛化作用，但又不会造成特征图的减 小。因此，本节构建了一个并行三分支架构，使用不同空洞率（rate=l, 2, 3） 的卷积来获得多尺度感受野，以增强特征的表达。对于每个分支，首先使用一个 1X1的普通卷积对特征图进行跨通道的交互和信息整合，然后使用两个3x3的空
洞卷积获取指数级增长的感受野，最后再次使用1X1普通卷积减少卷积核参数。 最后因为三个并行分支的特征图大小不一致，故采用通道像素叠加的方式将三个 特征映射连接起来（这里注意，与前文的通道数叠加区别），达到增强多尺度感
24


受野信息的效果。具体框图如图3-8所示:

Output
图3-8多尺度感受野增强模块
3.2.2通道注意力增强
通道注意力机制的核心思想是通过损失的大小来学习通道权重，使得有效的 特征通道具有较大的权重，而无效的特征通道具有较小的权重。挤压操作（SQ） 和激励操作（EX）是通道注意力机制的核心操作。挤压操作沿着空间压缩特征， 并将每个二维的特征通道转换为实数，从而使输出维度与输入特征通道的数目相 匹配。为了利用挤压操作中聚集的信息，激励操作能够完全捕获信道相关性，它 能够学习通道之间的非线性交互，并学习它们的非互斥关系。通道注意力增强模 块的具体结果如图3-9所示：

设进入通道注意力增强模块的特征图为X,大小为HxWxC,通过挤压（其 实就是在每个通道上求平均值）得到一个权重相同的lxlxC特征向量Z,该特征 向量再通过激励操作，给每个通道根据相关性进行激活，即先进入ReLU激活函 数，再进入sigmoid激活函数，得到一个权重各不相同的lxlxC特征向量S,最 后，将原始的特征图X与之相乘，即获得权重各不相同的HxWxC特征图Y,就
25

此，完成特征增强的全过程。
其中，挤压操作和激励操作的实现过程分别如公式(3-11)、(3-12)所示: W H
ZcFq(u)=爲》Gm)
i=l j=l
sc = Fex(Z,W} = &@(Zc，W)) = 8 @(必6(W1Z』))	(3-12)
其中&表示ReLU激活函数，g表示sigmoid激活函数，其具体表达式如公式 (3-13)、公式(3-14)所示。％和“2分别是两个全连接层的权值矩阵。
X	if x > 0
.0	if x < 0
1
Sigmoid^x)=亍工尹
最后的输出特征图Y表示为｛和氏的向量积： Y = FscaZe(S,U) = Sc-Uc
3.2.3特征尺度增强模块
本节提出的特征尺度增强模块(FSFM)由两部分构成：第一部分使用多尺 度感受野模块获取更高感受野；第二部分使用通道注意力增强模块来增强有用信 息并抑制无用信息。具体结构如图3-10所示。

最后，将本文提出的FSFM和FSEM嵌入到FasterR-CNN中，完成基于特 征尺度的小目标检测算法的建模。具体框架结构如图3-11所示：
26


3.3实验结果与分析
本章提出的基于FSFM和FSEM的Faster R-CNN检测框架先在通用数据集 VOC ±进行验证，并根据物体-图像比将VOC数据集划分出大、中、小物体， 将Faster R-CNN的预测结果作为baseline,比较了使用FSFM+FSEM在小目标 检测效果上的性能；然后在以富含小目标图像著称的COCO数据集上与当前主 流目标检测算法进行比较，验证了本章在特征尺度上的优化对于提高小目标检测 精度的可行性。
3.3.1评估指标
IOU (Intersection-over-Union)即交并比，是一种测量在特定数据集中检测相 应物体准确度的一个标准。IOU表示了产生的候选框(candidate bound )与目标 框(groundtruth)的重叠度，也就是它们的交集与并集的比值。相关度越高该值 越大。最理想情况是完全重叠，即比值为1。假设候选框A和目标框B,则A和
B的IOU计算公式如(3-16)所示:

候选框与目标框的IOU值决定了检测结果的性质	真正例(TruePositives,
TP)、假正例(FalsePositives, FP)、真负例(TrueNegatives, TN)和假负例(False
Negatives, FN)。其具体定义如下：
1) 真正例(TP)：被正确地划分为正例的个数，即候选框和目标框的IOU大
于等于设定的阈值(VOC数据集上釆用的阈值是0.5,即如果IOU>0.5,贝U认为
27

它是TP,否则认为是FP。而COCO数据集的评估指标是对不同的IOU阈值分 别进行计算）。	心宀•.、
2）	假正例（FP）：被错误地划分为正例的个数，即候选框和俞框的IOU小 于设定的阈值。
3）	真负例（TN）：被正确地划分为负例的个数，目标检测中未使用到这个指
4）假负例（FN）：被错误地划分为负例的个数，即漏检的目标框。
有了 TP和FP,便能够计算检测的精确率（Precision）。精确率代表了在识
别岀来的正样本中，目标物体所占的比率。精确率的计算公式如下:

有了 TP和FN,便能够计算检测的召回率（Recall）。召回率代表了所有正
样本中，被正确识别为正样本的比例。召回率计算公式如下:

精确率和召回率互相影响，理想状态下追求两个都高，但是实际情况是两者 相互“制约”：追求精确率高，则召回率就低；追求召回率高，则通常会影响精确 率。为了权衡精确率和召回率，PR曲线是一个检测性能的好方式。PR曲线以 Recall为横坐标，以Precision为纵坐标。一个置信度下有一组P-R值，将置信
度从大到小降序取值，得到多组P-R值，即可描绘出PR曲线。

图3-12 PR曲线
AP是PR曲线下面的面积，通常来说一个越好的检测器，AP值越高。VOC 数据集采用11点插值的方法计算AP：对于某一特定类别，首先将检测器输出的 所有该类别的预测框，按置信度降序排序，然后设定不同的k值，选择top k个 预测框，计算 FP 和 TP,使得 Recall 分别等于 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,最后计算Precision并将得到的11个Precision取平均，即得 到AP。AP的计算公式如下：
28
AP=Tj^y	Precision^')	(3-19)
AP是针对单一类别的，mAP (mean Average Precision )是将所有类别的AP 求和，再取平均，对于有S个类别的数据集，mAP计算公式如下(单位为％)：
1 pS
mAP = - > APt	(3-20)
3.3.2 VOC数据集实验结果及分析
Pascal VOC是早期计算机视觉社区竞赛中最重要的竞赛之一。比赛使用的 数据集VOC 2007和VOC 2012是目前目标检测领域最为常用的公开学术数据 集，其官方网址为 http://host.robots.ox.ac.uk/pascalZV0C/。本节选取 VOC 2007 trainval集+VOC 2012trainval集(共20个类别)来训练模型，并在VOC 2007的 测试集上采用mAP来报告结果。实验的系统环境是ubuntu-16.04,使用的深度 学习框架是Keraso设置初始学习率为0.1,批次大小为1,训练轮数为12。训练 过程中采用学习率衰减：在第7个轮次、第11个轮次将学习率分别减小10倍。 训练过程中依据以上实验配置不断更新保存最优网络权重，测试时采用最优模型 进行验证。
表3-3本章模型与baseline性能床
i•比(单位为％)
类别
Baseline
FSFM
FSEM
FSFM 十 FSEM
Aero
76.5
72.7
70.1
73.1
Bike
79.0
92.4
90.2
93.0
Bird
70.9
91.8
8&1
91.6
Boat
65.5
71.8
67.9
71.5
Bottle
52.1
33.1
50.1
43.3
Bus
83.1
87.3
85.4
8&0
Car
84.7
74」
70.2
73.8
Cat
86.4
87.4
86.1
8&4
Chair
52.0
71.2
74.3
74.4
Cow
81.9
79.7
75.8
79.4
Table
65.7
91.4
8&7
91.7
Dog
84.8
90.0
88.7
91.0
Horse
84.6
92.8
91.4
93.8
Mbike
77.5
92.6
90.2
93.1
Person
76.7
74」
72.4
74.9
Plant
3&8
70,8
76.8
75.5
Sheep
73.6
59.4
71.5
67.1
Sofa
73.9
94.4
89.4
93.6
Train
83.0
89.3
75.0
83.8
Tv
72.6
60.3
67.7
65.7
MAP
73.17
7&83
78.50
80.36
29
如表3-3所示，表中统计了 Faster R-CNN和使用FSFM、FSEM以及 FSFM+FSEM的Faster R-CNN在测试集上全部类别的AP值以及整体的mAP（单 位均是％）。实验中Faster R-CNN在PASCAL VOC 2007+2012上的mAP基线是 73.17%o
从FSFM与baseline的对比上看，经FSFM后Faster R-CNN在测试集上的 mAP为78.83%,相比baseline上升了 5.66%。对于拥有20个类的中大型数据集， 5.66%提升算是有明显的提升。从具体类别AP值的变化来看自行车（bike）、摩 托车（mbike）、盆栽（plant）、椅子（chair）等的提升比较明显。二十个类别中， 绝大多数类别的尺寸有大有小，整体均匀分布，但盆栽和椅子是尺寸范围全都偏 小的两个类别，这两个类别AP值的提升，能够在一定程度上体现特征尺度融合 模块对于小目标的检测起到一定的积极作用。自行车、摩托车的尺寸分布均匀且 要么单独停放，要么被人骑行，出现的场景也较为简单，AP值的提升在情理之 中。但从表中还能发现同样是尺寸范围整体偏小的瓶子（bottle）、电视（tv）的 AP值下降比较明显，其中瓶子出现的场景大多在餐桌上，遮挡物较多导致其AP 值下降明显；而电视AP值下降主要是其与背景墙界限不明显所致。
再看FSEM与baseline的对比，可知经FSEM后Faster R-CNN在测试集上 的mAP为78.50%,相比baseline上升了 5.33%。提升效果十分明显。同样地， 对于自行车、摩托车等出现场景较为简单的类别AP值的提升十分显著，因为增 强了特征的表达，目标特征和简单背景特征得到增强，就像两颗距离较远的“黑 白棋子”被移的更远了，更易于区分；而像牛（cow）、羊（sheep）等类别，一般 成群出现，特征增强后，虽然把“黑棋子”都移走了，但太多的“白棋子”被放大并 且叠靠在一起，较难将它们一颗颗挑出，导致它们的AP值有所下降。
最后，再看FSFM+FSEM,可知经FSFM+FSEM后Faster R-CNN在测试集 上的mAP高达80.36%,比baseline提升了 7.19%,且相对于独立使用每种优化 方式都有近2%的提升。绝大多数类别的AP值都有极大提升，仅有个别类别的 AP值相较于baseline有略微下降，这个结果符合预期，再次证实了本章两种特 征优化思路的正确性。
从上文可以论断FSFM+FSEM在通用数据集Pascal VOC上有良好的表现， 接下来为了验证本章提出的两种特征优化思路的正确性，本文将VOC数据集按 物体-图像比，化分为小目标、中目标和大目标，并对其进行了验证。
物体-图像比是指图像中待检测目标物体面积占图像总面积的比值，如图3- 13中所示。
30

图3-13物体-图像比
从图3-13中可以看到，待检测目标小汽车的所占面积为S1,整张图像的面 积为S2, S1:S2的大小即定义为物体一图像比”，本文选择该参数作为区分物体 “大”、呻”、“小”物体的评估参数。大目标、中目标、小目标的划分依据如表3- 4所示：
表3-4 VOC物体-图像比
目标
比值范围
小
Sl/S2<0.2
中
0.2<Sl/S2<0.5
大
Sl/S2>0.5
表3-5展示了在VOC数据集上划分了大、中、小目标的检测结果，从表3- 5可知，本章所提出的两种特征优化思路并不会影响模型对于常规（大、中）目 标的检测精度，甚至会对其有一定程度的提升；特征尺度融合和增强的方法能够 在保证常规目标检测精度的前提下，有效优化Faster R-CNN对小目标的检测精 度。
表3-5本章模型与baseline性能末
1•比（单位为％）
类别
Baseline
FSFM
FSEM
FSFM+FSEM
大
8&4
89.60
89.56
89.54
中
83.0
85.32
84.64
85.62
小
55.17
60.83
61.70
66.09
VOC数据集上检测结果部分示例如下图所示:
31


图3-14 VOC数据集上部分检测示例
3.3.3 COCO数据集实验结果及分析
MS COCO数据集是目前目标检测领域最为常用的公开学术数据集，其官方 网址为http://cocodataset.orQ。与Pascal VOC数据集相比，COCO中的图片包含 了自然图片以及生活中常见的目标图片，背景比较复杂，目标数量比较多，目标 尺寸更小，因此COCO数据集上的任务更难。对于检测任务来说，现在衡量一 个模型好坏的标准更加倾向于使用COCO数据集上的检测结果。COCO的目标 检测任务共含有80个类别，在2014年发布的数据规模包括82783张训练图像、 40504张验证图像以及40775张测试图像。学术界较为通用的划分是使用80k的 训练图像和35k的验证图像作为训练集(trainval35k),使用COCO 2015版的测 试集作为最终测试集。在分布方面，COCO含有更多实例，分布也较为均衡，每 张图片包含更多类和更多的实例(每张图片平均分别含3.3个类和7.7个实例)。 COCO数据集还针对三种不同大小(small, medium, large)的图片提出了测量 标准，COCO中包含大约41%的小目标(area<32><32 ) , 34%的中等目标 (32><32<area<96x96), 24%的大目标(area>96><96)。图 3-15 展示了 COCO 数据 集中的部分图像。
32



图3-15 COCO数据集部分图像示例
本节使用COCO 2014 trainval35k训练模型，并在COCO 2015测试集上计算 AP,再综合平均来报告结果，还分别报告了大、中、小物体的检测结果。实验平 台和检测模型仍是上文所述，实验的系统环境是ubuntu-16.04,使用的深度学习 框架是Keraso设置初始学习率为0.1,训练的批大小为1, 一共训练40轮。训 练过程中采用学习率衰减：在第25个轮次、第35个轮次将学习率分别减小10 倍。使用上文在VOC数据集上训练基于FSFM+FSEM的FasterR-CNN最优权重 进行参数初始化。训练过程中依据以上实验配置不断更新保存最优网络权重，测 试时采用最优模型进行验证。
表3-6 COCO数据集上不同算法的性能对比（单位为％）
Method
Backbone
Time(ms)
Avg. Precision, IoU
Avg. Precision, Area



0.5:0.95
0.5
0.75
S
M
L
Faster R-CNN
VGG-16
147
24.2
45.3
23.5
7.7
26.4
37.1
MR-CNN
VGG-16
263
29.6
46.3
269
-
-
-
Faster R-CNN
ResNet-101
3360
24.9
45.7
23.4
15.6
3&7
50.9
R-FCN
ResNet-101
110
29.9
51.9
-
10.8
32.8
45.0
SSD»300
VGG-16

25.1
43.1
25.8
6.6
25.9
41.4
STDN-300
DenseNet-169
-
26.6
44.2
29.3
7.9
29.7
45.1
DSSD-300
ResNet-101
-
28.0
46.1
29.2
7.4
2&1
47.6
RefineDet-300
VGG-16
-
10.0
32.0
44.4
10.0
32.0
44.4
Ours-256
V+F+F
169
30.2
49.5
31.7
16.4
3&6
50.6
本章的算法在COCO数据集上进行了实验并和主流方法进行了对比，具体 如表3-6所示。表的第一列表示使用的算法，其中如SSD-300所示表示使用算法 SSD,图片输入尺寸大小为300x300,其它未做标注的即为原始论文中的设置。 第二列是该算法使用的骨干网络，表中“V+F+F”表示这里使用的骨干网络为嵌入
33 T FSFM的VGG-16,并后接了 FSEM。第三列表示单张图像的处理时间，单位 是毫秒，表中仅给出了两步检测算法的时间，其它单步检测算法时间消耗均低于 40毫秒，具体数字未给出。第四列是不同IOU阈值上的AP值，其中又细分了 三种情况：1） IOU从0.5到0.95,步长为0.05,然后取十个结果的平均得到的 AP值；2） IOU=0.5时得到的AP值；3） IOU=0.75得到的AP值。第五列是分 别对COCO数据集上已经划分好的小、中、大目标得到的AP值。
首先对比都采用了 Faster R-CNN算法但骨干网络不同的三项结果（表中第 一、三、九行）：基于ResNet-101骨干网络的Faster R-CNN比基于VGG-16的各 项AP值都有很大的提升，主要原因在于ResNet-101在网络深度上远大于VGG- 16 （101层和16层的差距），而且，ResNet-101引入了残差项，使得网络模型更 加复杂，特征处理能力更强。但由此引发的问题是时间的消耗，从表可知，处理 一张图片，基于ResNet-101的Faster R-CNN使用的时长是VGG-16的近23倍， 虽然检测精度极高，但对于实时目标检测问题就无能为力了。而本章所提出的算 法不仅在检测精度上高出基于ResNet-101的Faster R-CNN,但时间花销却与基 于VGG-16的Faster R-CNN基本持平。最重要的是，本章所提出的两种特征优 化思路不仅对大、中目标的检测有一定程度的提升（第一、三行对比），而且对 小目标检测的性能尤为突出。在小目标的检测性能上，它不仅远超基于VGG-16 的FasterR-CNN,而且比基于ResNet-101的FasterR-CNN高出了近1个百分点， 再次证明本章提出的特征尺度优化方案在小目标检测上效果显著。
其次对比本章算法与其它基于VGG-16的单步检测算法性能（表中第五、八、 九行）。从2.1节可知，单步检测算法在检测速度上比两步检测算法更具优势， 这里不再进行时间消耗上的对比。同样使用VGG-16作为骨干网络，本章提出的 算法在输入图像尺寸大小仅为256x256,但检测性能上却比SSD、RefineDet采 用300x300的输入，效果更明显。输入图像的大小很大程度上能够影响检测精 度，尤其是对于小目标的检测精度，输入图像越大，小目标检测精度越高，再次 证实了本章特征尺度优化思路的正确性。
最后纵观表3-6,得出以下结论：特征尺度融合模块和特征尺度增强模块的 引入对模型的检测精度有很大的提升。图3-16展示了 COCO数据集上的部分检 测示例结果。


图3-16 COCO数据集上部分检测示例

3.4本章小结
本章提出了一个全新的特征尺度融合模块(FSFM)和一个全新的特征尺度 增强模块(FSEM),并详细介绍了它们的工作原理。然后将FSFM嵌入Faster R- CNN的骨干网络VGG-16中，得到低层准确位置信息和高层语义信息相融合的 特征图，再用FSEM对该特征图进行增强，旨在增强特征图中对当前任务更有用 的特征的表达，抑制无用特征的表达。最后在Pascal VOC和MS COCO数据集 上进行对比试验：在VOC数据集上，以经典目标检测算法FasterR-CNN在该数 据集上的mAP作为baseline,分别分析了特征尺度融合预测、特征尺度增强预测 以及二者综合利用预测的结果；然后还在该数据集上划分了大、中、小目标，小 目标的mAP从55.17%±升至66.09%,精度提升了接近11个百分点。在COCO 数据集上，分析对比了本章提出的优化算法与当前主流算法的检测精度和检测速 度，实验结果进一步证实了这种特征尺度优化方式的可行性，为提高小目标检测 性能提供了一种思路。
35
第四章基于内容维度的小目标检测研究
在内容维度上，本章以小目标内容中的类别不平衡和空间不平衡为出发点, 在引入清洁度概念的基础上，采用一步检测算法中GHMLossPO］的思想改进 Faster R-CNN中RPN网络的损失函数，以梯度均衡机制解决anchor样本不均衡 的问题，以提高小目标检测的性能；然后提出了数据增强算法，分别从训练数据 集大小和数据集中对象独有特点两个方面解决小目标自身内容不足的问题，最终 达到提升小目标检测性能的效果。
4.1 RPN网络中的anchor解决方案
RPN网络是Faster R-CNN检测算法的核心内容，其目的是获取候选框。RPN 网络首先在特征图上滑动生成anchor,再对这些anchor进行分类和回归，然后 通过置信度评分对anchor进行排序和筛选,将初步筛选的anchor映射回原图后， 进行极大值抑制，最后得到提选的候选框。从整个过程中可以看出，anchor是 RPN网络的核心。而在anchor分类过程中，会引入一些新的问题，对后续的网 络训练造成影响。
4.1.1 anchor噪声及其分布不平衡
(1) anchor划分引入噪声
理想状态下,如果一个anchor和groundtrutli的IOU=1,那么这个anchor为 正样本；如果IOU=0是负样本。但是中间的那些应该怎么分配类别呢？从第三 章可以知道，现在常见的方法是卡阈值，IOU高于0.5即为正样本，低于0.5为 负样本，然而这并不是一种最优的策略，毕竟把一个IOU=0.55的anchor当做正 样本会引入一些噪声。图4-1展示了图像中的anchor及其分类标签示例。

(a)	(b)	(c)
Negative	.	. ■	Poshive

图4-1图像中的anchor及其分类标签
36
之所以说划分anchor正样本和anchor负样本的启发式方法会引入噪声，是 因为对于所有目标，无论其类别、形状、大小等，阈值都是手动选择和固定的。 例如，对于不规则形状的目标，具有高IOU的anchor可能包含背景噪声甚至包 含其他物体，如图4-1中的卡车上的绿色汽车和笔记本电脑前面的狗。另一方面, IOU较小的anchor可能仍然包含重要的线索。例如，图4-1中包含长颈鹿头的 anchor将被视为负样本，但它包含识别长颈鹿和定位长颈鹿的有用外观信息。这 种划分正负样本集合的方法，会导致用于训练分类分支的标签是带有噪声的，样 本噪声大，学习困难，将产生较大的损失。因此，当使用像2.2.2节中提到的OHEM 或Focal Loss这样的重采样方法来解决问题时，噪声也会被相应放大。
(2) anchor样本不平衡
在传统的anchor类别划分过程中，通常根据anchor与ground truth的IOU 范围划分其类别，本章使用的划分依据如图4-2所示：

图4-2 anchor样本划分依据示意图
图4-3展示了 anchor样本的4个具体类别，其中红框为ground truth□依据 上述划分依据可以推断出：按正负样本分类，负样本将远远多于正样本；按简单 困难样本分类，简单样本将远远多于困难样本。

图4-3 anchor样本分类

37
以图4-3输入网络训练为例，调整图像大小为256x256,经过了 VGG-16特 征提取网络后，特征图尺寸为16x16, anchor的尺寸共计9种，那么总共有 16x16x9=2304个anchoro根据置信度得分初筛得到2000个anchor,而该图中仅 有一个待检测目标，会有大量的anchor标注为负样本。极少数的正样本中，简单 样本所占比例更是微乎其微。
香港中文大学多媒体实验室李教授团队Do】认为正负样本数不同不是影响检 测算法性能的本质问题，产生本质影响的问题是简单困难样本的分布不均衡。更 进一步，每个样本对模型训练的实质作用是产生一个梯度用以更新模型的参数, 不同样本对参数更新会产生不同的贡献。简单样本的数量非常大，它们产生的累 计贡献就在模型更新中有巨大的影响力甚至占据主导作用，而由于它们本身已经 被模型很好的判别，所以这部分的参数更新并不会改善模型的判断能力，也就使 整个训练变得低效。
4.1.2清洁度及其在RPN上的应用
传统的交叉矯损失如公式（2-2）所示，进一步可以将其改写为公式（4-1）, 如下所示：
CE(p, p*) = -p* - log(p) - (1- p*) - log(l 一 p)	(4-1)
其中p*是anchor的标签，取值为1或者0,分别代表正负样本，p是预测其 输出为1的概率，取值范围为0〜1。
而根据上节提到的anchor类别的划分依据，可以知道：
=卩，0.5 <IOU<1
P lo, 0<IOU < 0.5	b 勺
在上节中分析，anchor在上述方法打标签的时候会引入噪声，为减少其在分 类和回归过程中的影响，本节创新性的提出清洁度这一概念并将其作为anchor分 类标签应用于RPN网络。清洁度指标将与每个anchor关联，以便在训练期间自 适应调整其重要性。所以定义清洁度十分重要，它必须满足以下几点要求：1） 能够自动确定，而不是人工经验的启发式设定；2）能够反映anchor成功回归到 所需位置并分类为目标（或背景）标签的概率。
首先，针对RPN网络中的分类分支，网络本身预测的置信度在一定程度上 可以体现出噪声水平，而且它能够满足连续性要求，所以直接将它作为清洁度指 标的一项。其次，基于分类和回归的不一致性卩®,直接将回归anchor和ground truth的IOU加入清洁度指标。这样清洁度的定义如下：
{
a - locr + （1 — a） ■ clsc for b G
n	f	c /	（4-3）
0	for b e Aneg	' '
其中Zoe”和c/Sc分别表示回归精度（回归之后anchor和ground truth的 IOU）和分类置信度（RPN网络打的分数），a控制相应的比例，b为每一个anchor,
38
Apos和Aneg分别表示每个按IOU排序后的前N个正样本和负样本。
这个清洁度c可以直接当做anchor中分类的标签，即代替公式(4-1)中的 一方面能够避免模型产生一些过拟合的预测，另一方面减少了分类和回归的 不一致。表4-1展示了在RPN网络中引用了清洁度的算法流程。
表4-1引用清洁度的算法流程	 输入=1、GT、B、cls_c、loc_r> ◎、N
I：输入图像；GTi输入图像I中的ground truth; Bz anchor集
cls_c：分类置信度；locjr^ anchor的回归精度；a：调节因子，取0.5
N：正样本集的大小，取100
输出*分类和回归的损失：厶心和厶佗Q
1： 4pQs =0、^neg = 0、S= 0
2: for gt E GT do
3:	indices = argsort{10U(B,gty)	-降序排列
4:	Apos- AposU{indices[0: N]: gt}
5: end for
6： Aneg = {(B - A“s)・ indices: 0}
7: for bi € Apos do
& c= a • locr + (1 — a)・ clsc
9:	S= SU{bf:c}
10: end for
11: for b[ G Aneg do S= SU{bj： 0} end for
12： L 血=2汕£(內5)
13: Lreg = ^smooth_Ll
上述算法中的6-11行即为引入清洁度的过程，从中可以看出，利用分类和 回归分支的输出为每个anchor引入了精心设计的清洁度评分，不仅可以作为衡 量anchor回归成功与否和分类概率的指标，而且还可以在训练期间动态调整。
4.1.3 GHM Loss及其在RPN上的应用
在2.2.2节中提到的Focal Loss通过大大降低简单样本的分类损失来平衡正 负样本，但是其设计的Loss引入了两个需要通过大量实验来调整的超参数a和 Y，不能直接得到最优方案。
本节基于4.1.1的分析，模型训练的过程是一个在梯度优化的过程，如果梯 度优化的过程被简单样本过度影响，会极大的降低模型的整体性能。这些简单样 本的单个loss虽然不高，但由于数量众多，最终合起来会对总loss有很大的贡 献，从而导致梯度优化的时候过度关注这些简单样本，这样会使模型收敛到一个 不够好的结果。李教授团队卩0]在针对一步检测算法中简单、困难样本分布极度不 平衡问题，基于梯度优化提出了 GHM Losso接下来具体介绍GHMLoss。
对于一个样本，如果它能很容易地被正确分类，那么这个样本对模型来说就 是一个简单样本，模型很难从这个样本中得到更多的信息，从梯度的角度来说,
39

这个样本产生的梯度幅值相对较小。而对于一个错分的样本来说，它产生的梯度 信息则会更丰富，更能指导模型优化的方向。
如2.2.2节中提到，传统的分类损失通常采用cross-entropy (CE),即
-log(p) p* = 1
-log(l -p) p* = 0
其中P为模型预测的分类概率，pe[0,l]； p*为实际标签，p*e{0,l}o如果
用"表示模型的输出，则有：

图4-4样本梯度模长的分布统计图
于是某个样本的g值大小就可以表现这个样本是简单样本还是困难样本。从 一个收敛的检测模型中统计样本梯度的分布情况如图4-4所示。从图中可以看岀， 简单样本的数量要远远大于困难样本。也可以发现在g接近1的时候，样本比例 也相对较大，这里认为这是一些离群样本(outlier),可能是由于数据标注本身不 够准确或是样本比较特殊极难学习而造成的。对一个己收敛的模型来说，强行学 好这些离群样本可能会导致模型参数的较大偏差，反而会影响大多数已经可以较
40
好识别的样本的判断准确率。
基于以上现象与分析，李团队提出了梯度均衡机制，即根据样本梯度模长分 布的比例，进行一个相应的标准化，使得各种类型的样本对模型参数更新有更均 衡的贡献，进而让模型训练更加高效可靠。由于梯度均衡本质上是对不同样本产 生的梯度进行一个加权，进而改变它们的贡献量，而这个权重加在损失函数上也 可以达到同样的效果，此研究中，梯度均衡机制便是通过重构损失函数来实现的。 为了清楚地描述GHM损失函数，需要先定义梯度密度(gradient density)这一概 念。仿照物理上对于密度的定义(单位体积内的质量)，李团队把梯度密度定义 为单位取值区域内分布的样本数量。具体来说，将梯度模长的取值范围划分为若 干个单位区域(unitregion)。对于一个样本，若它的梯度模长为g,它的密度就 定义为处于它所在的单位区域内的样本数量除以这个单位区域的长度：
g)	，- c、
(g) = —Ug)—	()
其中N为每个batch中的样本总量，gk表示第k个样本的梯度模长，而且 有：
5£(gfc,g) = I1 g-|-gfc-g + f	(4-10)
0 otherwise
le(g) = min (g + |,l) -max(g-|,0)	(4-11)
所以梯度密度函数GD(g)就表示梯度落在区域［g - £/2, g + 8/2］的样本数量o 最后定义梯度密度协调参数(gradient density harmonizing parameter) p：
N
pi=歸	⑷⑵
其中N为样本的总数目。最后把GHM的思想应用于分类损失上，得到
GHM Loss：
LGHM = Pi^CE(Pi>Pi)= 丫舊'吧"	(4-⑶
一般模型训练初期会倾向于让模型对于各类的预测是相同概率的，但是对于 样本极度不平衡问题，这其实是非常不利的，因为负样本的损失会很大。所以在 将GHMLoss应用到RPN网络中时，本节还分析了最优初始化设置。
假定数据集中共有C个类别，样本量为M，其中正样本为弘。模型刚开始 对所有样本的预测概率值都是p，,根据厶防损失，那么对于所有正样本的损失和 为-Np ■ log (pz)>所有负样本的损失和为—QNSC — - log (1 — pA)o最后的厶CE 损失为正负损失和与心的比值，整理可得：


最优的初始化设置应该是要使整体损失最低，可以对公式（4-14）求导数来
确定p':

很容易计算出最优p'=Np/NsC,在模型训练之初，CT（wx + b） a CT（b） = p'=
Np/MC。那么可以计算出bias的最优初始化值为:


最后即可将GHMLoss应用于进行了最优初始化设置的RPN网络。综合清
洁度概念，优化后的RPN网络算法流程可展示为表4-2:
表4-2 RPN综合优化算法流程
输入：	GT、B、cZsq locjry a> N
It输入图像；GT:输入图像I中的ground truth; B： anchor集
clsjCz分类置信度；loc_Tz anchor的回归精度；a：调节因子，取0.5
N：正样本集的大小，取100
输出：分类和回归的损失：厶心和厶创
1： Apos = 0、Aneg = 0、S= 0、最优初始化
2: for gt E GT do
3:	indices = arg sort (I OU (B, gt))	■降序排列
4： Apos = Ap0SU{indices[Q:N]:gt}
5: end for
6: Aneg = {(B - Apos). indices: 0}
7: for b( G Apos do
8:	c= a • locr + (1 — a) ■ clsc
9:	S=SU{®:e}
10: end for
11: for bi G ^neg do S= SU{d： 0} end for
12: Lcls = LGHM = Pi^cE(Pi>€d =	'監:；?
13: Lreg =	smooth Ll
改进后的Faster R-CNN端到端结构如图4-5所示，后文通过实验证实了综 合优化的RPN训练有效性。
42

图4-5改进后的Faster R-CNN端到端结构图

4.2数据增强策略
通过合成或者转换的方式，从有限的数据中生成新的数据，数据增强技术一 直以来都是一种重要的克服数据不足、内容不足的手段。在图像领域，常见的数 据增强包括重采样、平移、镜像和对比度增强等。这些策略源于图像分类任务, 现阶段在目标检测任务中也会适当使用。其实，当数据集过小时，不管是分类还 是检测任务，模型极容易出现过拟合，采取数据扩充是必要手段。在中、大目标 的检测上，有无数据增强策略没有太大影响，但在小目标的检测上，数据增强能 够弥补小目标在内容上的严重不足。本节提出基于旋转的数据集增强策略和基于 掩膜的小目标实例增强策略，并将其应用到汽车轮毂X光数据集中，为汽车轮 毂X光图像的缺陷位置检测提供优质的数据基础。
4.2.1小目标数据集增强策略
2019年，来自谷歌的Brain团队针对目标检测任务上的数据增强做了大 量的研究和工作。研究者定义了 22个数据增强运算，包括对颜色、几何变换进 行增强，将检测任务上的数据增强策略搜索视为一个离散优化问题。研究者规定 了最终的数据增强策略由K个子策略组成，每个子策略包含N个数据增强运算。 每个运算又包含两个超参数：被用到的概率P和强度Mo P和M被离散化为6 份。训练的时候，每次抽一个子策略，然后把这个子策略包含的数据增强运算应 用到图像上。这样整个搜索空间的大小为：(22XMXP)KXN = (22x6x6)5x2 « 9.6 x 1028,然后用AutoML【38】的策略进行最优数据增强方法搜索，研究者发现 目标旋转是出现次数最多的增强策略，并在多个数据集上验证了目标旋转策略能 够极大的提升目标检测的性能；此外，研究者还得出结论，训练数据集很小的时 候，数据增强方法带来的涨点最多，尤其在小目标上涨的特别多。
Brain所使用的旋转仅对目标实例而言，未改变数据集中图像的数量。本节 基于Brain团队的发现，为对小目标数据集进行扩充并改善角度不变性的限制而
43

导致小目标定位和识别性能差的问题，提出一项基于旋转的数据集增强策略。
基于旋转的数据集增强策略具体步骤如下：在训练过程中，将训练图像每隔 N度旋转一次，一共旋转M次，因此旋转后的数据集扩充至原数据集大小的(M+1) 倍，数据集得到扩充,并且训练的过程中需要各样本有一定区别。图像进行旋转， 图中的目标框也会随之旋转，但旋转后目标框的坐标不是水平矩形，使得新的注 释文件不能在CNN中使用。所以必须对旋转后图像的实例重新计算目标框坐标， 其计算过程如下:
首先计算目标框旋转后的坐标，如公式(4-18)所示，其中冬,％是图像(wxh) 的中心坐标：xc = w/2, yc = h/2, x, y是旋转前目标框的坐标，x', :/是旋转 后目标框的坐标。0是旋转的角度：
(x' = (x — xc) cos 0 — (y — yc) sin 0 + xc
\y' = (x — xc) sin 0 + (y — yc) sin 0 4- xc
则新目标框的左上角为公式(4-18)：
(xti =	瑶，竝)
[yt,z =
新目标框的右下角为公式(4-19)：
xdr = max(%i，X2，X3，x4)
其中也2为新目标框左上角的坐标，为新目标框左下角的坐标，*d,r为新 目标框右上角的坐标，『为新目标框右下角的坐标，址，必为旋转后目标框的 四个坐标。
4.2.2小目标实例增强策略
数据集得到扩充后，检测过程中需要的小目标实例随之扩充，但相应的非目 标背景内容也会随之扩充。小目标实例在整个数据集中所占的比例仍保持不变。 本节提出的小目标实例增强策略不仅能够扩充小目标实例的数量而且能够增加 小目标实例在整个训练样本中所占比例。本节提出的小目标实例增强策略是基于 目标掩膜(Mask)信息提出来的。具体来说，复制小目标实例的掩膜，然后将该 掩膜粘贴到同一图像中的其他位置。
小目标能够匹配到的anchor极少，通过复制小目标的掩膜并粘贴到图像中 的其它位置，从而在训练过程中有更多的anchor与小目标正匹配，这样能够改 进小目标在RPN训练期间损失函数中的贡献。图4-6展示了所提出的小目标实 例增强策略是如何在训练中增加匹配的anchor数量。
44


图4-6不同尺寸的anchor与ground truth匹配示意图
图4-6 （a）是一个anchor示意图；（b）是一^小目标所对应的anchor, —共 只有三个anchor能够与该小目标配对，且配对的IOU也不高；（c）是将小目标 的掩膜粘贴到同图像的其它位置后anchor的匹配情况，同一张图像中的小目标 实例变多，匹配到的anchor数量也就变多，则此小目标被检出的概率也就变大。 对于COCO数据集来说，图像的标注信息里有目标的掩膜信息，所以在COCO 数据集上进行小目标实例增强时只需保证两点要求：1）在复制时，只考虑未被 遮挡的小目标，因为使用有遮挡区域的不连续目标实例会使图像失真；2）在粘 贴时，要确保小目标掩膜不与任何现有的目标重叠且不超出图像边界。具体算法 流程如图4-7所示：


COCO数据集上基于掩膜的小目标实例增强效果如图4-8所示，左图为原始 图像，右图为复制网球掩膜并随机将其粘贴到图像合适位置：

4.2.3汽车轮毂X光数据集
在汽车工业中，保证各金属部件的完全可靠性至关重要。近年来，X射线检 测虽然在缺陷检测领域得到了广泛的应用，但仍需大量极具经验的人去判别缺陷 位置及缺陷级别。利用计算机视觉和深度学习技术，结合X光图像，识别微小缺 陷势在必行。
为了建立符合实际生产的标准数据集，本节使用从Dicastal公司的数据库中 导出的汽车轮毂X光图像。轮毂X光原始图像由工业专家使用注释工具进行标 签、边框以及掩膜信息注释，以保证注释的专业性。注释文件以JSON文件保存 为MSCOC0数据集样式。X光图像的原始大小为1024x1024,工业专家按实际 生产需求，将所有缺陷按标准分为一级、二级和三级缺陷。如图4-9所示，其中 所示的两个代表性缺陷分别位于轮辐和轮毂上，都是在铸造阶段产生的。通过图
像实例可以直观看出缺陷的特征：1）缺陷相较于整个X光图像十分微小；2）部

本节采取中国科学院大学于教授团队a］对目标实例的大小定义一绝对大 小(absolute size, AS )和相对大小(relative size , RS ) □假设使用 = 来描述数据集中的第i个图像中的第丿•个目标的边界框，其中 (切，沟)表示左顶点的坐标，Wij,如是边界框的宽度和高度。Wi，厲分别表示 第i张图像的宽度和高度。故目标实例的绝对大小和相对大小计算如公式(4-20) 和公式(4-21)所示：

本节统计了汽车轮毂X光数据集的绝对大小、相对大小和长宽比的平均值 和标准差，并与COCO数据集做了对比，统计结果如表4-3所示。
表4-3汽车轮毂X光数据集与MS COOC数据集对比表
数据集
绝对大小
相对大小
长宽比
X光

从图4-8和表4-3可以得出结论：相较于以小目标著称的COCO数据集而 言，汽车轮毂X光数据集中的目标“更小”，所以，整个汽车轮毂X光图像集可 视为一个小目标检测数据集，并且所有缺陷目标实例覆盖的面积相较于原图很小, 这意味着小目标的检测难度极大。汽车轮毂X光图像原始数据分布如表4-4所 示：
表4-4汽车轮毂X光图像原始数据分布

图像(张)
实例(个)
一级
二级
三级
训练集
从表4-4可以推断出，数据集本身数量很小，缺陷目标实例太少，可能会导 致训练不出合适的模型；二、三级缺陷相较于一级缺陷数量更是稀少，但其比例 符合实际生产情况。
本节将基于旋转的数据集增强策略和基于掩膜的小目标实例增强策略应用 在汽车轮毂X光数据集上，以达到数据集扩充和缺陷实例增强的效果。
47

(a)	(b)	(c)

图4-10汽车轮毂X光图像新标注坐标的计算过程
在权衡了检测器的速度和精度后，本节在汽车轮毂X光数据集进行扩充时， 将N取为15, M取为5,即每张图像每隔15度旋转一次，共旋转6次。图4-10 展示了汽车轮毂X光图像新标注坐标的计算过程：图4-10 (a)显示出了旋转后 的标注框(红色框)不再是水平矩形，导致注释文件不能在CNN中使用。本节 使用原标注框的外接矩形框作为旋转后的新标注框(绿色框)，如图4-10 (b)所 示，此时，新标注框的注释文件能够为CNN所用。图4-10(C)即为最终的标注 框。
汽车轮毂X光数据集得到扩充后，再对具体的缺陷实例进行增强。上一节 的图4-7已具体展示了增强流程。在确定可粘贴区域时，这里需要注意将掩膜粘 贴到轮毂区域，而不仅限于整个图像区域。由于X光图像属灰度图，空白区域是 完全白色的，像素值为255,因此可以通过简单的图像二值化方法直接划分出轮 毂区域和非轮毂区域。然后随机选取一个坐标点作为缺陷掩膜的候选中心点，计 算出所选掩膜对其它缺陷实例的IOU值。如果IOU值大于阈值，就重新选择一 个候选中心点，直到候选区域满足以上两点要求。最后，重复多次粘贴过程，以 生成多个缺陷实例。
表4-5每张图像的缺陷实例增强算法
输入：带有缺陷集A的原始图像X,粘贴重复次数为N
输出：带有新缺陷集A'的增强图像X'
1:确定灰色轮毂区域：H’ = XV 255并初始化4'为一个空集
2: for k = 1 to N do
3:	随机选择一个中心点(x,y)和一个缺陷实例e A
4:	根据中心点仗,刃和缺陷实例Ai(xli,ylilx2i,y2i')iy算掩膜区域范围(x1,y1,x2,y2)
5： If Mean(Hx[x1:x2,y1:y2]) > 0.9 and Max(10U([xx: x2,y2], Anew)) < 0.1
6:	添加(衍,x2f y2)到新缺陷集灯
7: end for

实现效果如图4-11所示，黑色框为原始缺陷实例，其他红色框是通过小目 标实例增强策略生成的。

图4-11缺陷实例增强示例

4.3实验结果与分析
本章主要针对RPN中anchor划分引入噪声、不平衡和汽车轮毂X光图像集 样本过少提出了相应的优化方案，将Faster R-CNN的预测结果作为baseline,优 化RPN后的Faster R-CNN.数据增强方案以及两种优化同时使用，对比进行实 验，并对结果进行了分析，验证了本章提出的方法在给定的数据集上带来检测精 度的大幅提升；然后根据本章汽车轮毂X光图像集，使用第三章所提的特征尺 度优化方案和本章所提的内容维度优化方案进行综合实验，并分析了结果。
4.3.1内容维度的实验设置与结果分析
本节实验采用的数据即为421节介绍的汽车轮毂X光图像集，原始数据分 布如表4-4所示。使用本章所提出的数据增强后的分布如表4-6所示，相较于表 4-4,这里仅对训练集进行数据增强，图像张数变为原来的6倍，实例个数变为 原来的36倍，为保证缺陷等级分布符合实际生产，不对二、三级缺陷进行过采 样操作；验证集和测试集保持不变。
表4-6汽车X光图
像增强后数据分布

图像（张）
实例（个）
—级
二级
三级
训练集
5058
36756
29880
4788
2088
验证集
85
108
89
10
9
测试集
125
159
132
16
11
总数
5268
37023
30101
4814
2108
实验的系统环境是ubuntu-16.04,使用的深度学习框架是Keraso设置初始学

49
习率为0.1,批次大小为1,训练轮数为80。训练过程中采用学习率衰减：在第 48个轮次、第70个轮次将学习率分别减小10倍。实验一共进行六组进行，表 4-7至表表4-12详细展示了相应结果，其中一组是baseline （原始FasterR-CNN+ 原始数据集），还有一组实验为探究清洁度中a的最佳取值，其他四组分别是相 应内容优化之后的对照实验。
表4-7数据增强与baseline性能对比（单4
位为％）
方法
mAP
一级
二级
三级
Faster R-CNN
43.3
39.4
42.3
4&2
Faster R-CNN +增强数据
53.5
49.3
51.2
60.1
方法如表4-7所示，表中统计了基于原始数据的Faster R-CNN和使用数据 增强后的Faster R-CNN在测试集上三个级别缺陷的AP值以及整体的mAP （单 位均是％）。实验中Faster R-CNN在原始汽车轮毂X光图像集上的mAP基线是 43.3%。从表中可知经数据增强后Faster R-CNN在测试集上的mAP为53.5%, 相比baseline上升了 10.2%。由此看来，数据增强对于小目标的检测确实有较为 明显的提升，与预期一致。
表4-8清洁度中a不同取值对应结果（单位为％）
a
mAP
0.3
42.3
0.4
44.1
0.5
44.5
0.6
44.3
0.7
43.2

表4-9清洁度的引入与baseline性能对比（单位为％）
方法
mAP
一级
二级
三级
Faster R-CNN
43.3
39.4
42.3
48.2
Faster R-CNN+清洁度
44.5
40.3
43.4
49.7
在探究清洁度对检测结果的影响实验中，先探究了公式4-3中a的最优值, 如表4-8所示，实验结果显示，当a-0.5时，性能最佳，这与分类分支与回归分 支是并行计算的事实相一致。因此，后续的实验中将a设置为0.5。从表4-9中 可以看出，经过清洁度优化anchor类别划分后，检测精度有所提高。
表4-10 GHM Loss与baseline性能对比（单位为％）
方法
mAP
一级
二级
三级
Faster R-CNN
43.3
39.4
42.3
4&2
Faster R-CNN +GHM Loss
44.7
41.9
43.2
49.0
50
如表4-10,从测试结果看来，GHMLoss通过梯度均衡机制，减小了样本不 平衡的负面效应；观察中可以发现，对于一级缺陷的提升效果更为明显，因为缺 陷更小的图像中，划分anchor时的不平衡将更为严重，再次证明了 GHMLoss在 解决样本不平衡上的有效性。
表4-11 RPN综合优化与baseline性能对比（单位为％）
方法
mAP
一级
二级
三级
Faster R-CNN
43.3
39.4
42.3
4&2
Faster R-CNN + 清洁度+GHM Loss
45.1
42.2
43.6
49.4
从表4-11可以得知，在消除了 RPN网络中anchor的噪声、不平衡影响后, 在测试集上取得了 45.1%的显著效果，相对于独立使用清洁度、GHM Loss有一 定性能提升。再次证明了本章针对原始RPN训练过程中训练anchor样本这个步 骤中的问题所提优化方案的有效性。
表4-12数据增强+综合优化RPN与baseline性能*
1■比（单位为％）
方法
mAP
一级
二级
三级
Faster R-CNN
43.3
39.4
42.3
4&2
Faster R-CNN+优化RPN+增强数据
58・4
52.4
55.6
67.3
如表4-12所示，论文提出的结合数据增强和RPN综合优化对目标内容综合 优化方法在测试集上取得了最高的mAP, 58.4%O再次证明本章提出的针对小目 标检测在内容维度的优化方案都是行之有效的。
4.3.2综合实验结果分析
最后本文根据汽车轮毂X光图像集，使用第三章所提的特征尺度优化方案 和本章所提的内容维度优化方案进行综合实验，并分析了结果。
表4-13综合优化实验结果（单位为％）
缺陷等级
原数据
Faster R-CNN
原数据
特征尺度优化
增强数据
特征尺度优化
增强数据
内容维度优化
增强数据
综合优化
一级
39.4
3&7
49.5
52.4
54.1
二级
42.3
42.5
55.7
55.6
5&2
三级
4&2
49.0
68.7
67.3
70」
MAP
43.3
43.4
57.9
58.4
60.8
对比表4-13的第二、三列可知，在上一章提出的特征尺度优化方法在训练 数据量不够大的情况下，对检测性能的提升基本没有,但是当训练样本得到增强， 即表中第四列所示，从模型结构上解决了使用和制造对小目标更有表达力的特征, 检测结果大为提高。再对比第二、五列，在小目标缺乏训练样本的情况下，对小 目标在内容维度上进行优化，其提升巨大。最后综合特征尺度和内容维度的优化
51

方案，得到高达60.8%的结果。图4-9展示了汽车轮毂X数据集上的部分检测示 例。

4.4本章小结
本章根据检测任务中目标内容的不足，分别从数据自身的不足和训练过程中 anchor内容的不足提出优化方案。首先针对训练过程中，RPN网络中anchor样 本分布的不平衡，应用GHMLoss的梯度均衡机制，解决其样本不平衡问题；然 后在训练数据集上，提出基于旋转的汽车轮毂X光数据集的增强策略，以丰富 训练数据集；其次在目标实例上，基于小目标的掩膜信息，将该掩膜粘贴到同一 图像中的其他位置，以丰富目标实例；最后，在汽车轮毂X光数据集上进行对比 试验：在内容维度实验上，以经典目标检测算法Faster R-CNN在该数据集上的 mAP作为baseline,分别分析了数据增强、RPN分类Loss优化以及二者综合利 用预测的结果，检测性能得到验证；在综合优化实验上，分析对比了第三章提出 的特征尺度优化方案和本章提出的内容维度优化方案检测精度，实验结果进一步 证实了本文所提优化方式的可行性，为提高小目标检测性能提供了两种新思路。
52
第五章总结与展望
5.1工作总结
小目标检测的研究具有很强的理论价值，诸如图像语义理解、目标识别等的 高级计算机视觉任务的效果往往取决于目标检测算法的具体性能，而小目标检测 的效果在很大程度上决定了通用目标检测算法的性能。小目标检测的研究在通信、 交通、安全、救援等方面具有重要的应用价值。日常普通图像、由X光探测的工 业和医疗影像、通信基站摄像机等安装高度高的图像捕获设备采集到的图像和军 用无人机摄像头拍摄的图像中都存在着许多小目标。本文以基于深度学习的小目 标检测技术作为研究方向，在特征维度上，提出了特征尺度融合模块和特征尺度 增强模块；在内容维度上，提出了清洁度的概念、GHMLoss的迁移应用以及特 定数据集上的数据增强方案。本文的主要研究内容及成果如下：
（1）	论文对小目标检测的内容进行调研分析。小目标检测近年来获得了长 足的发展，各种基于深度学习的网络模型和优化方案层出不穷。本文详细调研了 小目标检测领域的相关技术及研究，主要包括两步检测算法和一步检测算法演进 过程及尺度维度和内容维度上的主要发展方向与主要成果等。对以上内容的学习 为后续工作提供了重要的理论支撑。
（2）	因为小目标包含的特征信息少，经卷积神经网络提取后特征图所包含 的特征弱，所以增强小目标的特征表达是关键，以此来缩小与大目标的特征表示 差异。论文在Faster R-CNN的基础上，创新性的提出了特征尺度融合和特征尺 度增强的优化网络。其中，特征尺度融合模块通过通道数叠加的方式实现了不同 尺度特征层的融合，并且，采用的超像素卷积并不会引入计算量。基于多尺度感 受野和通道注意力的特征尺度增强模块确保了特征图的稳健。优化网络顶部的特 征图既有底层的细节信息，又有高层的语义信息，有效的解决了小目标经过卷积 网络后特征表达不足的问题。
（3）	因为小目标本身数量稀缺，自身内容的不足以及在卷积神经网络训练 中的不均衡都会导致小目标的检测性能过低。论文在Faster R-CNN的基础上， 首先，引入清洁度的概念并采用GHMLoss的思想改进损失函数，构成梯度均衡 机制，不仅可以缩小在RPN中正负样本不均衡的问题，而且还能大大缩小简单 样本与困难样本在数量上的巨大差距。然后提出了小目标数据增强算法，基于旋 转的数据集增强策略使数据集得到了扩充；基于掩膜的小目标实例增强策略将小 目标的掩膜粘贴到同一图像中的其他位置，使得训练样本有一定区别且对检测效
53
果友好。最终达到提高小目标检测性能的效果。
5.2工作展望
本课题对小目标检测进行了细致的研究，但一部分工作仍不够详细，模型的 实时检测距离实际应用仍存在差距。未来的研究工作包括但不限于以下几个方面:
1）	本文在解决因模型中anchor造成的问题时，采用的是清洁度和GHMLoss 的解决方式。未来可以研究anchor&ee的方案，摒弃anchor,从源头上解决该类 问题。
2）	本文基于两步目标检测算法Faster R-CNN对小目标检测的精度进行优 化，但其检测速度仍达不到实时检测的应用级别，未来可以研究压缩模型、训练 提速等方式提告其检测速度。
3）	本文尽管使用数据增强丰富了小目标检测的训练样本，但其增强算法是 于特定数据集而言的，未来可以研究一种通用的数据增强方式，来迁移于不同数 据集上。
参考文献
[1] Hinton G E，Salakhutdinov R R. Reducing the dimensionality of data with neural networks [J].Science, 2006, 313(5786): 504-507.
[2] Krizhevsky, A., Sutskever, I. and Hinton, G.E. ImageNet Classification with Deep Convolutional Neural Networks [C]. Advances in Neural Information Processing Systems, 2012: 1097-1105.
[3] ShanshanZhang, Rodrigo Benenson, Bemt Schiele. CityPerson: A Diverse Dataset for Pedestrian Detection [C]. IEEE Conference on Computer Vision & Pattern Recognition. 2017.
[4] Lin T Y, Maire M, Belongie S, et al. Microsoft COCO: Common Objects in Context[J] ,2014.
⑸ 王海屹.基于特征融合的小物体目标检测研究与应用[D].北京邮电大学，
2018.
[6] Ren S, He K, Girshick R, et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [J]・ IEEE Transactions on Pattern Analysis & Machine Intelligence, 2016: 1-1.
[7] Neubauer C. Evaluation of convolutional neural networks for visual recognition [J]. IEEE Trans on Neural Network, 1998, 9(4)： 685-696・
[8] Dai J, Li 乂 He K, et al. R-FCN: Object Detection via Region-based Fully Convolutional Networks [J]. 2016.
[9] Redmon J, Diwala S, Girshick R, et al. You Only Look Once: Unified, Real-Time Object Detection 卩]* Computer Science, 2016.
[10] Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot MultiBox Detector [J]. Computer Science, 2016.
[11] Girshick R, Donahue J, Darrell T, et al. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation [J]. Computer Science, 2014: 580587.
[12] He K, Zhang X, Ren S, et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition [J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2014,37(9): 1904-1916.
[13] Girshick R. Fast R-CNN [J]. Computer Science, 2015.
[14] Redmon J, Farhadi A. YOLO9000: Better, Faster, Stronger [C]. IEEE Conference
55
on Computer Vision & Pattern Recognitioil. 2017.
[1 习 Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition [J]. Computer Science, 2014.
[16] Redmon J, Farhadi A. YOLOv3: An Incremental Improvement [J]. 2018.
[17] M. Everingham, L. Van Gool, C. K・ I. Williams, et al. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results ,2007.
[18] Lin T Y, Dollar Piotr, Girshick R, et al. Feature Pyramid Networks for Object Detection [J]・ 2016.
[19] Fu C Y, Liu W Ranga A, et al. DSSD: Deconvolutional Single Shot Detector [J]. arXiv preprint arXiv: 1701.06659, 2017.
[20] He K，Zhang X, Ren S? et al. Deep residual learning for image recognition [C]. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.
[21] Ren J, ChenX, Liu J, et al. Accurate single stage detector using recurrent rolling convolution [J]. IEEE Conference on Computer Vision and Pattern Recognition, 2017, (5):752-760.
[22] B・ Singh and L・ S. Davis. An analysis of scale invariance in object detection-snip [J]. IEEE Conference on Computer Vision and Pattern Recognition, 2018・
[23] B. Singh and L. S. Davis. SNIPER: Efficient Multi-Scale Training [J]・ arXiv: 1805.09300v3, 2018.
[24] Yanghao L, Yuntao C, Naiyan W, et al. Scale-Aware Trident Networks for Object Detection [J]. IEEE Conference on Computer Vision and Pattern Recognition, 2019.
[25] Mate Kisantal, Zbigniew Wojna, Jakub Murawski, et al. Augmentation for small object detection [J]. IEEE Conference on Computer Vision and Pattern Recognition,
2019.
[26] Jiangmiao P, Kai C, Jianping S5 et al. Libra R-CNN: Towards Balanced Learning for Object Detection [J]・ IEEE Conference on Computer Vision and Pattern Recognition, 2019.
[27] Takeki A, Trinh T? Ybshihashi R, et al. Combining deep features for object detection at various scales: finding small birds in landscape images [J]. IPS J transactions on computer vision and applications, 2016, 8(1):5.
[28] Ren Y, Zhu C, Xiao S・ Small object detection in optical remote sensing images via modified faster R-CNN [J]. Applied Sciences, 2018,(2):134-136.
[29] Yi K, Jian Z, Chen S5 et al. Knowledge-based recurrent attentive neural network
56
for small object detection [J]. arXiv Preprint, 2019, (4):15-18.
[30] Buyu Li, Vu Liu, Xiaogang Mug. Gradient Harmonized Single-stage Detector [J]. IEEE Conference on Computer Vision and Pattern Recognition, 2019.
[31] Neubeck A, Gool L V. Efficient Non-Maximum Suppression[C]// International Conference on Pattern Recognition. 2006.
卩 2] Lin T Y, Goyal P, Girshick R, et al. Focal Loss for Dense Object Detection [J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017, PP (99):2999- 3007.
卩3] Tong Yang, Xiangyu Zhang, Zeming Li, et al. Metaanchor: Learning to detet objects with customized anchors・Advances in Neural Information Processing Systems. 2018. 2
[34] Shrivastava A, Gupta A, Girshick R. Training Region-based Object Detectors with Online Hard Example Mining [J]. 2016
[35] Xuehui Yu, Yuqi Gong, Nan Jiang, et al. Scale Match for Tiny Person Detection [C].// IEEE Conference on Workshop on Applications of Computer Vision 2020.
[36] Borui Jiang, Ruixuan Luo, Jiayuan Mao, et at Acquisition of Localization Confidence for Accurate Object Detection [C]. // IEEE Conference on European Conference on Computer Vision 2018.
[37] Banet Zoph, Ekin D. Cubuk, Golnaz Ghiasi, et al. Learning Data Augmentation Strategies for Ol)ject Detection [J]. arXiv: 1906.11172vl ,2019.
[38] Xin He, Kaiyong Zhao, Xiaowen Chu. AutoML: A Survey of the State-of^the-Art [J]. arXiv:1908.00709v3,2019.
