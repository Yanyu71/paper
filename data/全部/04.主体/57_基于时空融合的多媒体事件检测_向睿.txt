第一章绪论
1.1	研究背景及意义
随着计算机技术、网络技术和多媒体技术的发展和普及，视频越来越成为一个重 要的多媒体信息载体。视频由于其内容的丰富性、直观性和生动性，一直被广泛应用 于我们生活的各方各面。但是随着“互联网+”时代的到来，视频数据规模急剧扩大， 人工地去分析和管理海量的视频数据将耗费巨大的人力，如何高效地检索海量的视频 中的感兴趣事件，即多媒体事件检测（Multimedia event detection, MED）己成为一个 迫切需要解决的问题，并成为最近几年来计算机视觉领域的一个热点研究内容。
从上个世纪90年代开始，随着数据库和搜索引擎技术的发展，基于文本的视频 分析方法被广泛地应用于视频检索当中，但这种检索方法过于依赖人工标注，难以充 分揭示和描述视频的特点，且带有很大的主观性，因此基于文本的检索方法存在明显 的局限性。
随着近十几年来信息学和概率与统计理论的不断深入，基于内容的视频检索和事 件检测方法逐步兴起。基于内容的检索方法提出了一种“特征”的概念，用于连接低层 视觉信息与高层抽象概念之间的“语义鸿沟”口][2]。我们可以根据特征的距离来表示 视觉上的相似度，以此达到检索和分类的需求。
视觉特征提取方法历经了三个主要阶段：
第一个阶段，图像的特征依赖于人工设计的具有旋转和平移不变性算子去提取图 像全局特征（例如“颜色直方图"，“Hu's不变矩”，“梯度直方图”），由于这些简单特 征对图像的抽象还停留在较浅层的程度，如果直接拿它们作为图像高层语义分类或检 索的依据就舍弃了原有的大量的信息，所以效果并不理想
第二个阶段，在简单特征的基础上兴起了基于兴趣点（例如SURF[3],SIFT[4]兴 趣点）的特征表达方法。这种方法对一幅图像中感兴趣的点去提取局部特征，然后利 用基于词袋模型的方法对兴趣点特征进行融合，这样的多个兴趣点融合成的图像特征 信息更为丰富，对图像分类和检索任务的效果有着显著提高。与此类似，一些基于视 频轨迹特征的方法也在视频检测的竞赛上取得了较好成绩。
第三个阶段，从2012年开始兴起的深度学习方法在不同的图像分类和检索竞赛 上都取得了新突破，已经成为当今学术界和工业界的热点之一2]。深度学习能够在大 量数据集下有监督地学习出有效的图像特征表示，其框架的可扩展性和端到端的模式
1
也使得它能够在实际中得以广泛应用，然而，目前在一些特定的视频检测任务上，深 度学习方法由于缺乏成熟的面向视频的网络结构和受到设备的限制，依然没有取得较 大突破。
自2001年开始，由美国国家标准与技术研究所(The National Institute of Standards and Technology, NIST)主办的 TREC (The Text REtrieval Conference)检索会议开设 了 The TREC Video Track评测，为基于内容的视频检索技术研究提供开放的、标准的 评价方法，旨在以公平公开的评测方式展现视频检索的最新技术和进展，从而推动视 频检索及相关技术的发展。评测每年举行一次，包括多个视频内容理解任务，吸引了 超过数十家世界知名大学、研究所和企业参与。作为一项重要评测任务，多媒体事件 检测因极具挑战性而备受关注。本文针对多媒体事件检测中的难点从特征提取到分类 器设计进行了较深入的研究。
1.2	研究现状
多媒体事件检测引起了计算机视觉领域中越来越多的研究者的关注，其任务可表 述为：给定一个视频片段，判断该视频是否发生了某个指定的事件。
该任务的基本形式与视频多分类任务相似，但首先它的类别划分是以发生的事件 为判断依据，这意味着相比过去有关视频数据上行为识别的研究，多媒体事件检测是 一项更具挑战性的任务，因为我们需要检测的事件往往更加复杂和多变，例如，“dog show”的事件可以由一个正在奔跑或跳跃着的狗、牵着这条狗的主人和鼓掌的人群构 成，随着事件的发展，镜头可能会转向不同的场景。多媒体事件检测中事件的判定往 往并不是由单一的因素决定的，需要同时考虑多种信息并对其进行整合再进行预测。 其次，它的关注点更多地在于“检测”上，即需要将指定类型的视频从大量的背景视频 中进行分离，这意味着所用方法必须具有较好的泛化性，而不仅仅是针对特定视频类 别的“专家型”模型。
在过去的十年中，一些基于手工特征和特征聚合的方法在简单的行为识别任务的 获得了较大的成功，这其中包括基于静态图像的算子如HOGE2、SIFT［2。］特征，也包 括一些基于光流图口S7JU2］的算子如HOFE2、MBHE2特征和一些3D算子如3D-HOG【⑶ 特征。行为识别常用的视频数据集为UCF-101 口叫该数据集收集了 101种人的行为， 这些视频内容均以做出某动作地人为主体，同时背景相对稳定和不变。在该数据集上， 基于光流图的方法和3D算子的方法被发现能够达到较高的准确率，相比于静态图像 x»*
2
算子，它们更容易捕捉到视频中人的运动信息。
密集轨迹跟踪(DT)⑸利用光流图的计算来实现视频中的兴趣点跟踪，然后提 取HOG、HOF、MBH特征和相关兴趣点运动信息来描述兴趣点在连续帧上形成轨道， 这些局部特征使用如Fisher Vector0】的特征聚合方法得到对整个视频中人物行为的描 述，从而进行识别。后来IDT?在此基础上加入了人体检测和全局运动消除，取得了 更好的结果。
虽然基于光流图的方法在动作识别任务取得了较好的成果，但［22］中提到，这些 方法在更为复杂、接近于实际的视频分类数据集［15,16,17］上却表现得一般，这说明 这些方法对视频中不规则的镜头运动、复杂的运动背景和参差不齐的质量的鲁棒性较 差。
［23］介绍了一种新的视频表示方法Video Stojy用于行为识别，该方法使用了视频 的演变方向来表示人物行为。然而该方法是针对动作识别任务进行的设计，缺乏对一 般视频数据的普适性。
在近几年深度学习在图像领域取得了巨大成功之后，在口,2,3,9,11,12］中也提及了 基于深度学习的视频分类方法。
［12］中使用了卷积神经网络(CNN) Ml和递归神经网络(rnn)四级联的结构。 CNN网络将静态图像抽象出概念，然后使用RNN对得到的概念序列进行融合，该方 法有效地利用上了图像领域和文本、语音领域的最优方法来对视频进行处理。
［1］提出了双流CNN结构来同时进行空间静态信息和动作信息的捕捉，他将静态 原始图像和光流图各用一个CNN网络进行概念上的抽象，即Spatial CNN和Temporal CNN,提取的特征使用特征聚合或RNN方法进行融合。该方法可类比于过去常结合 静态图像算子和基于光流图的算子进行人物行为表示，而Spatial CNN特征的表达能 力会强于静态图像算子，Temporal CNN特征的表达能力会强于基于光流图的算子， 这使得该方法较传统手工特征的方法也有所提高。但方法的Temporal CNN在较为复 杂的数据集上往往难以收敛，只在UCF-101数据集上取得了较好的结果。
［11］结合了轨迹跟踪的思想，在双流CNN的基础上进行了一定改进。它首先利 用idt算法提提取出对应的轨迹，再利用了 CNN特征图响应机制在网络中提取了轨 迹特征，并利用多层CNN特征融合的方式略微提升了行为识别的准确率。该方法在 对特征图响应机制的利用上具有新意，但缺乏显著的准确率的提升，同时也存在与双 流CNN相同的问题。
另一种更具有表示能力的视频特征由⑵提出，该方法不再考虑连续帧的时序关 系，它使用特征聚合法统计视频的属性。该方法使用了局部聚合描述向量(VLAD)
3
口3,来对CNN特征进行融合，同时该方法与［11］相似地利用了卷积神经网络中的特征 点思想，并用多尺度的方式增强了局部特征的表达能力，最终该方法在MED数据集 上的结果显著地高出了 IDT特征。
从近几年的视频分类与检索发展状况不难看出：
1、上述方法在进行视频分类问题上大部分采用了特征提取、特征融合、类别预 测三步式框架，只有少数CNN与RNN级联模型可看作是端到端的框架。这是由于 受到目前硬件环境和不成熟的框架的限制导致的。
2、多数方法都是基于过去传统方法的基础上进行的改进，这表现在例如利用新 的特征去替换传统特征，而缺少了机制上的突破。
1.3	论文内容及章节安排
本文针对MED任务中的难点，从特征的提取、视频内容的表示、分类器学习等方 面进行了深入的研究，主要工作包括：
1	.提出了一种融合多个深度网络模型的图像特征提取算法,提高了特征的区分性 和泛化能力。
2	.基于经典VLAD编码方法，提出了一种改进的视频表示算法，增强了视频表示 的有效性。
3	.提出了一种基于线性支持向量机的视频分类算法，实现了视频的快速分类。
4	.搭建了一个基于时空融合的多媒体事件检测系统，在TRECVID2016MED评测中 的10EX和100EX任务中均获得了第一名，同时在其他多个数据集和全国首届舆情挑 战赛上也取得了好成绩，验证了本文提出的MED检测框架及算法的有效性。
论文共包含六章内容。其中：
第一章为绪论，在这一章中讲述了本文要研究的问题和在该问题上国内外最新的 一些研究成果。
第二章为多媒体事件检测框架，在这一章将先介绍当前主流的MED框架，并对本 文提出的MED框架进行介绍。
第三章为基于深度网络的特征提取，在这一章中将介绍各种特征提取方法及对比
4
实验。
第四章为多媒体事件的时空表示，在这一章中将介绍各种编码及融合方法及对 比试验。
第五章为多媒体事件分类，在这一章中将介绍多媒体事件分类器的设计，预测 方法，同时将介绍本文方法的相关竞赛成绩。
第六章为总结与展望，总结本文所有工作；并对下一步研究做出展望。
5
第二章多媒体事件检测框架
2.1	当前主流框架
目前主流的多媒体事件检测大体可以分为两类：基于语义表达的多媒体事件检 测，基于平均帧的多媒体事件检测。
2.1.1	基于语义表达的多媒体事件检测
基于语义表达的多媒体事件检测框架的基本结构可表示为图2T所示，其过程可 以表小为:
对视频的帧提取语义信息，该语义信息包括对应视频帧中出现的物体、人物行为 以及所处场景等等；
利用语言模型和视频帧的时序关系对提取的语言信息进行分析，得到对应视频的 描述；
根据训练视频的描述训练分类器，对测试视频进行相关事件的检测。
视频帧
视频帧	语言模型 -'
语义提取	•:	分析语义	一
分类器
图2-1基于语义表达的多媒体事件检测框架基本结构
该框架的优势利用了成熟的语言模型对视频出现的概念进行抽象，能够对于一个 视频的视觉内容进行综合性的考虑。
然而该方法需要提供大量的概念对视频进行描述，这意味着需要用大量样本来训 练语义提取模型，否则将出现概念缺失无法很好地描述视频内容；同时对于视频帧中 的物体、行为、场景的检测一般需要使用多个模型，这大大限制了该多媒体事件检测 框架的检测速度。
2.1.2	基于平均帧方法的多媒体事件检测
基于平均帧方法的多媒体事件检测框架可的基本结构表示为图2-2所示，其过程
6
可以表示为：
（1）提取视频帧的视觉特征；
（2）对所有帧的视觉特征进行平均，得到描述整个视频的特征；
（3）根据训练视频的特征训练分类器，对测试视频进行相关事件的检测。
视频帧
分类器
图2-2基于平均帧方法的多媒体事件检测框架基本结构.
该方法利用了视觉特征来对视频帧中的空间视觉信息进行描述，能够避免对大规 模训练数据的需求。同时在视觉特征选取上，［2］采用了基于卷积神经网络的特征， 结果有了较大程度的提高。
但由于缺少成熟的从帧级别特征到视频级别特征转换的框架，该方法简单地对不 同时刻的特征使用了平均操作，这意味着该方法将丢失大量时序信息，从而达不到较 好的效果。
2.2	基于时空融合的多媒体事件检测框架
本文基于对当前主流的多媒体事件检测框架的优势与缺点的分析，并结合了当时 视频分类任务上较新的一些方法，提出了如图2-3所示的基本框架，其过程可以分为 三个部分：
（1）基于深度网络的特征提取。本文利用深度的卷积神经网络对视频帧进行特 征提取，包括密集局部特征和显著区域特征提取算法。同时，我们也比较了非深度学 习方法中的IDT算法。最终选取了密集局部特征作为基于深度网络的特征提取的基本 算法，具体细节将在本文的第三章中进行详细的介绍。
（2）多媒体事件的时空表示。利用视频帧的时间与空间关系进行特征融合，形 成对视频的时空表示。本文比较了 VLAD、LSTM、netVLAD、时域卷积等特征融合方法, 最终选取了 VLAD作为多媒体事件的时空表示的基本算法，具体细节将在本文的第四 章中进行详细的介绍。
(3)基于Linear SVM的多媒体时间分类。根据训练视频的特征训练线性SVM 分类器，对测试视频进行相关事件的检测，具体细节将在本文的第四章中进行详细的 介绍。
视频帧
\基于深度网络 臂的特征提取
多媒体事件的 时空表示
基于Unear
SVM的多媒体 事件分类
图2-3基于时空融合的多媒体事件检测框架基本结构
本文的方法结合上述主流两种框架的优点，并改进了相应不足的地方。
首先，在视频帧的表示上，本文采用了基于深度卷积神经网络的特征提取，相比 于提取语义信息，介于图像像素信息和抽象语义信息之间的视觉特征与大大加深了对 视频帧的描述的精确性。
其次，与利用语言模型对语义信息进行分析与整合相似，本文提出采用时空融合 的方式来处理帧级别的视觉特征，这相比于求平均的方式将会有较大程度上的提高。
2.3	优势与创新点
2. 3. 1框架优势
本文致力于高效、准确的基于时空融合的多媒体事件检测框架搭建，本文提出的 框架具有以下优势：
(1)准确
本文框架具有较高的准确性，能够达到目前多媒体事件检测任务中较领先的水 平。该框架在国际竞赛TRECVID2016中的MED评测任务，并获得了第一成绩。
(2)高效
本文提出框架具有较高的效率，本文在保证精度的情况下简化系统复杂程度，剔 除了对最终结果影响较小而又耗时的算法，利用多线程让能够并行的部分并行处理, 同时为了简化人为的操作流程，本文采用了抽样的方式并根据经验对部分参数进行估 计，实现了程序一键式运行。
8
（3）易泛化
本文提出的框架具有较好泛化性。由于TRECVID数据集是收集真实视频网站上的 数据和整合多个标准视频数据集而来，视频长度、大小和质量都参差不齐，本文采用 了 TRECVID MED数据集作为基本数据集进行方法测试，这使得本文选取的方法对视频 数据的风格与类型并不敏感，保证了本文提出的框架的高泛化性。同时本文提出的框 架在全国网络舆情（音视频）分析技术邀请赛特定视频识别比赛中也获得了较好的成 绩，这也说明了本文框架易泛化的优点。
（4）低存储
本文提出的框架能够在较小规模的平台上也能完成事件检测的训练和测试任务， 程序在运行过程中不会占用过多的内存和磁盘资源，能够稳健地运行在一台中低配置 的服务器上。
（5）数据易获取
利用本文提出的框架进行多媒体事件检测或其他视频分类任务时所需提供的数 据非常容易获取到，只需要提供少量标注的训练数据就能够训练一个新模型，且标注 不需要精细到帧的级别，仅仅是给定一个视频整体所属的类别标签即可。
2.3.2创新点
本文提出的基于时空融合的多媒体事件检测框架创新点主要包括两方面：
（1）首先，本文结合深度学习和传统特征聚合方法的优势，将CNN和VLAD应用 于视频事件检测当中，取得了显著的成果。
（2）其次，本文在探索基于时空融合的多媒体事件检测的方法时提出多个新的 算法，其中包括显著区域特征提取、netVLAD、时域卷积，尽管在部分算法的准确率 在当前训练数据规模下还略有差距，但通过实验结果分析，这些算法仍然具有超越现 有算法的潜力。
9
第三章基于深度网络的特征提取
作为多媒体事件检测基本任务，特征的选择和提取某种意义上对检测性能起着决 定性的作用。本文在分析比较现有的多种时空特征的基础上，提出了基于深度网络的 特征提取算法。
3.1	DT/IDT 特征
密集轨迹(DenseTr句ectojy)特征是视频运动信息捕捉常用的算子之一，它可以 实现在视频中对兴趣点的跟踪和局部特征的提取用于表示轨迹的特征，而轨迹的特征 可以结合一些特征融合的方法来实现对整个视频的特征表示。
DT特征的计算可以分为三步，分别为光流图的计算、轨迹跟踪、轨迹特征计算。
3.1.1	光流图的计算
光流图表示的是一副图像的运动矢量图，他利用连续两帧图像对各个位置的运动 方向进行估计，从而构成一张2通道的图像，两个通道分别表示原图像各个位置向横 轴的运动分量和向纵轴的运动分量。
假设图像某点坐标为(x,y),此时时刻为t,将其灰度表示为I(x,y,t),假设在下一时 刻t+At时该点运动到位置(x+Ax,y+Ay),该点的灰度值应该是不变的，即有：
I(x,y,t) = I(x + Ax,y + Ay, t + At)	(式 3-1)
利用泰勒公式展开可以得到:
I(x + Ax, y+ Ay, t + At)
=I(x5y9t)+ —Ax + —Ay + —Az + —A/ + //.OT	(式 3-2)
ox oy dz ot
H.O.T代表更高阶的分量，在移动量较小时可以忽略。
结合式(1) (2)我们可以得到
di Ax di Av 57 八
----+---+— = 0 dx A/ dy A/ dt
(式 3-3)
10
di T7	di TZ	di A
—K +—Vv+— = 0 dx	dy ■	dt
(式 3-4)
其中:、卷和寺是当前点三个方向的梯度，而匕和匕是需要求解的量，由于
利用当前点只有一个方程而有两个未知变量，无法直接求解方程，一般采用利用迭代 或者特定的假设的方法进行求解，这里不做过多的介绍。
3.1.2	轨迹跟踪
DT 一般会在起始帧进行兴趣点的采样，这个过程是由一个空间下采样的金字塔 完成，以
T = 0.001 x maxmin（2', ）	（式 3-5）
为阈值滤除掉各个尺度上变化平稳的点，其中£.和者为第i个兴趣点的特征值o 剩下的点将被进行跟踪，由于我们已经计算出光流图，那么可以根据当前任意一点的 位置得到下一时刻该点出现的位置，即：
即1，乂+1）=（尻,乂） + 匕 + 匕，	（式 3-6）
其中为和y,为当前时刻的横坐标与纵坐标，x㈤和y“］为相邻一帧的横纵坐标，匕 和/是光流图上点（x,y）的值，即运动速度，将当前点的位置和他的位移相加即可得到 下一时刻的位置。那么以此方式来对兴趣点进行跟踪，可以在视频中形成多条轨迹， 同时为了降低跟踪的累计误差，每隔一定帧数之后会中断这条轨迹的跟踪，之后的帧 会根据兴趣点筛选的情况重新进行跟踪。
3.1.3	轨迹特征计算
在计算出轨迹之后，我们利用一些算子去提取这条轨迹周围的特征，即可表示出 轨迹特征。这里采用了三种算子：HOG、HOF、MBHO其中HOG为梯度直方图，表 示的是当前区域的纹理信息；HOF为光流图的直方图，光流图是这幅图像的运动矢 量图，所以HOF表示的即为当前区域的绝对运动信息。MBH为光流图的梯度直方图, 其中光流图的梯度差分可表示图像的相对运动，则MBH表示的是当前区域的相对运 动信息。
11
对于一条轨迹的邻域，可以把它分为2x2x3个网格，按时间轴对每个网格求平 均得到每个网格的特征，最终将这12个网格的HOG、HOF、MBH特征和轨迹上兴 趣点运动速度拼接起来构成最终的轨迹特征。
图3-1中描述了整个轨迹跟踪特征计算过程。
Tracking in each spatial scale separately	Trajectory description
图3T密集轨迹特征计算示意图⑸
Improved Dense Trajectory是在Dense Trajectory基础上做的改进。由于视频中经 常有镜头转移、晃动的情况，按照DT的轨迹跟踪方法，所有兴趣点都会被认为处于 一个运动状态，即使是目标是完全静止而只有镜头的运动，从而影响到轨迹跟踪和 HOF特征表达的效果。而IDT通过全局运动估计来计算镜头运动产生的矢量，从而 修正光流图，提高了 HOF特征的对全局运动鲁棒性。同时IDT加入了人体检测，由 于人往往是动作识别视频的主体，镜头跟随着人体运动，所以人体总是与镜头保持相 对静止，这样也能够估计出镜头的运动矢量来辅助去除镜头运动带来的噪声。
3.1.4	IDT与CNN对比实验
1	.实验设置
本文采用IDT方法做视频的轨迹跟踪在MED数据集上进行测试。对每个输入视 频，将其解码得到的帧进行等比例的尺度放缩。在试验中，我们以短边长为150像素 作为视频帧的输入尺度。
对每个输入视频，将其解码得到的帧进行等比例的尺度放缩。根据IDT论文的 提及的在UCF-101上所做的实验，输入帧分辨率为160x120,由于MED数据集视频 内容更为复杂，将其帧的分辨率设置得略大一些，以短边长为150作为视频帧的输入 尺度。
在进行轨迹跟踪时，采用逐帧跟踪的方式，每条轨迹跟踪长度为15,密集采样 间隔为5个像素。同时由于MED数据集中事件的主体不一定是人，而IDT的人体检 测会移除人体框内点与人体框外点的错误匹配，这并不利于一些人与动物、物体同时
12
出现时的跟踪，所以在实验中去除了人体检测这一功能。
在进行轨迹特征计算时，对跟踪的轨迹取附近32个像素的区域计算特征，每条 轨迹的领域构成的三维区域在空间域上被划分成2X2个网格，在时域上被划分成3 个网络，总共形成12个网格，对于每一个网格选取了 8维HOG特征、9维HOF特 征、16维MBH特征，加上30维轨迹运动向量，总共构成426维特征来表示轨迹特 征。根据DT论文描述，使用426维级联的特征比使用单个HOG、HOF、MBH特征 拥有更高的准确率和鲁棒性，所以这里沿用了论文中的特征使用方式。
IDT特征采用了由PCA3降维和Fisher Vector进行特征聚合用来表示视频，其原 理可参照多媒体事件的时空表示部分。使用PCA降维时的投影矩阵由训练集中随机 采样25万样本计算得出，投影后的维度为256维，使用Fisher Vector时设置高斯函 数个数为256,输出特征维度为131 072维，输出特征使用了平方根归一化和12归一 化。
由此得到的视频特征使用支持向量机(SVM)[36]进行类别预测，并用mAP[35]作 为其性能评测标准，与卷积神经网络特征提取中的密集局部描述子进行了对比，后者 实验设置可参考CNN特征的实验描述。
在进行实验时，将TRECVID2015 MED口61的训练集分成了小规模的训练集和测 试集进行了实验：
由于TRECVID MED竞赛最主要评测项为10EX,即每类事件有10个正样本用 来训练，本文在构建小规模的训练集和测试集用于验证算法时也采用了相似的数据分 布。数据集包含20个事件和背景样本，总共8030个视频样本，每类事件有100个正 样本。
训练集：包含800个视频，每类事件有10个正样本；
测试集：包含7200个视频，每类事件有90个正样本。
在之后的实验中我们将此数据集命名为SMALL_10EXo在此基础上得到的CNN 特征与IDT特征在验证集上的mAP结果如表3-1所示。
表3T CNN与IDT对比结果
方法	mAP
CNN	0.504
IDT	0.395
(2)结果分析
13
由以上结果可以看出，IDT特征作为视频基本描述特征相比于CNN特征具有显 著的差距。一方面这是由于IDT特征都是由一些直方图向量构成，与CNN特征比较 本身就缺乏一定表达能力；另一方面在进行轨迹跟踪时容易受到镜头移动引起的全局 运动的干扰，同时对空间尺度变化不鲁棒，所以导致跟踪效果比较差，得不到较有效 的特征表达。
本文可视化了一些监控视频数据集和MED数据集中的一些轨迹跟踪结果进行对 比
(a)	(b)
（c）	（d）
图3-2 IDT结果可视化
如图3-2所示，（a）是监控摄像头拍摄的视频中的一帧，轨迹跟踪结果如图（b） 所示，红色的点表示经过采样和筛选过后的兴趣点，而绿色的线则表示轨迹，可以从 图（b）看出，运动的人的轨迹被正确地跟踪了。而图（c）是MED视频数据中的一 帧，轨迹跟踪结果如图（d）所示，可以看出跟踪的结果杂乱无章。相比较图（a）中 始终是静止的背景，图3中的场景较为复杂，一方面自行车的轮子在转动，另一方面 镜头始终跟随自行车而使得地面相对于镜头在运动，这就导致了错误的跟摩，从图（d） 中可以看出，尽管轮胎转动的趋势可以被反映出来，但地面上大量的兴趣谓被错误地
14
跟踪，呈现出错误的运动趋势，从而影响到特征表达。
从另外的角度来看，IDT在行为识别上的良好表现还依赖于已知动作的主体是 人，我们能够通过轨迹的特点来推测人做了什么动作，IDT的特征本身设计旨在使特 征在人的不同动作上具有较好的区分度；而如果不知道运动主体，很难通过轨迹来推 测具体发生了事件，因此IDT特征在事件分类上也就缺乏较好的区分度。
3.2	基于CNN的密集局部特征
3. 2.1卷积神经网络
卷积神经网络是在原始的神经网络基础上新提出的一种结构，图3-3中，(a) 为原始神经网络的连接结构，(b)为CNN的链接结构。相比于原始的神经网络， CNN最首要的一个变化在于局部感受野的提出，利用参数共享的滤波器实现了从底 层图像原始信息到高层语义信息的逐步抽象。
(a)	(b)
图3-3卷积神经网络网
局部感受野的思想被证实与人的视觉神经类似，CNN网络中卷积层的节点与前 一层的特征图呈局部连接结构，每个节点都对应了前一层中局部区域，它只接受这些 节点的输入，这种局部连接的结构使得网络参数大大减少。而另一方面，这些局部连 接的节点间的参数是共享的，而不同的视觉概念靠多个不同的卷积滤波器来映射，这 种参数共享的结构同样极大程度上减少了参数，使得在图像领域训练高层的神经网络 结构成为可能。
在CNN对原始图像进行逐层抽象后，高层的滤波器需要更大的感受野来抽象出 新的概念，而CNN中的下采样层通过直接减少特征图尺寸的方式变向扩大了感受野,
15
使得越高层的滤波器对应原图像上的感受野越大，从而实现了原始图像信息被抽象、 融合、再抽象、融合 的过程。下采样层目前一般采用Max Pooling或者Average Pooling,分别是对输入求最大和求平均的操作，这两种结构都保证了局部的旋转不变 性和尺度不变性，使得模型具有良好的泛化性。Max Pooling能够突出感受野的响应, 作为中间的下采样层十分有效，而Average Pooling是对响应的平均，适合用于网络 最后一个的卷积层融合特征图作为图像的输出特征。
在目前常用的CNN网络中，一般采用ReLUm这种非线性激活函数，相比于传 统的Sigmoid函数，ReLU具有更快的收敛速度，同时使神经元具有更强的稀疏性， 而且被证实更加接近于生物的脑神经信号。两种非线性激活函数如图3-4所示。（a） 为Sigmoid函数，而（b）为ReLU函数。
（a）	（b）
图3-4非线性激活函数
3. 2.2 CNN特征图响应机制
CNN是一种依靠局部连接结构进行逐层抽象的结构，而由于这种特性，可以发 现CNN特征图响应之间的关联性。CNN网络在进行卷积时，特征图的尺寸是不变的， 而特征图上的每一个点都有接受的是前一层特征图以同样位置的点为中心的局部区 域的区域，而多个这样的卷积层进行堆叠之后，虽然高层的特征图的点会接受较大的 感受野，但其始终都是对与其在相同位置的点（即感受野中心）的响应最强，这是由 于高层神经元总是从相同位置的底层神经元接收到的输入次数最多。在图3-5中，最 底层3号节点将值作用于最高层节点有三条路径，而2、4号节点只有两条，1,5号节 点只有一条。而最终导致3号节点对高层节点的值影响最大。
16
图3-5特征响应机制示意图
这也说明CNN特征图响应具有位置敏感性。如图3-6所示，在在网络任意一层 的特征图上任取一个位置，一定对应了原始图像上的一块区域，该点不同通道的响应 值对应了原始图像区域不同属性的强弱分布，那么该点各个通道构成的向量就可以表 示成这块区域的特征。
原始图像
特征图	局部特征
图3-6特征图位置敏感性示意图
而特征图尺寸的改变仅仅是在下采样时进行的，例如一般的网络在进行下采样是 利用max pooling将特征图的长宽各减少一半，那么我们也可以认为经过此下采样层, 特征图上的每个点的对应原始图像的感受野增大了一倍，因此在同一个网络，越高层 的神经元的感受野越大。
3.	2.3图像与网络尺寸的适配
在利用CNN提取视频帧特征时，视频帧图像的尺寸和原始网络输入尺寸不一定 适配，即原始网络一般采用正方形尺寸的图像作为输入进行训练，而视频的帧可能有 不同的长宽比，于是需要对图像输入尺寸进行调整。
°常见的一般采用crop或resize的方法。crop方法如图3-7 (a)所示，该方法等
17
比例放缩视频帧后取图像正中的正方形作为网络输入，但正方形之外的区域信息将会 被丢弃。resize方法如图3-7 (b)所示，是以线性内插的方法对图像缩放至网络输入 大小，其对图像内容的形状有一定的扭曲，使得卷积层无法准确地对扭曲的图像进行 抽象，从而影响特征的表达效果。
(a)	(b)
(c)
图3-7图像与网络尺寸适配示意图
另外还有一种处理方法既不会像resize操作扭曲图像,也不会像crop操作丢失少 量信息，本文命名为whole方法。由CNN特征图响应机制可知，对于任意一个尺寸 的图像，我们都可以在任意特征图上得到图像区域的特征，所以对于长宽不等的图像 的特征提取，我们可以平均CNN最顶层的特征图响应作为图像的全局特征。但由于 输入图像大小往往是不确定的，对于图像边缘像素可能需要补零才能进行运算，这意 味着该方法容易产生边缘效应，如图3-7 (c)所示。
3.2.4密集局部特征
很多局部特征提取算法都运用到了空间金字塔结构，例如si任特征利用了多尺度 高斯差分金字塔进行兴趣点的筛选,dt特征使用了空间金字塔下采样进行轨迹起始点 的筛选。
18
本文利用空间金字塔结构和特征图响应机制进行密集局部特征提取。如图3-8所 示，本文构建多个呈空间金字塔结构的Pooling层，用不同尺寸的Pooling核在特征 图上进行运算，得到多个新的不同大小的特征图，这些特征图上每一个位置对应了原 图某个区域的特征，描述了原始图像上不同位置和不同大小的感受野，本文把这些特 征图响应作为原始图像的视觉特征，他们具有多尺度的特性。
多尺度
Map Pooling
图3-8密集局部特征提取示意图
在密集局部特征提取过程中，由于提取的视频帧特征还需进行进一步处理才能用 来表示视频，一般会使用Max Pooling而并非Average Poolingo Average Pooling会使 局部响应被平均作用抑制，而MaxPooling的操作保留了更多的细节。
3.	2.5网络模型对比实验
1	.实验设置
本文比较了 4种目前在ImageNet上表现较好的网络模型，其中VGG19, GoogleNet,ResNetl5214］均在ImageNet的1000类图像数据上进行训练，模型参数文件 由［11］上下载而来，而GoogleNetl2988c［皿在ImageNet的12988类数据上进行训练。 视频帧特征提取采用密集局部特征提取，在此基础上得到的CNN不同特征提取方式 在SMALL_10EX数据集上的mAP如表3-2所示。
表3-2网络模型对比实验结果
模型名称	mAP
VGG19	0.467
GoogleNet	0.475
GoogleN etl2988c	0.504
19
ResNetl52	0.498
2	.实验结果分析
由上述表格可以看出，GoogleNetl2988c为最优模型，在本文其他实验中，若不 作特殊说明，均默认使用GoogleNetl2988c为基本卷积神经网络模型。
在上述4中模型中，VGG19,GoogleNet和ResNetl52参加过ImageNet竞赛，分 别为2014年亚军、2014年冠军、2015年冠军，在测试集上的准确率由高到低依次为 ResNetl52、GoogleNet, VGG19。我们可以发现，这三个模型在多媒体事件检测任务 上的准确率由高到低也依次为ResNetl52、GoogleNet. VGG19,因此我们可以得出 结论，本文框架下的事件检测准确率随着所用特征提取模型的精度的提高而提高。
对比 GoogleNetl2988c 和 GoogleNet,两者结构不变，但由于 GoogleNetl2988c 用了更大的数据量进行训练，模型的泛化性较好，所以在事件检测上的准确率较高。 尽管GoogleNetl2988c只有22层，相比于ResNetl52的152层结构，理论上准确率 会较低，但大数据下训练带来的高泛化能力使得在事件检测上的表现优于ResNetl52 网络，这也证明本文框架下的事件检测准确率随着所用特征提取模型的泛化性的提高 而提高。
因此我们可以得到结论，特征提取模型的精度和泛化性共同影响着事件检测的准 确率。
3.	2.6帧预处理技巧对比实验
由于从视频中采样的视频帧尺寸不一定与原网络适配，本文测试了三种图像与网 络适配的方式对最终事件检测准确率的影响。
该试验采用VGG19网络作为视频帧的特征提取模型，视频帧采样间隔为每5帧 取一帧，VGG19网络的原始输入默认为224X224的正方形图片，本文通过以下三种 方法进行处理：
(1)	resize
对于一张长宽不等的矩形视频帧图片，该方法将图像以线性内插的方式放缩到 224X224。在最后一个卷积层均按照密集局部特征提取的方式提取IX 1,2义2,3X3,6 X6四种尺度特征图的响应，每帧得到50个局部特征。
(2)	crop
对于一张长宽不等的矩形视频帧图片，该方法先将图像在保持长宽比不变的情况 下放缩到短边为224的尺寸，再居中截取出丁个224X224的正方形图片作为网络输
20
入。在最后一个卷积层均按照密集局部特征提取的方式提取IX 1,2X2,3X3,6X6四 种尺度特征图的响应，每帧得到50个局部特征。
(3)	Whole
该方法先需要对网络参数进行一定转换，由于VGG19网络中含有全连接层，而 全连接层限制了前一层特征图的尺寸，所以无法支持变尺度的图像输入，这种情况可 以考虑将全连接层转换为卷积层，但由于我们只提取网络最后一层卷积层的输出，所 以实验中去掉了全连接层和softmax层，并在最后一层卷积层之后接上了高度为3的 SPP层。对于一张长宽不等的矩形视频帧图片，该方法先将图像在保持长宽比不变的 情况下放缩到短边为224的尺寸，再按照该矩形图像尺寸调整网络尺寸。在进行前向 传播之后，对SPP层的输出进行切分，每帧得到21个局部特征。
在此基础上不同的视频帧处理方法在SMALL_10EX数据集上得到的mAP如表 3-3所示：
表3-3视频帧处理方法对比实验结果
视频帧处理方法	mAP
resize	0.423
Crop	0.451
Whole	0.446
结果分析
上述表格可以看出，crop方法为最优方案。
resize方法会对原图的形状比例由一定程度的拉伸，一定程度上影响了局部特征 的准确性，使得最终事件检测准确率较低。
crop方法不会对原图的形状产生扭曲，但会舍弃图像边缘的一部分信息，然而由 于视频本身的主要目标往往集中出现在视频中央，所以crop方法舍弃的信息对最终 事件检测结果影响较小，其准确率较resize较高。
而whole方法既没有对原图进行拉伸也没有丢弃边缘信息，但是由于网络最后一 个卷积层的输出尺寸短边长为7,而长边长不确定，可能存在进行pooling运算时对 其边缘补零的情况的情况，这种边缘效应使得图像边缘产生的局部特征缺乏一定准确 性，从而降低最终准确率。而crop方法由于输入尺寸固定，可以人为设计pooling核 大小和步幅，例如实验中采用(大小，步幅尸(7,6),(4,3),(3,2),(2,1)四种pooling核来获取 1X1,2X2,3X3,6X6三种尺度的特征图，避免了最后一层特征图上补零的情况。但比 较crop方法和whole方法，两者在最终结果上相差不大。
3.	2.7优化策略
视频帧特征提取是整个框架耗时最多的一部分，本文同时采取了多种简化优化策 略来对特征提取过程进行加速和减少开销，但由于优化策略根据实际运用时硬件配置 的不同也将不同，所以每本没有给出对于每种策略不同参数下的实验结果，仅仅列出 整体的优化结果并根据理论上的依据对其进行分析。
1	.加大帧的取样间隔
对于帧的取样间隔取舍，一般认为小的取样间隔将会使总的局部特征数量增加， 使整个视频被描述得更加详细，从而使最终的事件检测准确率提高，而大的取样间隔 会使总的局部特征数量减少，对视频整体的描述更加粗略，从而使最终的事件检测准 确率降低。而过小的视频间隔会使相邻的视频帧提取出的局部特征缺少变化，过于相 似的特征可以认为是冗余的。另一方面，由于MED视频长度从10秒到10分钟不等, 长视频将产生远多于小视频的局部特征，对于长视频可以采取加大采样间隔的策略。
在本文其他的实验中，采用了 5帧一次的固定取样间隔的方式，之后对其进行了 优化，改为了 8帧一次的采样间隔，于多余3000帧的视频，本文加大了视频帧采样 间隔，使其采样的总帧数接近3000帧。本文就改进后的准确率和耗时跟之前的方式 进行了对比，网络采用了 GoogleNetl2988c模型作密集局部特征提取，其在 SMALL」0EX数据集上的mAP与耗时如表3-4所示：
表3-4取样间隔优化实验结果
方法	mAP	特征提取耗时
改进前	0.504	34h
改进后	0.506	22h
2	.并行处理
并行处理可以大幅度提高视频解码速度和特征提取速度，本文从工程方面优化了 特征提取的过程，对于视频解码出的需要进行特征提取的帧，本文采用Mini-batch的 形式对其进行合并作为网络进行前向传播。由于卷积神经网络进行计算时均以矩阵乘 法作为基本运算，mini-batch的形式将多个小矩阵的相乘的形式合并成了大矩阵的相 乘的形式，这使得计算效率大大提高。同时由于视频解码主要依靠CPU,特征提取 依靠GPU,而视频解码和特征提取采用了异步的方式，这意味着两者需要互相等待, 资源不能被同时利用上，所以本文采用了多线程调度的策略来使特征提取过程加速。
22
本文测试了 Mini-batch大小为10、4个线程的情况下的特征提取耗时，测试样本 为SMALL_10EX数据集上8030个视频，由于并行策略并不会改变事件检测准确率, 所以该试验并没有进行特征融合和类别预测的过程，仅在表3-5中列举出了并行情况 下的耗时与非并行情况下的耗时。
表3-5并行优化实验结果
方法	耗时
非并行	22h
并行	5h20min
3	.减少局部特征数量
本文尝试了减少局部特征数量来减少中间存储消耗，由于局部特征数量关系到最 终准确率的结果，本文仅对比了由IX 1,2 X 2,3X3三种Max Pooling尺度产生的14 个局部特征和IX 1,2X 2,3 X 3,6X6四种Max Pooling尺度产生的50个局部特征的准 确率和存储消耗。实验以GoogleNetl2988c为模型进行特征提取，使用VLAD进行特 征融合和SVM进行类别预测，其在SMALL_10EX数据集上的mAP和存储消耗如表 3-6所示：
装3-6减少局部特征数量实验结果
Max Pooling 尺度	存储大小	mAP
1X1,2X2,3X3,6X6	1.2T	0.498
1X1,2X2,3X3	367G	0.497
4	.实验结果分析：
根据实验结果，我们可以观察到，一定程度地加大取样间隔对特征提取速度有较 大地提高,而准确率没有几乎保持不变;并行处理对特征提取速度有着大幅度的提高; 一定程度地较少局部特征数量在保证准确率几乎不变的情况下大幅度地降低了存储 消耗。这三种优化策略可同时使用，能够大幅度提升系统整体的运行效率和节省资源 开销。
3	.3基于CNN的显著区域特征
3. 3.1显著性思想
23
显著性思想被运用在很多手工特征的设计中，例如sift兴趣点和dt轨迹的选取， 都去除了图像变化平滑的区域，只提取了目标周围的局部特征。考虑过多的背景并不 会增加特征的信息量，因为实际对于分类或检测任务最有区分度的信息集中在目标周 围，而加入过多背景信息会使不同类别之间的区分度降低，从而使结果变差。
对于基于CNN的密集局部特征的提取过程，我们可以看成是原始图像被打上了 多种尺度的网格，而每条特征来源于其中的一个网格，而大量处于视频帧的背景中的 网格同样提取出了特征，我们认为这些特征是不具有代表性的，希望从中剔除他们。
我们考虑用候选框的方式代替网格，这些候选框只包含我们认为有显著的目标， 而不包含背景部分。同多尺度特征提取的方式一样，我们可以对这些候选框对应的特 征图上的位置进行Max Pooling来作为这个候选框的特征，我们提取每个视频帧候选 框的特征去描述这个视频。
3. 3. 2 Icept ion 结构
本文提出的显著区域特征提取算法中使用了 CNN中的Inception结构，该结构也 是一种空间金字塔结构，能加强网络对空间尺度的鲁棒性。
Inception在网络结构如图3-9所示，它的设计上不再只简单地利用多层卷积层的 堆叠，而在同一层卷积层中使用了不同空间尺度的卷积核，这些卷积层的输出节点也 就拥有了不同大小的接受域，最后这些输出节点被连接起来，作为Inception结构的 输出，GoogleNet依靠Inception结构的堆叠使得网络拥有更少的参数和更快的运算速 度，同时也有了更加强大的拟合能力。
图 3-9 I ncept i on 结构［241
3. 3. 3 RPN 网络
传统的候选框提取方法如selective search^。屎用了大量手工特征比对的方式，能
24
够产生高召回率的候选框，但由于其速度慢会直接影响视频特征提取速度。而在最近 的基于CNN的物体检测框架faster R-CNN⑹J均放弃了利用原始手工特征去产生候 选框方法。
在faster R-CNN中，候选框的选取由Region Proposal network (RPN)⑹i网络产 生，其结构如图3-10所示，RPN网络由不同尺度的anchor1。和前背景分类器和回归 器构成，RPN网络在训练时以与anchor的重叠率划分出前背景样本进行训练，产生 候选框时以判断为前景的anchor的边框回归值作为候选框。
图3-10 Faster R-CNN中RPN网络结构⑹
而在实际视频帧的显著区域特征提取中，直接使用RPN网络会有一些缺点：
首先，过多的重叠的anchor是不必要的，否则这些anchor的特征的重复计算将 引起较大的计算开销；
其次，过于精确的回归过程是不必要的，因为最终目标是提取出视频帧的显著性 特征，回归产生的候选框一般与输入anchor框一般落在相同的网格内，产生相同的 特征。
3. 3.4显著区域特征提取
本文结合RPN网络，采用了一种更为简单的机制去产生显著区域的候选框：
对CNN的特征图上添加一个显著性判别分类器，该分类器将产生一个新的特征 图，以阈值滤除响应较低的位置，以能覆盖住剩下位置的最小矩形框作为显著区域的 候选框。
图3-11展示了输入图像与分类器产生的特征图的位置对应关系，图中较暗的位 置为分类器产生的特征图滤除的部分，越亮的部分表示特征图上响应越强的地方，我 们用能覆盖住响应的最小矩形作为显著区域的候选框。
25
*
图3Tl显著区域特征提取示意图
最小矩形的计算可表述为图3-12所示。首先挑选一个响应对应的矩形，判断他 是否与另一个矩形相接，如果相接则融合成一个较大的矩形；重复这个过程直到所有 矩形都不相接或者只剩下一个矩形。
图3-12最小矩形计算示意图
显著区域特征提取主要流程按如下进行：
（1）首先对于一个在ImageNet^上预训练好的网络，我们去掉全连接层然后在 最后一个卷积层之后加上两个Inception结构用于特征的调整，在Inception之后加上 尺寸为Ixixc的卷积层和softmax用于分类，C为类别数，本文将交通工具、人即人 脸、动物、杂物作为需要判别为显著区域的类别，外加一个背景类别，构成了一个5 分类的分类网络。以AlexNet网络为例，假设图像输入尺寸为MxM,而这个网络将
26
产生一个尺寸为M/32xM/32xC的特征图。
（2）用一个高度为1的SPPM层将该特征图下采样至IxlxC,表示预测标签。
（3）将Inception结构之前的层学习率都置为0,将图像按一定梯度进行缩放后 作为网络输入（本文中将输入图像从128以32为步长放缩至256）,再使用交叉燧 损失来训练该网络。使得网络能够通过调整新添加的Inception结构对不同大小图像 输入也能正确给出预测。
（4）对于视频中的一帧，我们将其等比放缩至短边长为224像素，将其通过网 络得到softmax层可以得到一个短边长为7的概率分布图，每个通道对应了原图中相 应类别的分类概率。我们以阈值划分滤除响应较低的位置，融合相邻的位置（包括对 角），最终得到的区域即为显著性区域。
（5）由于由最后得到的概率分布图短边长只有7,无法对较小区域产生强响应, 所以利用了更为浅层的特征图搭建了相同的显著区域检测结构，该检测结构输入的特 征图短边长为14,感受野和原始图像上的步幅均为较小特征图的1/2,同样添加了两 层Inception结构和softmax分类器，用相同的方式进行训练。检测输出会与较小特征 图产生的结果进行合并，合并方式先由非极大值抑制以置信度大小的排序为优先级去 除重合度较高且同类别的的区域，然后融合剩下依然重叠的区域，所有不同类别的区 域不会进行抑制和融合。
（6）在完成显著性区域检测后，所有的区域按特征图响应机制映射到需要提取 特征的特征图上，对每个区域的映射结果进行Max Pooling,得到最终结果。
图3-13中显示了整个显著区域检测网络训练和显著区域特征提取的流程。
27
预训练	预训练 'L ..
的网络 \	/ 的网络 \
训练	测试
图3T3显著区域特征提取示意图
3.	3. 5显著区域特征提取与其他方法的对比实验
1	.实验设置
本文比较了三种不同的CNN特征提取方法，在CNN特征提取方法比较的实验 中，均采用caffes框架，使用了 GoogleNetl2988c卬］网络，视频帧采样频率均为每5 帧取一帧。
(1)全局特征
在提取全局特征时，将原始帧按照crop方式输入网络，提取了最后一个Average Pooling层的输出，得到1024维全局特征向量。
(2)密集局部特征
在提取密集局部特征，同样将原始帧按照crop方式输入网络，在最后一个卷积 层之后添加了 3个Max Pooling层，得到大小分别为1 X 1,2X2,3 X3的特征图，构成 共50个1024维密集局部特征。
(3)显著区域特征
在提取显著区域特征前，首先需要进行RPN网络的训练。本文的试验中从 ImageNet上收集了约2000张关于“人”“动物”“交通工具”“规则物体”共四类并且带有
28
Bounding Box的图片，在SUN[43]上收集了约2500张风景照。利用随机裁剪的方式 对样本进行扩充，对于ImageNet上的图片，与原Bounding Box重合度小于0.3的样 本被作为负样本，重合度大于0.7的样本作为正样本，介于0.3与0.7之间的样本被 丢弃；对于风景图片的剪裁全部被加入到负样本当中，以此样本被扩充到越20000 张图片。同时对图像进行尺度上的扩充，分别以128、224作为图像等比例放缩后的 短边长度的最小值和最大值，最终训练样本大约被扩充至越十万张图像。对于样本的 剪裁和尺度变换能够使显著区域检测时得到的特征图响应对目标遮挡和尺度变换具 有更好鲁棒性。
在训练时，本文采用每30张图像进行一次权重更新的方式。
在训练完RPN网络后，对视频帧进行显著区域特征提取，每帧可得到若干个1024 维特征。需要注意的是，本文认为对于较为复杂的事件，视频中的背景环境同样可能 有助于视频分类，所以在显著区域的候选框集合中始终添加一个全局的候选框，使得 对每帧视频的特征提取时总会“看一眼”该帧的整体，但该候选框有助于加强算法鲁棒 性，但由于每帧只会产生一次全局的候选框，对视频的特征表达产生主要影响的仍是 显著区域。
CNN特征采用了由PCA降维和VLAD进行特征聚合用来表示视频，其原理可参 照多媒体事件的时空表示部分。由此得到的视频特征使用SMALLJ0EX数据集训练 支持向量机并进行类别预测，在此基础上得到的CNN不同的特征提取方式在验证集 上的mAP如表3-7所示。同时由于PCA投影矩阵和VLAD码数训练时需要从训练集 中采样，所以训练集视频帧级别的特征不可避免地需要存储在硬盘当中，所以存储消 耗被列入了特征提取的一项评测标准。
表3-7特征提取方法对比实验结果
方法	存储大小	mAP
全局特征	13G	0.481
密集局部特征	367G	0.497
显著区域特征	46G	0.495
2	.实验结果分析
由上述结果可以看出，显著区域特征和密集局部特征两者结果显著高于全局特 征，由于密集局部特征和显著区域特征都是利用特征图响应机制提取的局部特征，能 够表示出更多的细节，所以也响应地提高了结果。
29
密集局部特征和显著区域特征相比，本文认为密集局部特征更优，首先在mAP 上略微胜出，同时由于其形式简单，不许要额外的训练过程，同时由于不需要进行显 著区域的判别，可以很容易地将该方法泛化到其他视频分类或检测的应用场景当中」
本文将密集局部特征提取作为基本的帧级别特征提取方法，在本文其他实验中， 若不做特殊说明，默认使用密集局部特征并沿用其模型、取样间隔、预处理等设置。
然而对于一个小规模的平台，显著区域特征能够在几乎不降低准确率的情况下有 效地减少存储消耗，也是一种候选方案。
本文认为密集局部特征的提取过程中，多尺度的Max Pooling始终会包含显著区 域，但会包含更多的非显著区域；而在显著区域特征提取当中，这些大量的非显著区 域被显著性区域判断给滤除了，所以最终得到的特征占用的存储空间大大减少，并且 这些非显著区域并不会对结果有较多的帮助，所以也没有引起准确率的显著下降。
30
第四章多媒体事件的时空表示
' 多媒体事件的表示是多媒体事件检测的另一个重要内容。本章首先剖析了当前的 主流视频表示方法的优缺点，然后结合特征的提取提出了有效的事件时空表示算法， 进行了性能评价。
4.1	VLAD
4.1.1	BOVW 模型
视觉词袋模型(BOVW)模型是传统的特征融合算法之一，在深度学习未广泛应 用之前，使用局部特征点和视觉词袋模型表示图像信息是计算机视觉领域主流的方 法。视觉词袋模型属于生成式模型，更加关注于类别条件概率的建模，而相比于直接 关注于分类问题本身的判别式模型，生成式模型在特定分类任务上并不具有直接的优 势，但其也拥有很多判别式模型无法代替的优点，例如能够处理长度不一的数据，拥 有较好的泛化能力，能够合理运用先验知识简化求解复杂度。
最早期的词袋模型是利用词频来描述文章，而在图像领域则是利用图像潜在概念 出现频率来描述图像。对于图像上已提取的兴趣点局部特征，由于在不同的图像上数 量也不同，无法直接比较相似度。而视觉词袋模型利用聚类的方法，将样本点分为几 类，统计每类出现的频率，以此构成等长的向量来进行相似度的度量。如图4-1所示, 聚类形成的类别，可以看成是潜在的视觉属性，而最终视觉词袋的特征表示的就是各 个视觉属性出现的频率。越是相似的图像，其出现相似的兴趣点概率便越高，越容易 被投影到相同的类别当中，统计的视觉属性频率便越相似，视觉词袋特征的距离便越 相近。
31
图4-1 BOVW模型示意图
视觉词袋模型虽然简单，但其被广泛应用后来延伸的一些特征融合算法，同时也 是图像检索中基本的索引算法。
4.	1. 2 Fisher Vector 模型
Fisher Vector是概率模型下BOVW的衍生产物，Fisher Vector利用混合高斯模型 进行聚类，再由得到的混合高斯模型去描述样本点，得到Fisher Vector表示。
Fisher Vector的计算过程如下：
首先，求解混合高斯模型。
多维高斯分布概率密度函数定义为：
1 r 1 T ,	'
N{x\u^= . exp --（x-//） Z （x一4）	（式 4-1）
42平|	L 2	J
其中，x代表样本，〃代表高斯模型均值，Z为协方差矩阵。
混合高斯模型由多个高斯模型构成，当模型个数趋近与无穷大时，可以拟合任何 连续的概率分布。我们假设有K个高斯模型，特征原维度由D维，其似然函数为：
K
p（x 14,2） = E P（k）p（x I k）
K	（式 4-2）
hl
根据样本点求解该模型，即使对数似然函数最大：
32
logZ（//, S）= log p（xxx2 …xG N ⑺	（式4-3） N	K 1=1	*=1
4。）表示第i个样本是由第k个高斯模型产生的概率，由于存在“和的对数”的形 式，不方便对其直接进行求导，我们利用EM算法求解。
E步：用前一步结果的后验概率形式估计既切：
M步：	叫⑴*=。（后1的） 二。（左）*P（x, l -） - K ... .. >i	（式 4-4） 二兀:N5\n： ~ K j=l 用E步结果最大化似然函数求解参数，即最大似然估计： N 兀k=£叫（式 4-5） i=l 1 N 4 =方2 ?（无比	（式4-6 ） N i=l 1 N	2 。=旷 £ ?（初 W - A-I	（式 4-7 ） M 7=1	2
迭代EM步直至收敛，使用（―，-^，―）的归一化形式表示需要进行编码的样 on dp. da
本簇，即：	尸帅er（X） = （g—）	（式 4-8 ）
其中	g"£= /—Z（秋（M 畋）	（式 4-9） 7九k
33
（式 4-10）
（式 4-11）
在Fisher Vector中，每个样本点被表示为混合高斯模型似然函数在该点对于各个 参数的偏导，对于混合高斯模型系数的关于系数的偏导g.,可以看作是BOW形式, 维度为K维；对于混合高斯模型系数的关于均值的偏导ga,可以看作是VLAD形式, 维度为KxD维；对于混合高斯模型系数的关于标准差的偏导g/维度为KxD维。 所以Fisher Vector总维度为(2K+l)xD,由于BOW形式只有K维，所占特征维度比 例较小，在某些情况下也被省略，即此时Fisher Vector为2KXD维。
4. 1. 3 VLAD 计算
VLAD是基于传统的BOVW的改进，是一种非概率模型下的Fisher Vector,其 示意图如图4-2所示。
图4-2 VLAD编码示意图
VLAD的计算过程如下：
(1)使用k-means聚类训练样本码书。
挑选K个样本作为初始化的聚类中心。
对于每个样本，计算离他最近的聚类中心，并把该样本划分到该类下，并计算各 个样本与其最近的聚类中心的距离和
34
厂这Z（Xj-从，	（式4-12）
7=1 X产， 重新计算每类样本均值作为聚类中心
从=2马	（式4-13）
重复2,3过程直到距离和V不变
（2）对于新的样本点x进行VLAD编码
%=	E（X1-c“）
xsuchthatNN（x）=Cf	（式 4-14）
其中，c,表示第i个聚类中心，为表示样本x的第j维，D为总维度数。 （3）对样本集合｛/）｝的编码结果进行平均
N
%工吧	（式4-15）
n-l
其中N为样本集合数，匕？为第n个样本的VLAD编码结果中第i类下的第j维, 为最终编码结果第i类下的第j维。
（4）对结果进行12归一化
*甘皆	（式4-16）
/=1 7=1 '
VLAD相对于Fisher Vector的变化有如下几点：
（1）在距离的度量上，使用了 L2距离，而Fisher Vector则是以高斯模型产生该 样本点的概率度量样本点到聚类中心的距离，由于VLAD没有使用高斯函数，使得 距离的计算上速度回远高于Fisher Vectoro
（2）在聚类方法上，使用了 k-means聚类，而Fisher Vector使用混合高斯模型 类进行聚类。VLAD划分类别是以最近的聚类中心作为样本点划分依据，其聚类方法 对于球状样本簇的拟合效果较好，而混合高斯模型能够更好的拟合狭长型的样本簇， 这也是混合高斯模型在理论上具有的优势。但混合高斯模型假设之一便是样本簇的产 生符合高斯分布，而在实际应用时往往样本点不一定完全符合这一假设，这使得两者 在聚类方法上互有优劣。
0<3）在特征表达上，Fisher Vector对各个高斯模型系数的偏导由于所占维度少，
35
对于距离的度量几乎没有贡献，主要的信息集中于对高斯模型均值的偏导和对高斯模 型标准差的偏导，其中对于均值的偏导可以表示为样本点与各个聚类中心差值按照到 他们的距离作为系数进行分配的结果，而对标准差的偏导并没有特别直观的解释。而 VLAD的特征表达则要简洁的多，只向其最近的聚类中心分配样本点与其的差值，这 样形成了更为稀疏的表达，且特征维度仅有Fisher Vector的一半。
4.	1.4 k近邻软分配VLAD
为了增大传统VLAD对码数大小选择的鲁棒性，不在采用选择最近聚类中心进 行分配的形式，而采用了k近邻软分配的形式，即挑选最近k个聚类中心按距离大小 为权重比例分配其与聚类中心的差值，可以表示为：
K D
”	(式 4-17)
i J
其中％为软分配权重，可以表示为
一：堂(二名♦"三多)),,如《尤,cJ w topk\dist(x, j )}
a, = "允exp(- ydist(x,Cj))	(式 4-18)
0	, dist(x, q )任 topk [dist(x, j )}
其中y为可调软分配系数
dist(x, q ) = £(X/ - c” 丫	(式 4-19 )
J
4.1.5最大主成分分析(PCA)
最大主成分分析广泛地被应用于数据降维当中，是一种常用的数据处理技巧，其 求解过程可表述为：
将输入的样本矩阵记作X,将投影后的矩阵记作工它们之间存在着线性关系
Yr = A{X - E(X))「	(式 4-20 )
将矩阵y和矩阵X写成行向量形式y=(%y2 ... yn), X = & x2 ... x„)o 我们对矩阵Y和矩阵A给定以下约束，若能满足则称Y为主成分矩阵：
(1)加为，乃，…,北之间正交，为表示矩阵V的第j列向量；
(2)。(M)>£>(%)>/)(%)>...>。(券)，。(月)表示第j列向量的方差；
36
(3)
其中(1) (2)保证了矩阵y中每一个靠前的维度的方差都尽可能大，(3)保 证了从矩阵X投影到矩阵Y时其方差总和不变。
现利用给定的上述三个约束求解矩阵幺，只要得到矩阵/之后我们就可以将任意 一个样本X'投影到尸。
=E(aj}x} + aJ2x2 +...+ajnxn)-ajXE{xx) + aj2E(x2)+... + ajnE(xn)(式 4-21) =0
则由匕构成的矩阵y的均值E(y)则为一个零向量。
故有F的协方差矩阵
5(必)	、
CovY = (Y- E(Y)y (Y ~ £(K)) = YtY =	"%)	(式 4-22)
、	59
上述协方差矩阵中，由于V的列向量之间相互正交，所以非对角线上的元素均为 Oo
我们设乙=。(匕)，又有yr = 2(x-E(X))"于是得到
N 、
A(X - E(X)y(X - E(X))T =	&	(式 4-23)
将/写成列项链形式2=勺，有
3、
02(X-£(X))，(X_E(X)) =
37
%(X-E(X)y(X-E(X)” (a}\ %(X-E(X)) _ %曾
⑸)t(X-E(X))
=>	at(X - E(X)),(X - E(X)) = a 内
= CovXa：=九jCij
(式 4-24)
可以看出乙是X的协方差矩阵的特征值，勺，是特征向量。这样我们就求解出了
矩阵/=
利用公式尸=/（X- E（X）y将X投影到新的维度下的矩阵Y , Y即我们想要求 得的主成分矩阵。
利用主成分矩阵降维的过程：
（1）计算样本牙=&,々,…,苍）（样本数为m且小于n）的协方差矩阵CovX；
（2）计算协方差矩阵CovX的特征值和特征向量
（3）按特征值从大到小的顺序将对应的特征向量排列成一个方阵4= "：即
为投影矩阵；
（4）利用公式Yr= A（X-E（X）y投影得到主成分矩阵Y ；
（5）若需要降至p维，取主成分矩阵/的前p （p＜n）维即为降维后的结果。
在降维时，也可在步骤（3）中只取前p个特征向量来构成px〃维矩阵，然后用 公式X=N（X-EG））,即得到p维主成分矩阵L 该结果与＜5＞中一致，且可节省一 部分的计算。
4.1.6归一化
在特征进行PCA降维之前需要进行零均值单位方差归一化，此步骤防止了特征 不同维度尺度不统一导致部分维度对于主成分贡献过大或过小的问题，使PCA在降 维时能够更好地去除相关性。
零均值单位方差归一化可以表述为：
假设有m个样本，每个样本表示成一个n维向量
X'z = 1,2,3,...,»»	（式 4-25）
样本均值：
1 m
j = 1,2,3,-,« （也将勺记作E（x0）	（式 4-26）
m a
样本标准差：
力■	一勺）2	j= 123,…,«	（式 4-27）
样本方差：
Dj = j = 1,2,3,..., n	（式 4-28）
归一化样本：
X；=立以 j = 1,2,3,..., n	（式 4-29）
7	cr
VLAD的归一化形式有三种，平方根归一化,Intra归一化和L2归一化。 平方根归一化：
\j = $3（%（式 4-30） intra 归一化：
39
（式 4-31）
°	，匕,尸0
L2归一化:
（式 4-32）
其中最近邻VLAD一般采用平方根归一化和L2归一化，k近邻软分配VLAD 一般采用平方根归一化,Intra归一化和L2归一化。
4.1.7	基于VLAD的多媒体事件的时空表示
本文利用已提取的视频帧级别特征和VLAD编码来进行对视频多媒体事件的时 空表示，其过程如下：
(1)从训练视频帧特征中随机抽取一定比例样本，称为采样样本
(2)对采样样本的数据进行零均值单位方差归一化
(3)用采样样本的归一化结果计算PCA投影矩阵
(4)用PCA投影矩阵计算采样样本进行降维
(5)用采样样本PCA结果进行k-means聚类，即训练码书
(6)存储采样样本均值和方差，PCA投影矩阵，k-means聚类中心
(7)对任意新的视频帧特征，对其用己存储的样本均值和方差、PCA投影矩阵 进行零均值单位方差归一化和PCA降维，对降维结果用存储的k-means聚类中心进 行VLAD编码和平方根归一化、Intra归一化、L2归一化。
本文将第(7)步的结果作为多媒体事件的时空表示。
基于VLAD的多媒体事件的时空表示具有良好的可解释性。视频的帧级别特征 特征通过VLAD编码被投影到了不同的聚类类别下，我们把聚类得到的每个类别看 作一种属性，VLAD向量中不为零的部分首先反映了这幅图具有哪些属性，同时投影 的特征点的信息被以计算与其最近聚类中心的差值的方式保存了下来，那么两个 VLAD向量在进行距离的比较时每个类别下的差值信息均会被进行比较，这代表了在 同一个属性下的差异也会被VLAD向量反映出来。如图4-3所示，“猫”和“狗”被 投影到同一个聚类中心下，因为它们可能都属于两幅图中“动物”的属性，而背景则 被投影到另外的聚类中心下尸红色的箭头代表了一个视频中“猫”的投影情况，绿色
的箭头代表了另一个中“狗”的投影情况，两个视频进行比较时，“猫”和“狗”的 差异也将被图中红色箭头与绿色箭头的夹角所反映。
图4-3 VLAD原理示意图
使用VLAD对视频的帧级别特征进行编码代表了对视频属性的统计，并把细节 信息保留了下来，形成了视频级的特征。在多媒体事件检测中，这些属性被用以区分 发生的事件，不同的事件往往具有的视觉属性也不同，所以基于VLAD的多媒体事 件的时空表示能够很好地被应用于多媒体事件检测任务中。相比较于BOVW只统计 属性出现频率的方法，VLAD拥有理论上的优势。
4.1.8	帧级别特征预处理实验
1	.实验设置
本文中比较了对局部特征不同的处理方式带来的准确率上的差异。
视频帧采样频率均为每5帧取一帧，每帧按照crop方式输入GoogleNetl2988c 网络，在最后一个卷积层均按照密集局部特征提取的方式提取了 1 X 1,2X2,3义3三种 尺度特征图的响应。
CNN特征采用了四种处理方式进行对比实验，分别为：
(1)直接进行PCA；
(	2) PCA后使用零均值单位方差归一化，即白PCA；
(3)零均值单位方差归一化后进行PCA；
(4)零均值单位方差归一化后再经过12归一化最后进行PCA降维。
在进行实验时，将MED15的训练集分成了.小规模的训练集和验证集进行了实验。 数据集与前述CNN特征提取方式的实验一致，采用800个样本作为训练集、7230个
41
样本作为验证集的方式进行实验，在此基础上得到的不同的帧级别特征处理方式在验 证集上的mAP如表4-1所示。
表4-1帧级别特征预处理实验结果
方法	mAP
PCA	0.504
PCA+零均值单位方差归一化	0.476
零均值单位方差归一化+PCA	0.504
零均值单位方差归一化+12归一化+PCA	0.483
2.实验结果分析：
从上述结果可以看出，方案（1）和（3）可以达到最高的准确率。考虑以后的实 验中会出现有多种特征级联的情况，那么就需要先经过零均值单位方差归一化将多种 特征归一化到相同尺度上，考虑到算法鲁棒性，本文认为（3）方案为最优方案。
4.1.9	k近邻软分配与原始VLAD对比实验
1	.实验设置
本文就软分配方法与原始vlad进行了对比。实验使用GoogleNetl2988c为基本模 型，使用了密集局部特征的方式提取了帧级别特征，之后随机采样训练样本中约25 万条特征，用PCA将维度降低至256维，使用5近邻软分配vlad进行特征融合，软 分配系数凭经验给出，本文采用的其计算公式为
软分配系数=2.5+PCA投影时前256个特征值之和
其中PCA投影时前256个特征值之和可等价于vlad输入特征各维方差之和。在 使用5近邻软分配时依次使用了平方根归一化、intra归一化、12归一化，而原始VLAD 默认只使用平方根归一化和12归一化。之后使用支持向量机进行类别预测，在此基 础上得到的5近邻软分配vlad与原始vlad在SMALL_10EX数据集上的mAP如表4-2 所示。
表4-2 k近邻软分配与原始vlad对比实验结果
方法	mAP
原始	0.489
5近邻软分配+3种归一化	0.504
2	.实验结果分析
42
由上述实验结果可知，5近邻软分配的方法略微高出原始方法。从原理上考虑， 本文也认为k近邻软分配的形式相比于最近邻的形式更加鲁棒，所以本文最终将5 近邻软分配的方法作为VLAD的基本形式，在其他涉及VLAD的实验中若无特殊说 明，均使用5近邻软分配的形式。
4.1.10	聚类中心数对比实验
1	.实验设置
本文就VLAD不同的聚类中心个数对事件检测结果的影响进行了测试，其在 SMALL_10EX数据集上的mAP如表4-3所示。
表4-3聚类中心数对比实验结果
VLAD聚类中心个数	mAP
64	0.425
128	0.491
256	0.504
512	0.506
2	.实验结果分析
由上述结果可以看出，聚类中心个数在64至512范围内，事件检测准确率随着 VLAD聚类中心数增加而提高，这是因为VLAD依赖于稀疏的结构，在聚类中心数 过少时，不相似的特征也将被投影于相同的聚类中心当中，平均操作丢失了它们的信 息，从而导致VLAD特征表达能力减弱。但聚类中心数从256增加至512时，准确 率已无较大的提升，而特征维度增加了一倍。所以本文认为以256作为VLAD聚类 中心个数最为合适，在本文其他实验中，若无特殊说明，均默认VLAD的聚类中心 个数为256o
4.1.	11 VLAD 与 F isher Vector 的对比实验
1	.实验设置
本文对比了 CNN特征使用VLAD与Fisher Vector两种多媒体事件的时空表示方 法的准确率。本文将VGG16作为特征提取模型，在最后一个卷积层均按照密集局部 特征提取的方式提取1 X 1,2X2,3 X3,6X6四种尺度特征图的响应，其中VLAD采用 默认设置，而Fisher Vector使用了与VLAD相同的取样样本数、聚类中心数，使用 了平方根归一化和12归一化。两种多媒体事件的时空表示方法均采用TRECVID MED
43
竞赛中10EX评测标准计算准确率，其训练集总共5030个视频，包含20个事件，每 个事件10个正样本，其余为背景样本；测试集约30000样本。两种方法在 MED14J0EX网上的mAP如表4-4所示：
表4-4 VLAD与Fisher Vector的对比实验结果
融合方法	mAP
VLAD	0.232
Fisher Vector	0.218
2	.实验结果分析
由上述结果可知，VLAD相比于Fisher Vector在准确率上有着较小的优势，但由 于Fisher Vector在相同的聚类中心下维度是VLAD两倍，并且由于使用了高斯函数, 所以速度会显著慢于VLAD,本文认为VLAD相比于Fisher Vector更适合用于CNN 特征的融合上。
而对于文献［13］中提到Fisher Vector与VLAD相比在IDT、SIFT特征融合上拥有 更高的准确率，本文认为局部特征的性质导致了 VLAD与Fisher Vector的结果差异， Fisher Vector尽管在理论上有着更加完备的优势，但需要注意的是Fisher Vector有着 样本满足高斯分布的假设，IDT与SIFT中的局部特征多数来自于直方图的统计，这 与高斯分布是比较相符的。而本文CNN局部特征可看做是高度抽象的视觉信息，是 取自Max Pooling的结果，这种特征形式是否与高斯分布相符有待论证，本文不做赘 述，但就本文实验结果和考虑效率而言VLAD相比于Fisher Vector更加适合。
4. 2长短时间记忆单元(LSTM)
4. 2.1循环神经网络(RNN)模型
RNN节点就是与普通的神经元节点之间的区别就是它多了一层链接同层的节点 到自身节点的权重，每个RNN节点能够接收到上一时刻的传递过来的信息，这其实 就是人为地添加了一个先验信息：认为RNN单元前一时刻的状态对后一时刻的状态 有直接影响，在这点上与马尔可夫模型【的有相似性。
44
Output Layer
Hidden Layer
Input Layer
图4-4 RNN结构示意图[34]
图4-4所示就是就是一个标准的单隐层的RNN网络，从下面的前向传播公式就 可以看出RNN节点在t时刻点的值来源于两个部分，一个是input layer的输出，另 一个就是hidden layer自身t-1时刻的值。
% =6 % + U也-+ 4）	（式 4-33）
弘=6,忆九+九）
（式 4-34）
其中为是时刻t的输入向量，々是时刻t隐层状态，乂是时刻t的输出向量，匕和 bh是输入层与隐层间的权重矩阵和偏执，Wy和力是隐层与输出层间的权重矩阵和偏 执，“是隐层上一时刻向下一时刻映射的权重矩阵，叫是激活函数，一般使用Sigmoid 函数或ReLU函数。
图4-5是RNN 一个前向传播的示意图，1-7表示的是不同的时刻，时刻越远的 节点用越黑的圆来表示：
输出层
隐层
输入层
2	3	4	5	6	7
图4-5 RNN前向传播示意图
我们可以看到，在时刻7,隐层实际上接受了从1到7时刻所有传过来的信息, 并且这个过程中权重矩阵4是保持不变的。	° '
45
但是这也就出现了一个问题，由于4不变的，我们假设激活函数叫是ReLU, 就可以发现前面某一个时刻传递过来的信息被表示成&的连续乘积的形式。这意味 着权重矩阵中任意一个权重的幕次都会随着时间不断增加，该权重任意作用于一个输 入节点之上会使该节点传递的信息要么为零，要么很大，出现梯度消失和爆炸的情况。 如果我们把外换为Sigmoid函数，虽然能够控制梯度爆炸，但是还是无法解决梯度 消失的问题。
所以通常RNN在较长序列预测上会出现衰退或者震荡的情况。随着时间对自身 权重的不断累积，也就是为什么简单的RNN网络不能处理长序列的原因。
1）长短时序记忆（LSTM）同单元
如图4-6所示，长短时序记忆单元是一种对RNN单元的改进结构，他引入了三 个门：输入门/\输出门遗忘门每个门输出是一个近似二值化的形式， 起到了一种类似电路开关的效果，它根据当前的输入和上一时刻的输出决定自身的输 出，当门输出接近于零时意味着门被关闭，，接近于1的时候意味着门被开启。
图4-6 LSTM单元结构【】
输入门控制着信息是否向LSTM单元的输入，输出门控制着LSTM的信息是否 向外的输出。s,为LSTM的记忆单元，状态会随着输入不断累积。而为了避免LSTM 记忆单元的输出趋于饱和，遗忘门被引入进来，它在适当的时刻会使记忆单元清空， 控制着记忆单元是否遗忘信息。因此LSTM单元相比于RNN单元拥有更加完善的信 息记忆机制，不会因为序列过长而发生梯度爆炸或消失，在其门的结构控制下能够实
46
现对数据更加有选择性地学习。
LSTM的前向传播公式。
输入门：
1	H	C
a； =£+ £w" 丁	（式 4-35）
21	h=l	e=l
b：=_f（a；）	（式 4-36）
输出门：
1H	C
% = Z *刷 + E whe）b'~l + Z “广	（式 4-37）
7	A=1	c=l
%=/«）	（式 4-38）
遗忘门：
]	H	C
W = E Wj# + £ 叫*7 + £ W稣S；1	（式 4-39）
i=l	h=\	c=l
4=/0）	（式 4-40）
记忆单元：
I	H
4=2 w遥'+ X W加娟	（式 4-41）
/=1	/?=1
S：=哈丁 + b；g（4）	（式 4-42 ）
网络输出
其=叫〃区）	（式4-43）
其中，
/W = -^—T	（式 4-44）
1	+ e
47
於）=春-2
2
3）=4尸1
（式 4-45）
（式 4-46）
4.	2.2基于LSTM的多媒体事件的时空表示
我们用LSTM单元替换RNN单元可以搭建一个简单的LSTM网络。该网络由一 个输入层、一个输出层和一个隐层组成，LSTM单元构成了隐层的神经元节点，而输 入层节点到隐层节点和隐层节点到输出层节点之间以全链接形式完成。
本文按照图4-7所示的方式来训练LSTM网络，一个视频的帧级别特征按时间顺 序作为一个输入的mini-batch,在训练时，对于同一个mini-batch下不同样本的输出， 用视频的标签作为其每帧监督信息进行训练；在测试时，对于同一个mini-batch下不 同样本的输出求平均，作为这个视频的预测标签。在每个视频序列输入完成后，将每 个LSTM单元的记忆单元清空，保证每个视频的特征序列在训练时LSTM单元接收 到的信息没有来自其他视频的。
图4-7 LSTM网络训练示意图
LSTM网络在训练时一般使用固定大小的Mini-batch进行训练，对于视频采样的 帧数大于Mini-batch大小的情况，本文采用居中截取一段视频帧的方法；对于视频采
48
样的帧数小于Mini-batch大小的情况，本文采用将视频从头开始重复取样直至补齐的 方式。
本文将视频的帧级别特征序列输入后LSTM单元的状态作为多媒体事件的时空 表示。由于LSTM网络中已包含了对事件的分类器，在进行多媒体事件检测时，本 文直接采用将网络输出层的概率分布平均的方式作为发生相应事件的置信度。
4. 2. 3 LSTM与VLAD对比实验
L实验设置
本文就LSTM网络预测结果与基于VLAD的多媒体事件的时空表示、SVM进行 类别预测的结果进行了对比。
使用LSTM网络进行预测时，使用了 GoogleNetl2988c提取全局特征，取样间隔 为每25帧一次，预测网络为一个单隐层的网络，输入层接受全局特征的输入，隐层 为512个LSTM单元，输出层为21个节点的全连接层，对应着20个事件和背景样 本。网络Mini-Batch大小为120,即接受长度为120的序列输入，若视频帧长度不够 120则从头开始进行重复取样直至补齐120帧，若长度超出120则截取中间部分。预 测时对输出置信度进行平均操作。本文也有对隐层层数和节点个数做一定的调整，但 均未得到更好的结果。
使用VLAD和SVM时，按默认方法进行。
在进行实验时，将MED15的训练集分成了小规模的训练集和测试集进行了实验：
数据集包含20个事件和背景样本，总共8030个视频样本，每类事件有100个正 样本。
训练集：包含7200个视频，每类事件有90个正样本；
测试集：包含800个视频，每类事件有10个正样本。
在之后的实验中我们将此数据集命名为SMALL_90EX,考虑到有监督的特征融 合方法对样本的需求，我们使用数据集中较大的一部分作训练集，较小的一部分作测 试集。
两种方法在SMALL_90EX数据集上的mAP如表4-5所示。
表4-5 LSTM与VLAD对比实验结果
方法	mAP
VLAD+SVM	0.640
LSTM	0.382
49
2.实验结果分析
有上述结果可知，LSTM方法并没有达到一个较好的结果，其最准确率相差 VLAD方法较远。
4. 2. 4 TRECVID2015 MED 竞赛结果
1.实验设置
LSTM方法参加TRECVID2015 MED竞赛，使用了 AlexNet和GoogleNet模型做 特征提取，用LSTM方法进行了融合，对置信度进行平均作为事件预测的置信度。 MED15竞赛结果如表4-6所示，结果采用infAP200度量。
表 4-6 TRECVID2015 MED 竞赛结果
任务	10EX	100EX
本文LSTM方法的结果	0.087	0.155
其他队最好的结果	0.303	0.365
2.实验结果分析
本文认为，利用LSTM方法进行多媒体事件的时空表示结果较差的原因主要有 以下几点：
(1)从结构上来看，LSTM尽管考虑了时序关系，但网络输出依然与当前输入 相关性最大,LSTM的引入并没有解决从“多点输入”到“单点输出”这种多对一的问题, 缺乏有效的整合数据的能力，其效果实际上是直接求平均的一种改良版。
(2)从机制上来看，LSTM用于对帧级别特征的融合，但由于提取的特征已是 高层的抽象语义信息，缺少了较多的空间信息，LSTM网络从中学习出的时序关系已 无法再反映出物体的运动趋势，所以以本文使用LSTM多媒体事件的时空表示的方 式从理论上也缺乏一定有效性。
(3)从数据上来看，事件检测本身过于复杂，数据质量过于参差不齐，LSTM 很难从中学习出有效的时序信息，而MED竞赛本身提供了较少的视频数据，可能使 LSTM网络的训练陷入了过拟合。
4.3 netVLAD
netVLAD⑼是结合了深度学习和VLAD思想提出的一种新算法。原始的VLAD
50
编码是利用生成式模型由无监督的方法进行学习，而实际在分类任务上，直接采用判 别式模型往往会更加有效。深度学习在图像、文本、音频等领域的成功很大程度上也 受益于判别式模型，利用深层的网络结构实现了强大非线性拟合能力，利用端到端的 训练直接优化模型，避免了像传统多步骤的模式损失信息。在使用VLAD进行视频 事件分类时，一般需要结合SVM,而我们希望利用深度学习框架，搭建一个直接从 视频原始图像输入到视频类别端到端的框架，然而由于当前设备限制，本文提出的 netVLAD方法仅在从视频帧级别特征到视频类别上做端到端的构建和实验。
4.	3.1 netVLAD公式推导
我们考虑VLAD 一种更为普适的形式，用4（用）表示软分配的系数，若为最近邻 分配，则凡⑷趋近于无穷大。VLAD的形式可以表达为以下形式：
V（j,k） = E殁—q（/））	（式 4-47）
7=1
必（毛）=--；~~~p	（式 4-48 ）
我们发现软分配系数q（x,）中的关于X,的二次项形式都可以约掉，我们用队和 瓦分别表示项的一次向和常数项系数，则可以得到如下形式
= 2ack	（式 4-49）
4=-司4『	（式4-50）
F（j，k）这、心法「色⑺-，（加	（式4-51）
我们可以发现，软分配系数变成了一个线性变换和softmax层组合的形式，由此 我们可以构建netVLAD的基本结构，如图4-8所示。
51
图 4-8 netVLAD 结构
由于对每个局部描述子进行相同的线性变换，则实际进行的是1 X 1的卷积操作, 我们可以由1 X1的卷积层和softmax层组合得到netVLAD的软分配系数,VLAD core 进行的是苍-Q的操作，勺表示的是第k个聚类中心。
此外，由于视频长度的不确定性，为了消除长度不同的视频在网络各层中输出量 级的不一致性，本文在实现netVLAD表示时采用“平均”而非“加和”的形式，即使用 式4-52公式代替式4-51：
1 N
Vkj,彷=苗 %	⑺-Q （力）	（式 4-52）
4. 3.2参数学习
q可以由此和”表示出，但由于表达形式中含有相除的形式，在进行链式法则 求导进行梯度计算时会产生较为复杂的表达式，一方面减慢了反向传播的速度，另一 方面更容易引起梯度爆炸，所以我们采用其他的方式来对,进行表示，一般有两种思 路：
第一种思路，对样本的投影到某个聚类中心簇下的特征进行均值统计，由于投影 的映射关系会随着训练不断改变（即队和'的更新），需要随着迭代缓慢更新这个 均值，使得短时间内的迭代均值近似不变，但最终会趋近与聚类中心，这种思路可参 考 Batch Normalization 的实现方法；
第二种思路，将Q作为一组参数进行学习，这种方式相比于原始VLAD更能够 利用有监督学习的优势，而学出的&不一定对应着原来的聚类中心，而是通过最小化 损失函数得到的一个点，但这个点既然能够使损失函数更小，我们也认为相比于原始 聚类中心，这个点能够使VLAD向量更好地表达。
本文采用了第二种思路构建了 netVLAD结构。
52
4. 3.3基于netVLAD的多媒体事件的时空表示
图4-9为本文所用netVLAD网络结构，帧级别特征作为输入先经过1 X1卷积降 维再经过netVLAD编码得出视频特征，最后通过全连接层和损失层构成的线性分类 器进行类别预测。
*4-	..
二
■	■	守
1	归一化
；	VLAD core
| ；--------：
f softmax
* :_ ＞；/；,xn :	net
I 1X1卷积
图4-9 netVLAD网络结构
本文将netVLAD结构的输出作为多媒体事件的时空表示，与VLAD类似，它以 一个高维向量的形式表示视频信息。在进行多媒体事件检测时，全连接层的输出可作 为发生相应事件的置信度直接用于事件检测。
4.	3.4归一化层对比实验
1	.实验设置
由于在原始的VLAD当中使用了三种归一化，本文认为在netVLAD当中使用类 似的机制可提升网络性能，本文就不同的归一化层进行了实验。需注意的是，该实验 中的归一化层并非对提取出的特征做后处理，而是在网络训练过程中引入归一化操 作，这些归一化操作对网络反向传播也产生了影响。
本文基本网络输入为由GoogleNetl2988c提取的视频密集局部特征，网络采用通 道数为256、尺寸为1X1的卷积核进行降维。然后使用通道数为256、尺寸为1X1 的卷积层接上softmax层进行软分配系数的计算，软分配系数和降维结果作为VLAD core的输入进行编码，其中VLAD core中的参数可由0进行初始化或用随机值迸行
53
初始化，本文在实验没有发现VLAD core初始化对结果造成的明显影响。VLAD core 的输出使用归一化层进行归一化，然后通过通道数为21的全连接层和softmax层进 行训练和分类。
本文比较了了 12归一化和intra归一化对事件分类准确率的影响，没有进行平方 根归一化的相关实验。本文认为在神经网络中引入平方根归一化层从原理上考虑是没 有效果的。
由于归一化层的引入，改变了 netVLAD的特征输出响应尺度，本文为了使网络 能够用随机梯度下降法进行正常的优化，采用了 Power层使归一化后的输出乘以约 10倍将输出尺度还原到与未进行归一化时对等的程度，该操作也可由加大归一化层 后一层的学习率和降低正则项系数代替，缺少尺度还原的操作将使网络在添加归一化 层之后难以优化。
网络由softmax损失进行训练，各个类别的置信度由softmax层的响应得出，由 此得到不同的归一化层在SMALL_90EX数据集上的mAP如表4-7所示。
表4-7归一化层对比实验结果
方法	mAP
无	0.402
12	0.498
Intra	0.433
2	.实验结果分析
由上述结果可知，12归一化为netVLAD的最佳归一化方案。
4.	3.5损失函数对比实验
1	.实验设置
本文比较了不同的损失函数对netVLAD方法结果的影响。netVLAD网络采用了 12归一化。
在进行Softmax loss训练时，采用与上述归一化实验中相同的方式。
在采用Hinge loss进行实验时使用L1范数作为损失衡量方式,这与线性SVM的 优化公式是一致的，并且采用Sigmoid函数来代替Softmax进行类别预测，因为 Sigmoid相比于Softmax去除了置信度上的相关性，本文在用于Sigmoid进行预测时 的mAP会比使用Softmax时高出约1%。
在采用Triplet loss进行实验时，去掉用于分类的全连接层，Triplet loss直接作用
54
于VLADcore的输出，样本以五元组形式作为输入，包括两个来自相同类别的视频、 一个不同类别视频、两个背景样本，每个五元组将在损失层形成三个由三元组计算而 来的损失，这三个损失的平均作为网络的输出和梯度计算的基本单元，在网络训练完 成后，将每个视频的局部特征用netVLAD网络提取视频级特征，然后训练SVM分 类器，对测试视频进行类别预测。
三种损失函数在SMALL_90EX数据集上的mAP如表4-8所示。
表4-8损失函数对比实验结果
方法	mAP
Softmax loss	0.498
Hinge loss	0.525
Triplet loss	0.382
2	.实验结果分析
由上述实验中，Hinge Loss为训练netVLAD网络的最佳方案，其原理可等价于 线性SVM的求解过程，只不过优化方式使用随机梯度下降法实现而并非SMO算法。 Hinge Loss只优化分类边界而并非像Softmax Loss依据样本全体的置信度，在某些特 定的任务中有着优势，这也与VLAD在使用SVM进行类别预测得到的高准确率是一 致的。
而Triplet loss适合于特征学习，但由于事件检测目标是给出相应的视频在各个事 件上的置信度，需要用额外的分类器进行类别预测，而分类器的训练和Triplet loss 训练均是有监督的训练，若使用同一批训练样本则会使netVLAD网络提取出的特征 会向训练集偏斜，使得最终准确率降低，而不使用同一批样本意味着需要将训练样本 进行切分，变向降低了训练样本数量，也会引起最终的准确率降低。所以TripletLoss 从原理出发不会得到较好的结果，而实验结果也证实了这一点。
4.	3.6 netVLAD 与 VLAD 对比实验
1	.实验设置
本文比较了 netVLAD与VLAD方法的结果。两者均使用GoogleNetl2988c网络 提取的密集局部特征作为输入,netVLAD使用12归一化和Hinge Loss进行训练,VLAD 在进行特征融合后使用SVM进行类别预测，两者在SMALL_90EX上的mAP如下表 4-9所示。
表4-9损失函数对比实验结果
55
表4-9损失函数对比实验结果
方法	mAP
netVLAD	0.525
VLAD	0.640
2	.实验结果分析
由上述实验结果可知，netVLAD相比于VLAD方法在准确率上还有一定差距。
尽管netVLAD在原理上与VLAD有着共通的部分，但是netVLAD为了以深度 学习的框架进行优化，放弃了 VLAD中一些可能有效先验知识，例如VLAD core的 参数选取中，VLAD是根据信息量最大化原则人为选取了聚类中心，而netVLAD中 己不存在聚类这个概念，而是直接利用反向传播直接对参数进行优化;此外,netVLAD 也放弃了 VLAD中分配系数的选取方式，其分配系数由一个线性变换进行控制，其 参数优化与上述VLAD core参数的优化同时进行，如果再考虑其他例如利用神经网 络进行降维、分类的参数，这就使得优化过程变得更加不确定。尽管聚类中心的选取 只是参数的一组特殊的取值，理论上通过最小化损失得到的全局最优解会优于人为指 定的聚类中心，但随机梯度下降不一定总能得到全局最优解，它受限于初始化条件等 诸多因素，同时参数的增多也更容易是模型因为样本不足而导致过拟合。
4.4时域卷积
相比于LSTM和netVLAD,时域卷积的结构更为简单。传统的卷积核是在空间域 上进行二维的卷积实现逐层抽象，而时域卷积则是利用相同的思想在时间域上进行一 维的卷积，以此实现对时序信息的整合。时域卷积理论上同样可实现视频数据端到端 的训练，但本文仅仅用其实现特征融合。
4. 4.1原理介绍
时域卷积核可表示为大小为NxlxTxC的Blob,由于视频帧的全局特征不在具有 空间尺度特性而仅仅具有时间尺度特性，而特征维度为C维，用lxTxC的窗口按时 间轴滑动即可抽象出一个新的特征，与传统空间卷积相似，我们也用多个滤波器来进 行概念的抽象，即下一层特征维度为N维。时域的下采样与此类似，基本的网络结 构如图4-10所示。
56
图4-10时域卷积网络结构
时域卷积相比于本文其他多媒体事件的时空表示方法从理论上上而言更加适合 处理视频序列。相比于netVLAD和VLAD方法，时域卷积能够考虑到视频帧之间的 时序关系，但该关系被限制在卷积核的感受野之内，避免了像LSTM对长时间输入 序列的记忆引起过拟合的现象；同时，层级的结构能够使不同的层能抽象出不同长度 的时序关系，这使得网络有较强的表示能力。
相比早期卷积神经网络普遍使用的空间尺度较大的卷积核，最新的Inception结 构网络中更偏向于采用多层小的卷积核堆叠的形式。例如5x5的卷积核被拆分成两个 3X3的卷积核堆叠的形式，既保证了感受野不变，又减少了参数数量，还同时加深 了深度。而在一些Inception结构网络的中间层中，又把3X3的卷积核拆分成了 3X1 和1X3的卷积核堆叠的形式，参数变得更少同时深度也进一步加深了。
由此受到启发，我们可以结合空间域卷积和时域卷积代替3D卷积核来进行人物 动作信息的抽象，以此搭建的网络将会包含空间域卷积、时域卷积、空间域下采样、 时域下采样的交替使用，这将会使网络的形式更加多变和更大的提升空间，适当的技 巧和策略将会大幅度提升网络的准确率。但由于时间和条件限制本文对此的探索尚有 不足，仅仅利用时域卷积在多媒体事件的时空表示上进行小规模实验。
4. 4. 2基于时域卷积的多媒体事件的时空表示
时域卷积网络接受帧级别特征作为输入，使用“下采样、卷积、卷积”交替的形式 进行特征抽象，每次基本的概念抽象包括：首先利用了一个尺寸为5、步幅为3的 Max Pooling进行下采样，该层得到的输出再经过两个连续的尺寸为3、步幅为1的 卷积层进行信息的整合。
除了前两个卷积层的通道数固定为512之外，每次下采样之后的卷积层的特征图 通道数进行翻倍。这种交替的结构在重复三次之后使特征维度达到2048维，序列长 度缩短为原来的1/27,再此之后添加一个高度为1的SPP层对特征进行平均操作， 得到一个2048维向量。最后添上全连接层和softmax损失层进行训练和类别预测。
57
时域卷积网络在第一层使用下采样层而非卷积层的原因是为了避免网络对于时 序关系过多的拟合，下采样结果可以看作将短时间内的连续帧的特征融合成小的视频 段的特征。
本文将时域卷积网络的Average Pooling层的输出作为多媒体事件的时空表示， 相比于基于VLAD和netVLAD的多媒体事件时空表示，其特征维度要小得多。在进 行多媒体事件检测时，采用softmax的输出作为相应事件的置信度。
4.	4.3时域卷积与其他方法对比实验
1	.实验设置
本文进行时域卷积进行多媒体事件的时空表示的相关实验。
在使用时域卷积方法时，采用了 GoogleNetl2988c网络提取帧级别特征，该特征 由CNN网络的最后一个卷积层使用尺寸为7x7的Map Pooling得到，即每帧提取一 个全局的特征，之所以使用Max Pooling而不是Ave Pooling是由于Max Pooling更能 够更好地保留局部响应，这与传统的CNN的逐层抽象过程中使用的Max Pooling作 为下采样层是一致的。
基于时域卷积的多媒体事件的时空表示对比方法在SMALL_90EX数据集上 mAP如表4-10所示。
表4T0时域卷积实验结果
方法	mAP
VLAD	0.640
LSTM	0.382
netVLAD	0.525
时域卷积	0.565
2	.实验结果分析
由上述实验可知，时域卷积是三种基于深度学习的多媒体事件的时空表示方法中 准确率最高的，虽然与VLAD方法相比尚有不足，但本文认为时域卷积方法拥有更 大的理论优势，但受限与三方面的原因：
（1）在本文的实验中并没有使用端到端的框架。时域卷积网络仅仅被用来处理 帧级别特征，而多数深度学习方法都受益于端到端的训练。
（2）本文时域卷积网络的输入丢弃了空间信息。如果采用具有丰富空间信息的 。.特征作为时域卷积网络输入，可以利用时域卷积和空间域卷积去捕捉动作信息。
(3)训练数据的规模过小。如果训练数据足够，可以使用更复杂的模型去更精 确地拟合视觉概念的抽象过程，但本文的数据规模所限时域卷及网络只能使用很简单 的结构。
59
第五章 基于Li near SVM的多媒体事件分类
5.	1支持向量机
5.	1.1支持向量机求解
支持向量机常用于分类问题当中，其基本形式为根据样本最大化支持平面间隔， 如图5-1所示，若以二维平面作说明，支持向量机的求解即要使图中两条平行虚线间 隔最大，同时通过引入惩罚项允许部分离群点的出现。
,Maximize
图5T支持向量机示意图 其问题可以用以下公式描述：
Minimize
S 工	yi(wX1+6)Nl-£
y； e {-1,1},	i=l,2,...,N
(式 5-1)
其中w为支持向量，b为偏置，C为惩罚项系数，x,为样本，弘为标签。
对于此类问题的求解，可用对偶定理进行消元，再使用SMO[42]算法求解出全局 最优解(w,b)。由于该问题己被研究的较透彻，本文不再作赘述。
5.1.2基于Li near SVM的多媒体事件置信度计算
SVM用于二分类问题进行类别预测时，其样本再的正负性由决策值wx,+6的正 负性决定，而wx+b = O即为决策平面，wx,+6 >0意味着样本x,落在平面左侧，为正 样本，否则为负样本，如图5-2所示。
60
/ wx+b=O
图5-2置信度计算示意图
对于一个样本被判为正样本的置信度，本文认为该样本落在决策平面左边且离决 策平面越远置信度越高。相应的，样本落在决策平面右边且离决策平面越远置信度越 低。本文用有向距离dist表示样本x与决策平面的距离：
dist(x)
wx + b
e(-co, + oo)
(式 5-2)
则dist越大则其作为正样本的置信度越高，我们把dist归一化到(0,1)之间来作为 其样本被判为正样本的置信度：
1
score(x)=
1 + exp(-
1

1 + expJ 与草、
(式 5-3)
w
在统计各样本置信度的mAP时，由于对其产生影响的仅是置信度的相对排序， 而与置信度的绝对大小无关，所依置信度公式可进行进一步简化。
对于同一个SVM分类器中的不同样本，w和b为常数，故最终置信度公式可简 化为：
score(x)=
]
1 + exp(- wx)
(式 5-4)
对于不同事件的预测，本文训练了不同的SVM分类器，每个事件均对应着一个
标准的二分类线性SVM分类器。对于一个测试样本，将由分类器计算得到的score(x) 作为发生相应事件的置信度。
5. 2模型融合
61
5. 2,1多模型融合方法
本文更倾向于单模型带来的简洁、高效的框架，并未在多模型融合上做过多的尝 试。本文仅使用视频级别特征拼接的方式进行多模型融合，即对于测试视频，使用不 同的模型得到不同的视频级别特征，我们将这些向量拼接到一起作为融合后的特征， 即：
a =（W，…M"）	（式	5-5）
b = （b1,b2,...,bm）	（式	5-6）
fusion^,b） = （al,a2,...,a„,bi,b2,...,bm）	（式	5-7）
其中a、b表示两种不同的模型得到的视频级别特征，力sio〃（a,6）表示这两种特征的 拼接，两种以上模型的融合同理。拼接后的特征维度等于原始所有特征维度之和。然 后将拼接后的特征输入SVM进行训练和预测。
该方法在多种特征均较为有效并且互补时能够得到较好的结果，融合后的特征两 两之间的内积将更为稳定，这有利于SVM的训练。但相应的，若多种特征中的一种 与其他特征的效果差距过大，将会对准确率产生副作用。因为本文对各种特征均进行 了单独的测试，对于可区分性较差的特征不予采用。因此后续处理提直接采用了视频 级别特征拼接的方法。
5.2.2多模型融合对比实验
1	.实验设置
为了测试所提方法的有效性，本文比较了不同模型下的结果及融合结果，各方法 在SMALL_10EX上的mAP如表5-1所示。
在实验中，IDT使用Fisher Vector进行特征融合，特征维数为131072,而CNN 模型均使用VLAD进行融合，特征维数为65536o
表57多模型融合对比实验结果
方法	维数	mAP
IDT	131 072	0.385
GoogleNetl 2988c	65536	0.504
62
Resl52	65536	0.498
GoogleNetl2988c+IDT	196608	0.454
GoogleNetl 2988c+Resl 52	131072	0.528
表 5-1 中“GoogleNetl2988c+IDT”表示基于 GoogleNetl2988c 和 IDT 的方法进行 融合的结果，而“GoogleNetl2988c+Resl52”表示基于 GoogleNetl2988c 和 Res 152 的方 法进行融合的结果。
2	.实验结果分析
从表5-1的融合结果我们可以观察至(J, GoogleNetl2988c与Resl52相比IDT为更 有效的模型，他们的模型融合得到了更高的结果。而GoogleNetl2988c与IDT的模型 融合结果相比于GoogleNetl2988c单模型反而出现了下降，这意味着对视频级特征融 入缺乏表达能力的特征将损害事件检测的准确率。本文认为这是由于SVM训练当中， 样本被以两两计算内积的形式作用于支持向量的选取之故，由于IDT的Fisher Vector 特征比CNN的VLAD特征维度高出一倍，使得对于拼接特征的内积前者比后者的贡 献更大，而前者单模型下的低准确率将反映在特征内积的不稳定上，这使得最终模型 融合的结果反而较差。
考虑综合准确率和速度，本文采用“GoogleNetl2988c+Resl52”来作为本文最优多 模型融合方法。
5. 3多媒体事件检测实现
5. 3.1多媒体事件检测流程
本文进行多媒体事件检测流程如图5-1所示。其中红色的虚线表示训练阶段独有 的，普通的实线表示训练阶段和测试阶段共有的。
训练阶段：
(1)训练视频被解码并采样出视频帧；
(2)通过卷积神经网络提取出密集局部特征，即帧级别特征；
(3)先根据训练视频的帧级别特征先进行样本均值、方差统计和PCA投影矩阵 的计算，再使用归一化和PCA降维进行处理；
(4)先根据预处理后的帧级别视频训练VLAD编码所需码书，再进行VLAD编码 和归一化，得到视频级别特征；
(5)用训练视频的特征训练SVM分类器。
63
测试阶段：
(1)测试视频被解码并采样出视频帧；
(2)通过卷积神经网络提取出密集局部特征，即帧级别特征；
(3)对帧级别特征使用归一化和PCA降维进行预处理；
(4)对预处理后的帧级别特征进行VLAD编码和归一化，得到视频级别特征；
(5)对测试视频特征用已训练SVM分类器进行相关事件的类别预测，得到置信 度。
svm模型
图5-1多媒体事件检测框架流程图
频别征 视级特
在训练时，训练集的CNN特征和VLAD特征均会被存储于硬盘上,PCA和VLAD 的训练样本均由自适应的方法进行选取，PCA投影矩阵、VLAD码书、SVM模型被 存储于硬盘上。
在测试时，读取存储于硬盘上的PCA投影矩阵、VLAD码书、SVM模型，然后 对输入视频进行特征提取、特征融合、类别预测，这样只有测试视频的VLAD特征 和类别预测结果存储于硬盘中。如果有多模型融合的需求，存储在硬盘上的多个模型 提取的VLAD特征将直接用于多模型融合，以避免重复计算。
整个框架是一个任务提交式的结构，在指定训练集、测试集、基本参数后，框架
64
的三个部分一一特征提取、特征融合、类别预测均以中间结果的生成完整与否来判断 是否需要继续执行任务（若对应的中间结果已生成完整则会跳过当前部分），而每一 部分将根据指定的训练集和测试集和已存储的中间结果进行剩余任务计算，还需执行 的任务将被根据指定的线程数进行切分并分配到多个线程执行任务。此外，若有个别 视频数据存在的读取失败的情况，框架将会给出警告后并跳过，不影响其他任务的执 行。
5.	3. 2线程调用
本文提出的框架大部分计算均使用了多线程。具体地线程调用说明表如表5-1所
不。
表5-1线程调用说明表
线程分配	模块
单线程	PCA投影矩阵计算
外部多线程	解码、CNN特征提取、PCA降维、VLAD 码书训练、VLAD编码、SVM训练、 类别预测
内部多线程	解码、CNN特征提取、VLAD码书训 练、SVM训练
在本文的线程调用设计上，由于解码、CNN特征提取、PCA降维、VLAD编码、 类别预测的执行对各个视频而言都是独立的，直接使用了 OpenMP进行并行计算，即 外部多线程。其中解码由FFMPEG库提供了对于单个视频的多线程实现，CNN特征 提取依靠Mini-batch的形式也提供了对单个视频的并行方法。对于需要将视频训练集 作为整体输入的部分，无法直接从外部直接用多线程调用，VLAD码书训练由vlfbat 库提供了多线程的支持，对于SVM训练考虑到各个二分类的模型都是独立进行训练 的，也可由OpenMP实现模型间的并行训练。
最终仅有PCA投影矩阵由单线程方式实现。
5.4实验结果及分析
本文实验的硬件配置如表5-2所示。
65
表5-2硬件配置
编号	GPU型号	显存	CPU型号	核心数	内存	硬盘
1	GTX TITAN	12GB	i7-4790K	4	32GB	4T
2	GTX 980	4GB	i7-4790K	4	16GB	2T
5. 4. 1与其他方法的对比实验
为了测试所提方法的性能，本文与目前具有较好性能的方法进行了实验比较，表 5-3列出了 MED14_10EX的mAP结果对比。
表5-3 MED14J0EX数据集上与其他方法的对比实验
方法	mAP
Ours(VGG19)	0.232
Ours(GoogleNet 12988c)	0.279
Rank SVM	<0,1
C3D	<0.1
B-LSA	0.186
IDT+DCC	0.164
IDT+CNN+wSML+SVM	0.258
Joint Event Detection	0.252
表5-3中的Ours(VGG19)为本文提出的框架在使用VGG19提取帧级别特征时的 结果，Ours(GoogleNetl2988c)为本文提出的框架在使用GoogleNetl2988c提取帧级别 特征时的性能。两个CNN模型均采用在最后一个卷积层均按照密集局部特征提取的 方式提取1X1、2X2、3X3和6X6四种尺度特征图的响应作为帧级别特征。
Rank SVM和C3D方法分别为实现了文献［11］的方法和使用［3］的模型在多媒体事 件检测数据集上进行测试的结果；而B-LSA, IDT+DCC, IDT+CNN+wSML+SVM, Joint Event Detection 分别为文献［37］、［38］、［39］和［40］中提及的其在 MED14_10EX 数据集上的结果。
本文的方法与其他论文提及的方法在MED14_100EX的mAP对比如表5-4所示。
表5-4 MED14J00EX数据集上与其他方法的对比实验结果
方法	mAP
66
Ours(GoogleNet12988c)	0.380
Idt(cmu's method)	0.276
Semantic Saliency and Nearly-Isotonic SVM	0.344
其中 Idt(cmu's method)> Semantic Saliency and Nearly-Isotonic SVM 分别为文献 ［2］、［41］中提及的其在MED14_100EX数据集上的结果。
从表5-3和表5-4的对比实验可以看出，本文提出的MED检测方法在准确率上 明显优于现有文献中的其他方法。
5.	4. 2 TRECVID2016 MED 竞赛
1	.实验设置
TREC 视频检索系列测评(TREC Video Retrieval Evaluation, TRECVID)是由美国 国家标准技术协会(National Institute of Standards and Technology, NIST)组织的视频 信息检索领域的国际权威测评，旨在以公平公开的评测方式展现视频检索的最新技术 和进展，从而推动视频检索及相关技术的发展。评测自2001年起每年举行一次，包 括多个视频内容理解任务，极具挑战性，因而吸引了超过100家世界知名大学、研究 所和企业参与。本人所在的实验室参加了 TRECVED2016 MED〔i刀Pre-Specified Events 的评测，评测提交的结果采用了本文所提的方法
参加评测的mAP结果如表5-5所示。
表 5-5 TRECVID2016 MED 竞赛结果
Teams	PS_SUB_10EX	PS_SUB_100EX	Platform
Ours(single model)	0.336	0.469	SML
Ours(combined model)	0354	0.490	SML
Etter	0.014		SML
INF	0.298		SML
ITICERTH	0.318	0.462	SML
KU-ISPL	0.209	0.340	SML
MCIS	0.004	0.004	SML
MediaMill(FullAsSub)	0.354		SML
NIIHitachiUIT	0.007		SML
TokyoTech	0.279	0.415	SML
VIREO	0.335	0.419	0。	MED
67
nttfudan	0.328	0.457	SML
其中“Ours(single model)”为本文单模型方法，即利用GoogleNetl2988c模型进行 特征提取，VLAD进行特征融合，SVM进行类别预测的结果；而“Ours(combined model)”为本文的多模型融合方法，融合了 GoogleNetl2988c和ResNetl52两种模型。 “Platform”表示测试平台占用资源规模大小，占用资源规模越小意味着方法越容易实 施、适用性越广，其中“SML”表示使用小规模平台，“MED”表示中规模平台。
图5-3给出了多模型混合方法在各个多媒体事件检测上的准确率。
iaa
獭 10EX W 2 0GEX
图5-3 MED16竞赛多模型混合方法结果
2	.实验结果分析
由表5-5可知，本文的方法利用小规模平台达到较高的准确率，其中多模型方法 在两种数据集下均达到了最高的准确率，而单模型方法与其他队伍的方法相比优势也 较大。
通过图5-3可知，多数事件获得了较高的准确率，而且100EX的结果始终高于 10EX的结果，这说明扩大样本的数量对于提高多媒体事件检测的准确率是有帮助的。 但一些个别的事件始终维持在较低的准确率上，这也说明多媒体事件检测框架上还有 较大的改进空间，来提升对各种多媒体事件的检测效果。
5.4.3全国网络舆情(音视频)分析技术邀请赛特定视频识别
1	.实验设置
本文的方法参加了全国网络舆情(音视频)分析技术邀请赛特定视频识别比赛， 其成绩如下表所示。其训练集主要包括四类敏感视频：暴力、色情、集会、新闻，每 类视频样本200个，背景视频样本共10000个；测试集包括5000个视频，视频平均
68
时长约2min。识别结果如表5-6所示。
表5-6全国网络舆情（音视频）分析技术邀请赛特定视频识别成绩
排名	mAP	耗时
1	0.960	3h58m
2(Ours)	0.944	2hl7m
3	0.924	Ih42m
3	0.924	3h41m
5	0.902	4.5h
6	0.716	2h37m
7	0.501	2h45m
8	0.465	8h
9	0.086	8h
其中Ours表示本文的方法，但在进行该项比赛时，由于要求所有的结果必须在 8小时内完成，所用的机器配置为。。因此在比赛中本文使用了单模型方法，使用了 取样间隔为8帧一次、最大帧数为3000的可变间隔取样方法抽取视频中的帧，得到 的每帧使用视频帧处理方法中的whole方法每帧提取了 21个密集局部特征，即尺寸 为1X1、2X2和4X4的特征图响应，之后使用VLAD进行特征融合和SVM进行类 别预测。
2	.实验结果分析
本文的方法是基于多媒体事件检测进行的设计和实验，由于其基本框架也是一个 视频分类框架，所以可以直接用于网络舆情的特定视频识别任务。
由上述结果可知，本文在所有参赛队伍中取得了第二的成绩，虽在准确率上较第 一名略低，但本文的方法耗时仅为其约一半。
经过分析，本文方法没有取得预期效果的原因如下：
（1）由于时间所限，没有在该比赛上尝试多模型融合方法，如果使用多模型融 合方法可以在牺牲一定速度优势的基础上显著提高准确率；
（2）从测试集mAP普遍较高的现象中可以推断出，该比赛的测试集中含有较少 背景样本，而本文的方法是面向真实场景大规模视频数据设计的，测试过程中背景样 本所占比例远大于特定类别的视频比例。本文方法在进行该项比赛时使用了提供的全 部背景样本进行训练，使模型的性能能够在大规模视频数据下表现良好，对于几乎没 有背景样本的测试集缺乏针对性的调整，使得本文在比赛中没有取得顶尖的结果。该
69
比赛在设计测试集比例上并没有完善地考虑实际运用的需求。
当然从另外的角度考虑，如果需要在没有大量背景样本的测试集上进行视频分类 的任务，仅从策略上调整本文的方法即可，即在训练时减少背景样本的数量，而其他 步骤保持不变。因此本文所提方法具有一定的灵活性和可扩展性。
70
第六章总结与展望
6.1	总结
本文提出了高效、准确的多媒体事件检测框架，本文的方法结合了深度学习与传 统方法的各自的优势，在多个数据集上的得到了较好的结果。
经过本文在MED数据集上的实验，不难看出本文的提出的框架拥有以下优点：
(1)高准确率。本文对多种特征进行了实验，最终选取了卷积神经网络作为基 本的帧级别特征提取方法，并在此之上对多种技巧、模型进行实验，选取了最优策略; 同时本文也对多种多媒体事件的时空表示方法进行了实验，选取了最优方法并根据相 关文献和实际经验对其进行了细节上的改进，最终得到了鲁棒、稳定的多媒体事件的 时空表示方法。本文在这这两方面上所做工作使得本文的框架获得了较高的准确率。
(2)高泛化性。本文使用的MED数据集来自于真实的网络视频数据，其广泛 的来源使得本文的方法需要由较好的泛化性才能取得较好结果。本文利用大规模图片 数据集下预训练的CNN模型和无监督的特征融合方法保证了框架适用于各种风格的 视频数据，同时在网络舆情邀请赛上的竞赛成绩也证明了本文的方法能很好地泛化到 其他的视频分类任务。
(3)高效性。本文通过适度的方法简化和策略调整提升框架的速度，同时通过 实验排除了准确率较差的方法，避免了过多低性能方法浪费了系统资源，此外还将并 行策略合理地安排到了各个步骤当中，使得框架对于效率和资源占用率的折中分配具 有一定灵活性，能够在不同规模的平台和应用场景下使用。
6.2	创新点
同时本文认为，本文致力于高效、准确的多媒体事件检测方法的探索，在本文的 研究中，拥有以下两大创新点：
(1)首先，本文结合深度学习和传统特征聚合方法的优势，将CNN和VLAD 应用于视频事件检测当中，取得了显著的成果。
(2)其次，本文在探索基于时空融合的多媒体事件检测的方法时提出数个新的 算法，其中包括显著区域特征提取、netVLAD.时域卷积，尽管在部分算法的准确率
71
在当前训练数据规模下还略有差距，但通过实验结果分析，这些算法仍然具有超越现 有算法的潜力。
6.3	展望
但本文认为在多媒体事件检测中尚存在一些比较有潜力的方法未被加以运用，本 文认为未来的多媒体事件检测任务可以向着以下方向进行尝试：
(1)更大规模的数据用于预训练。本文认为不论是更大规模的数据图像数据还 是更大规模的视频数据，不论用本文当前的框架进行训练还是采用其他方法进行训 练，都将对整体的准确率有所提高。并且随着深度学习方法的流行，之后的方法会将 越来越多地依赖于数据，数据的收集与筛选方法也将变得更加重要。
(2)端到端的训练和预测框架。本文认为近年来深度学习的成功很大程度上受 益于端到端的学习框架，通过机器的自我调整很大程度上避免了人为引入不准确的先 验知识带来的性能下降，同时也使得商业上应用的流程变得更加简单。而多媒体事件 检测也可借助于深度学习框架实现端到端的训练和预测，本文中也提到了多个可以实 现的方法，虽然当前首先与硬件环境限制没有进行尝试，但本文认为端到端的框架必 然是多媒体事件检测未来发展方向之一。
(3)更合理的时序关系机制。虽然本文就目前训练数据量和特征融合方法下排 除了许多引入了时序关系的方法，但并非引入时序关系就一定引起准确率的下降，只 是这些方法在目前的训练数据量下很多方法显得不够鲁棒，降低了其在多媒体事件检 测任务上的表现。本文提出了时域卷积这种具有高鲁棒性的算法，但也不排除许多其 他原理上可行的方案，例如将RNN结构与CNN结构在更为浅层的特征图上进行结 合。本文认为更为合理的时序关系机制的引入将为提升事件检测的准确率，也是该任 务未来的发展方向之一。
72
参考文献
[1]	Annane D, Chevrolet J C, Chevret S, et al. Two-Stream Convolutional Networks for Action Recognition in Videos [J]. Advances in Neural Information Processing Systems, 2014, 1(4): 568-576.
[2]	Xu Z, Yang Y Hauptmann AG. A discriminative CNN video representation for event detection [A]. // Computer Vision and Pattern Recognition [C], Piscataway: IEEE, 2015: 1798-1807.
[3]	Tran D, Bourdev L, Fergus R, et al. Learning spatiotemporal features with 3d convolutional networks [A].// 2015 IEEE International Conference on Computer Vision [C], 2015: 4489-4497.
[4]	He K, Zhang X, Ren S9 et al. Deep Residual Learning for Image RecognitionfJ]. Computer Science, 2015.
[5]	Wang H, Klaser A, Schmid C, et al. Dense Trajectories and Motion Boundary Descriptors for Action Recognition [J]. International Journal of Computer Vision, 2013, 103(1):60-79.
[6]	Ren S, He K, Girshick R, et al. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks [J], IEEE Transactions on Pattern Analysis & Machine Intelligence, 2015:1-1.
[7]	Wang H, Schmid C. Action Recognition with Improved Trajectories [A].// IEEE International Conference on Computer Vision [C], 2013:3551-3558.
[8]	Gers F A, Schmidhuber J, Cummins F. Learning to forget: continual prediction with LSTM [J]. Neural Computation, 2000, 12(10):2451-71.
[9]	Arandjelovic R, Gronat P, Torii A, et al. NetVLAD: CNN architecture for weakly supervised place recognition [J]. Computer Science, 2016.
[10]	He K? Zhang X, Ren S, et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition [J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2014, 37(9):1904-16.
[11]	Wang L, Qiao Y、Tang X. Action recognition with trajectory-pooled deep-convolutional descriptors [J]. 2015:4305-4314.
[12]	Ng Y H, Hausknecht M，Vijayanarasipihan S, et al. Beyond short snippets: Deep
73
networks for video classification [J]. 2015,16(4):1613-1631.
[13]	Arandjelovic R, Zisserman A. All About VLAD [J]. 2013, 9(4):1578-1585.
[14]	Jegou H, Perronnin F, Douze M, et al. Aggregating Local Image Descriptors into Compact Codes [J]. Pattern Analysis & Machine Intelligence IEEE Transactions on, 2012, 34(9):1704^1716.
[15]	TRECVID2014 MED. http://www-nlpir.nist.gov/projects/tv2014/tv2014.html#med.
[16]	TRECVID2015 MED. http://www-nlpir.nist.gov/projects/tv2015/tv2015.html#med.
[17]	TRECVID2016 MED. http://www-nlpir.nist.gov/projects/tv2016/tv2016.html#med.
[18]	Buch N, Orwell J, Velastin S A. 3D Extended Histogram of Oriented Gradients (3DHOG) for Classification of Road Users in Urban Scenes [A].// British Machine Vision Conference [C]5 2009.
[19]	Soomro K, Zamir A R, Shah M. UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild [J]. Computer Science, 2012.
[20]	Chang Y, Lee D J, Hong Y, et al. Unsupervised Video Shot Detection Using Clustering Ensemble with a Color Global Scale-Invariant Feature Transform Descriptor [J]. Eurasip Journal on Image & Video Processing, 2007,2008(1):1-10.
[21]	Perronnin F, Liu Y, Sanchez J, et al. Large-scale image retrieval with compressed Fisher vectors [J]. 2010,26(2):3384.3391.
[22]	Ye H, Wu Z5 Zhao R W, et al. Evaluating Two-Stream CNN for Video Gasification [A].// Proceedings of the 5th ACM on International Conference on Multimedia Retrieval [C], New York:ACM, 2015:435-442.
[23]	Fernando B, Gawes E, Oramas M J, et al. Modeling video evolution for action recognition [A].// CVPR [C]9 2015:5378-5387.
[24]	Krizhevsky A, Sutskever I, Hinton G E. ImageNet Classification with Deep Convolutional Neural Networks [J]. Advances in Neural Information Processing Systems, 2012.25(2):2012.
[25]	Jolliffe I T. Principal Component Analysis [J]. Springer Berlin, 1986, 87(100):41-64.
[26]	Haykin S. Neural Networks: A Comprehensive Foundation [J]. 1998:71-80.
[27]	Nair V, Hinton G E. Rectified Linear Units Improve Restricted Boltzmann Machines [A].// International Conference on Machine Learning [C], 2010:807-814.
[28]	Szegedy C, Ioffe S,Vanhoucke Y et al. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning [J]. 2016.
74
[29]	Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions [J]. 2015:1-9.
[30]	Uy lings J R, Sande K E, Gevers T, et al. Selective Search for Object Recognition [J]. International Journal of Computer Vision, 2013, 104(2):154-171.
[31]	Deng J, Dong W, Socher R, et al. ImageNet: A large-scale hierarchical image database [C]. 2009:248-255.
[32]	http://caffe.berkeleyvision.org/model_zoo.htmL
[33]	http://tinyurl.com/imagenetshuffle.
[34]	Williams R, Zipser D. A Learning Algorithm for Continually Running Fully Recurrent Neural Networks [J]. Neural Computation, 1989,1(2):270-280.
[35]	Kishida K. Property of average precision and its generalization: an examination of evaluation indicator for information retrieval [J]. National Institute of Informatics, 2005.
[36]	Furey T S, Cristianini N, Duffy N, et al. Support vector machine classification and validation of cancer tissue samples using microarray expression data [J]. Bioinformatics, 20005 16(10):906-14.
[37]	Chang X, Ma Z, Yang Y et al. Bi-Level Semantic Representation Analysis for Multimedia Event Detection [J]. IEEE Transactions on Cybernetics, 2016:1-18.
[38]	Chang X, Yang 工 Long G, et al. Dynamic Concept Composition for Zero-Example Event Detection [J]. 2016.
[39]	Chang X, Yang Y, Long G, et al. They Are Not Equally Reliable: Semantic Event Search using Differentiated Concept Classifiers [J] ,2016
[40]	Chang X, Yu Y L, Yang Y et al. Searching Persuasively: Joint Event Detection and Evidence Recounting with Limited Supervision [A].// ACM International Conference on Multimedia [C], New York:ACM, 2015:581-590.
[41]	Chang X, Yang Y, Xing E, et al. Complex Event Detection using Semantic Saliency and Nearly-Isotonic SVM [J]. 2015.
[42]	Keerthi S S, Gilbert E G. Convergence of a Generalized SMO Algorithm for SVM Classifier Design [J]. Machine Learning, 2002, 46(1):351-360.
[43]	SUN. 114P:〃groups.csail.mit.edu/vision/SUN.
[44]	Krogh A, Larsson B, Von H G, et al. Predicting transmembrane protein topology with a hidden Markov model: application to complete genomes. [J]. Journal of Molecular Biology, 2001,305(3):567-580.
75
