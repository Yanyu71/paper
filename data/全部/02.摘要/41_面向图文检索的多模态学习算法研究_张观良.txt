摘要
近年来，随着网络、计算机技术的快速发展，数据分享的兴起， 人们面对的是文本与图像数据的大量增长，如何方便精准地为用户获 取感兴趣的数据成为一个突出问题。在利用文本搜索图片时，其技术 包括:基于文本的图像检索技术，图像数据需要有正确的标签和内容， 因此该技术要求有人工语义标注。在网络上的社交数据都有用户给定 的标注数据使得获取大量有语义标注的数据成为可能。然而这些文本 数据具有大量的随意性，包含大量噪声并且不完整；基于内容的图像 检索技术利用深度学习模型得到了巨大发展，但视觉特征与高层语义 之间的语义鸿沟使得检索精度不能满足要求，另外用户难以找到与检 索图片相似的样例。
由于图文检索技术上存在的以上问题，通过文本与图片内容的直 接关联，从而实现数据跨模态的搜索是非常核心的一个基础需求。基 于该需求，本文结合文本与图像上的成熟模型，研究了融合文本与图 片特征的多模态模型，具体工作如下：
(1)利用高斯玻尔兹曼机作为图像特征的输入模型，重复软最大 化模型作为文本特征的输入模型，本文通过联合特征完成了图片与文 本关系的建模，并在Flickr8k数据集上完成跨模态图文检索实验。该 模型具有能够特征重构的优点，通过建立特征索引，在较大的数据库 当中的检索速度相比需要计算跨模态特征相似度的模型更快。
(2)针对重复软最大化模型中单词特征简单和忽略文本结构的缺 陷，本文通过依赖树分析了句子结构，同时使用词向量作为单词特征。 根据依赖树的生成方式，使用递归神经网络对单词关系进行建模。最 后根据在一个模态下利用特征进行搜索的原理，建立排列损失函数训 练多模态模型。实验表明，该模型在检索精度上优于多模态深度玻尔 兹曼模型。
(3)针对一个固定长度的特征向量不足以描述复杂图片或句子的 问题，本文提出在更细粒度的层面上学习图像与文本之间关系的方法。
在图像方面，利用区域卷积神经网络作为物体识别算法，从物体上提 取图像特征。在文本方面，利用双向循环网络使得转化后的单词特征 能够包含前后文信息。最后定义细粒度图像特征与细粒度文本特征相 似性评价指标，使得可以利用排列损失函数完成多模态模型训练。由 于训练时间较长，该模型在深度学习框架Caflfe上实现。实验表明， 在检索精度上优于使用一个特征向量描述图像或文本的模型。
(4)利用自然语言模型和图像特征，本文从另外一个角度建立 能够进行图文搜索的多模态模型。在对数双线性自然语言模型中，引 入图像特征偏置项来影响基于上下文预测下一个单词的概率，从而建 立多模态模型。实验表明，在困惑度作为图像与文本关系的评价指标 体系下，提出的模型尽管在检索速度上较慢，但检索精度优于深度玻 尔兹曼机模型。
通过以上工作，本文从粗粒度到细粒度，建立了物体或动作在视 觉上与文本之间的关系。基于此，可以利用自然语言描述进行图片搜 索的功能，也支持利用图片搜索最为接近的文本描述的功能。
关键词:多模态;检索;深度学习
RESEARCH ON MULTIMODAL TRAINING FOR IMAGE AND TEXT RETRIEVAL
ABSTRACT
In recent years, with the rapid development of network and computer technology, the rise of data sharing, people are faced with massive growth of text and image data. How effectively to get data which we are interested has become a prominent problem. The technic of image retrieval includes Text-Based Image Retrieval (TBIR) and Content-Based Image Retrieval (CBIR). TBIR requires manual annotations for image data to get correct tags and contents. Social data given by users are labeled data which makes it possible to obtain a large number of semantic annotation data. Howere, such kind of text data are noisy and incomplete. Wth the use of deep learning, CBIR has been greatly developed, but the retrieval precision of CBIR is unsatisfactory because of the semantic gap between the visual features and semantic concept. In addition, it is difficult for users to find a similar picture with the query one.
Because of these problems, achieving cross-modal search with direct correlation between text and image is a core underlying demand. Based on this demand, combining with the mature model of text and image, this thesis dedicates to the research of multimodal model with the fusion of text and image features. The main work of this thesis are as follows:
1.With Gaussian Restricted Boltzmann Machine as input model of image features and Replicated Softmax Model as input model of text features, we complete modeling relationship between image and text through the join representation, and the retrieval experiment on Flickr8k dataset. This model can be able to rebuild the feature of data, so it is faster than the model which needs to calculate the similarity between image features and text features in large datasets through indexing.
2.We use dependency tree to analyse the structure of sentences and word embedding to represent the features of words. According to the principle of dependency tree, we implement Recursion Netural Network to model the relation between words. Finaly, we contract the ranking cost function to train the parameters in the model. Retrieval experiment show that this model is better than Multimodal Deep Boltzmann Model (M-DBM) on retrieval accuracy.
3.Because a single, fixed-sized representation is unable to descript the complex image or sentence, we propose a method to learn relation between image and text in a fine-gained level. Convolution Netural Net- work(CNN) is used to extract feature as fine-gained feature of image from objects after Regions with CNN locating the object positions in the image. Word embedding, as the fine-gained feature of text, makes use of Bidirectional Recurrent Neural Network to contain context information. With the fine-gained feature of text and image, we define a ranking cost function to train the multimodal model. Due to the long training time, we implement the model on Caffe. The performance in retrieval experiment show that the model with fine-gained feature is better than other models on retrieval accuracy.
4.From another point of view, we set up a multimodal model to retrieve image with natural language model. In Log-Bilinear Language (LBL), transformed image feature is as a bias to affect the predicted probability of the next word. Although the speed of retrival is slow, the multimodal LBL is better than M-DBM on retrieval accuracy.
In conclusion, through above work, we establish relations between visual features and semantic concepts and complete bidirectional retrieval of images and natural language descriptions.
KEY WORDS:multimodal;retrieval;deep learning
