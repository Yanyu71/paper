摘要
目前，基于深度学习的图像感知算法虽然取得很大的进步，但是 仅利用图像信息只能帮助智能设备去感知静态的外界环境，无法提取 到时序动作信息和关系信息。视频数据在当今社会生活中爆炸性增长, 相比于图像数据，视频数据拥有视觉信息，听觉信息，以及时序信息， 利用它可以更好地帮助智能设备去感知和理解外界环境。在深度学习 领域，视频作为样本输入可以带来更多的信息，也带来了更为庞大的 计算量。因此，如何降低视频深度学习算法的计算量已经成为一个重 要的研究问题。
在很多智能设备上如嵌入式移动终端，机器人、无人机、无人车 等均会有视频数据的产生和传输，然而这些设备一般不具有强大的算 力资源，传统的基于视频数据的深度学习算法计算量大，无法在低算 力的智能设备上顺利部署使用。同时，目前大多数智能设备采集的环 境视频数据，一般将数据传输到远程服务端，然后再进行算法处理。 由于部分场景如无人驾驶，无人机等要求具有高稳定性，低延时，将 视频传输回到远端服务器进行处理不可避免会存在延时问题。因此， 亟需在智能设备上部署一套轻量化的视频理解算法，是提升无人设备 智能化水平的重要手段。
为了解决以上问题，本文首先研究了深度神经网络轻量化技术， 在此基础上，深入研究了基于深度神经网络的环境感知和理解算法， 构建了基于环境视频作为输入的多模态数据的特征提取和特征融合 模型。本文主要研究内容和创新点如下：
(1) 针对2D神经网络计算量大导致单帧图像特征提取速度慢 的问题，本文提出了 Mobile SOSA模块和MEESP模块，基于这两个 模块构建了轻量型2D卷积神经网络M-SOSANeto为了验证网络的 基础特征提取能力，使用该网络模型进行图像分类的实验，在 CIFAR-10, CIFAR-100, ImageNet-1000k 分别到达了 95.64%, 78.6%, 72.8%的top-1精度，仅有290MFlops的计算量消耗。为了验证网络 的泛化性，使用该网络进行语义分割的实验，在PASCAL VOC 2012 上达到了 67.8%的mIOU,仅有0.82BFlops的算力消耗。为了分析神 经网络在真实设备上的实时性，分别在TITANX, 15-8400,嵌入式设 备JetsonTX2等三种不同类型设备进行验证，实现结果表明，M- SOSANet在以上三种设备运行的速度分别为180fps, 44fps, 5fpso
(2) 针对基于3D神经网络提取视频特征计算量大的问题，对 2D卷积神经网络RegNet进行重构设计，构建了轻量型3D卷积神经 网络RegNet3Do首先，为了验证神经网络的基础特征提取能力，在 较大规模的动作分类数据集SomethingSomethingV2进行实验，达到 7 45.07%的top-1精度，相比其他算法，计算量和参数量分别下降8 倍和6倍。为了验证网络对局部动作信息的提取能力，在大规模手势 分类数据Jester进行实验验证，达到了 90.1%的top-1精度，计算量 仅有0.62BFlops；为了验证网络的迁移泛化能力，在中规模的视频数 据集UCF-101进行实验，最终达到了 87.4%的top-1精度。
(3) 针对现有图像分类模型RegNetX-400M无法有效提取视频 中的音频模态数据的特征和未经过大规模数据集预训练影响其精度 的问题，本文引入了概率知识迁移的知识蒸憾方法。对该网络在著名 声音场景分类数据集UrbanSound8K上进彳亍实验，在未引入知识蒸 馆时达到了 78.3%的精度，引入之后提升至92.3%。并且该神经网络 的计算量仅由0.8GFlops,参数量5.2Mo
(4) 为了更好融合多模态特征，解决传统transformer结构存在 计算量大和参数量大的问题，本文提出了跨模态的自注意力机制，使 用轻量型transformer变体DeLighT替代原始版本，并结合跨模态注 意力机制，构建一套视频描述任务的编码器-解码器模型。本文使用该 模型在ActivityNet Captions数据集上进行实验，验证了多模态融合模 型的实际视频理解能力。实验结果表明，BLEU@3达到了 4.59, BLEU@4达到1.97, METEOR达到了 8.96,而参数量仅有30M,相 比其他算法，参数量下降约50%0实验结果证明，使用本文体系的多 模态特征提取网络和融合网络，能够实现面向智能设备的视频理解。
关键词:轻量化2D-CNN;3D-CNN;多模态注意力机制
RESEARCH ON LIGHTWEIGHT ENVIRONMENT PERCEPTION AND UNDERSTANDING ALGORITHM FOR INTELLIGENT DEVICES
ABSTRACT
At present, although the image perception algorithm based on deep learning has made great progress, only using image information can only help intelligent devices perceive the static environment, and can not extract the action information and relationship information in time sequence. Video data in today's social life explosive growth, compared to image data, video data has visual information, auditory information, and timing information, using it can better help the computer to perceive and understand the external environment. In the field of deep learning, video data not only brings more information, but also brings more computation. Therefore, how to reduce the computational complexity of video deep learning algorithm has become an important research problem.
In many intelligent devices, such as embedded mobile terminals, robots, UAVs, unmanned vehicles, etc., video data will be generated and transmitted. However, these devices generally do not have strong computing resources. Traditional deep learning algorithm based on video data has a large amount of calculation, and can not be deployed and used on low computing intelligent equipment. At the same time, most of the video data generated or collected by most intelligent devices usually transmit the video to the remote server, and then carry out algorithm processing. Because some scenes such as unmanned driving and UAV require high stability and low delay, there will be delay problem when video is transmitted back to remote server for processing. Therefore, it is necessary to deploy a lightweight video understanding algorithm on the intelligent equipment with low computational power, which is an important means to improve the intelligent level of unmanned equipment.
In order to solve the above problems, this paper first studies the lightweight technology of neural network. On this basis, the video perception and understanding algorithm based on deep neural network is deeply studied, and the feature extraction and feature fusion model based on video multimodal data is constructed. The main contents and innovations of this paper are as follows:
(1) In view of the problem that the computation of 2D neural network is large and the speed of feature extraction of single frame image is slow, this paper proposes Mobile SOSA module and MEESP module. Based on these two modules, a light-weight 2D convolutional neural network M- SOSAnet is constructed. In order to verify the basic feature extraction capability of the network, the experiments of image classification are carried out by using the network model. In CIFAR-10, CIFAR-100 and ImageNet-1000k, the top-1 precision reaches 95.64%, 78.6%, 72.8% respectively, and only 290MFlops is consumed. In order to verify the generalization of the network, the semantic segmentation experiment using the network has reached 67.8% of the mIOU in Pascal VOC 2012, with only 0.82BFlops calculation effort consumption. In order to analyze the real-time performance of neural network on real-time equipment, the verification is carried out in three different types of devices, such as titanx, i5-8400 and embedded equipment Jetson TX2. The results show that the speed of M-SOSAnet in the above three devices is 180ips, 44fps and 5fps respectively.
(2) In order to solve the problem of high computational cost of video feature extraction based on 3D neural network, the reconstruction design of 2D convolutional neural network regnet is carried out, and a light 3D convolutional neural network RegNet3D is constructed. Firstly, in order to verify the basic feature extraction ability of neural network, experiments are carried out in the large-scale action classification dataset SomethingSomethingV2, which achieves 45.07% of top-1 accuracy. Compared with other algorithms, the calculation and parameter amount are reduced by 8 times and 6 times respectively. In order to verify the ability of extracting local action information, the large-scale gesture classification dataset Jester was tested and verified, which reached 90.1% of top-1 accuracy, with a calculation amount of only 0.62BFlops. In order to verify the network migration generalization ability, the medium-scale video dataset UCF-101, was tested, and 87.4% of top-1 precision was achieved.
(3) In view of the problem that the existing image classification model RegNetX-400M can not extract the audio modal data effectively and affect the accuracy without large-scale dataset pretraining, this paper introduces the knowledge distillation method of probability knowledge transfer. Experiments on the famous sound classification dataset UrbanSound8K show that the accuracy of the network is 78.3% without knowledge distillation, and it is improved to 92.3% after knowledge distillation. The calculation amount of the neural network is only 0.8GFlops and the parameter is 5.2M.
(4) In order to better integrate multimodal features and solve the problems of large computation and parameter in traditional transformer structure, this paper proposes a cross-modal self-attention mechanism, which uses light transformer variant delight to replace the original version, and constructs a set of encoder decoder model for video description tasks combined with the cross-modal attention mechanism. This paper uses this model to experiment on the ActivityNet Captions dataset, and verifies the actual video understanding ability of multimodal fusion model. The experimental results show that, BLEU@3 It reached 4.59, BLEU@4 The results show that the number of parameters is only 30M, and the amount of parameters is about 50% lower than other algorithms. The experimental results show that the multi-modal feature extraction network and fusion network of this system can realize video understanding for low computational power intelligent equipment.
KEY WORDS:Lightweight 2D-CNN;3D-CNN;Multimodal SelfAttention
