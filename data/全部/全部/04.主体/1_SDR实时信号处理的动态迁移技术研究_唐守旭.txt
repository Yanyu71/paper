第一章绪论
1.1课题研究背景及意义
宏观上，随着移动通信技术的飞速发展，人们对无线宽带业务的需求与使用 持续增加，因此就需要不停的增加基站建设的投入，增加空中接口带宽。随之而 来的无线接入网络的能源消耗问题也越来越严重，同时也意味着运行维护成本的 增加，这些因素将变得越来越难以接受，需要我们不得不面对解决。
由于无线移动网络有一个固有的特性就是其使用用户都是处理不断移动的 状态，通常是顺着时间的变化从一个地方移动到另一个地方，而且用户的移动也 呈现出很强的时间规律性。例如在上班的时间段内，在办公区附近将会有大量的 用户业务需求，而居住地的用户将会很少；而晚上或者假期则正好相反。这样就 会导致用户多的地方，数据处理业务非常繁忙，网络负载将会居高不下，业务质 量也将会随之下降，而用户少的地方将会出现网络负载很多，造成大量的资源浪 费⑴。这一问题如果简单的通过增加或者减少基站来解决的话，可能会造成严重 的能源损耗和资源的浪费或者导致网络覆盖变差、网络容量减少，影响正常业务 的处理。这种整体计算处理资源比较丰富，局部计算处理资源十分紧张的情况， 不能简单的通过增加基站建设的投入来解决，这样只会造成更大的计算处理资源 的浪费。因此，需要实现能够使计算资源随着网络负载的变化而变化，这样既能 满足高负载时的业务处理能力，又能够在低负载时实现资源的节约，能源损耗的 降低。为了实现计算处理资源的动态变化，就需要将大量的计算处理单元整合起 来，形成一个计算处理资源池，使该资源池能够随着负载的大小来调节自身的计 算处理能力，既避免资源浪费，又能够满足业务处理的要求，从而提高资源的利 用效率。
另一方面从微观角度来讲，随着工艺与技术的飞快发展，物理硬件资源的性 能与容量得到了跳跃是的发展，而且价格却是不断降低，因此，在现有的系统平 台架构之下，许多物理硬件资源都得不到充分的使用，从而造成了大量的浪费。 所以需要能够尽量使用最新的物理硬件平台，提高资源利用率，因此一个自然而 然的想法就是将现有性能落后的硬件资源更新成最新的性能好的物理硬件，以便 能够充分发挥其使用效率。不过，人们一般都会岀于对系统的稳定和后续兼容升 级的角度考虑，并不总是采取大规模的更新现有物理硬件平台的方法来解决问题。
因此，虚拟化技术应运而生，为解决上述问题提供了一个全新的解决理念。 传统的虚拟化则指设计一个全新的虚拟化调度层一一虚拟机系统，该系统既实现 对物理硬件资源的分配调度，也为能够虚拟出业务所需的系统资源。而广义上的 虚拟化技术则是通过引入一个适当的调度层，可以使用传统的虚拟机系统来充当 该调度层，也可以是其他任何的能够调度资源进行迁移的应用程序等。通过调度 层来统一调度分布式的计算处理资源，形成一个计算处理资源池，资源池内部的 计算分配是动态调度的，对外表现出一个具有巨大计算处理能力的资源池。该资 源池既具有比单个计算处理单元强大得多的计算处理能力，同时也具有相当强的 可靠性，资源池内部可以通过动态分配调度来增强可靠性。
为了实现虚拟化技术，需要解决的最大的难题是，在计算处理资源池内部， 需要根据负载均衡、节省能源、容灾备份等一系列条件综合考虑，对资源池内部 的计算处理单元进行统一的调度和分配，从而达到预期的合理的状态。而实现对 计算处理资源池内部计算处理单元的合理的调度和分配的关键性技术是需要实 现在计算处理资源池内部对计算处理单元的动态迁移。只有能够实现计算处理单 元在计算处理资源池内部实现快速合理的动态迁移，才能够根据一定的条件和需 要来实现并且达到计算处理资源池的整体最优化。
1.2C-RAN 愿景
1.2.1传统RAN的局限
RAN(Radio Access Network，无线接入网)是基础设施建设中最重要的一环， 它能够给终端提供全天候连续的，高性能的业务处理服务，是基础设备中的核心 所在⑴。然而这些现有的RAN网络都具有以下几点不足：首先，一个基站都会 连接着个数不等的固定几个扇区天线，对该小区进行业务覆盖，因此，该基站将 仅仅可以实现该区域的信号处理业务；其次，传统RAN网络的系统中，不同的 基站之间都是相互无关的，无法进行统一协作，提高频谱利用率；最后，不同的 基站一般会在不同的硬件平台上进行开发实现的，具有垂直特性。随着移动网络 的发展和人们需求的增长，传统无线接入网的这些特点将会对移动业务的发展带 来更多的限制与挑战：业务的增长带来的大量的基站的需求将会带来高昂的基础 设施建设的费用，以及相应的后期维护工作和开销，而且大量的基站也将带来巨 额的运营花费；但是同时在已有的基站中，物理资源的实际使用情况往往不容乐 观，非常低效的，其平均的网络业务使用效率通常要大大少于业务高峰期的使用 效率，而且各个基站的硬件处理资源不能互相使用，这将导致对于频谱的使用效 率会很难再进一步有所改善；专有的硬件将导致不同的互相无法兼容的不同平台 版本，将给后续的系统平台维护与升级等操作带来诸多不便。总而言之，传统的 RAN网络的诸多特点将会越来越阻碍无线移动网络的长期发展。因此，需要一 套全新的网络基础架构为现今的移动互联网提供支持与保障。
1.2.2C-RAN 的优势
为了满足移动互联网的快速发展所提出的各种要求，针对基础设施建设提出 了绿色无线接入网(Centralized, Cooperative, Cloud RAN, C-RAN)的概念⑵。 C-RAN将是一种基于集中式基带资源处理池，远端无线射频单元与天线构成的 协作式无线网络和基于开放平台实时的具有云架构的基础设施的无线接入网。其 中，集中式的基带处理资源池方案将能够大量降低服务同等面积小区所使用的基 站的数量；协作式的无线远端模块和天线能够有效改善现有系统的频谱使用情况, 使频谱资源能够得到更加充分的使用；在开放的物理硬件基础平台上建立的具有 实时云架构模型的基础设施和由此实现的基站的虚拟化技术能够大大减少基站 建设的资本投入，并且能够实现不同基站之间的计算处理资源的共享，降低能源 损耗，使物理硬件基础平台设施的使用效率大大提升。这些特点决定了 C-RAN 将可以给终端用户带来全新的移动宽带网络的接入使用体验，同时也需要保证用 户能够体验到更加低价而优质的数据接入服务。因此，C-RAN也需要可以达到 以下几点要求：
1、	降低能源的损耗，能够有效减少资本开销和运营花费，提高能源使用率。
2、	对现有的频谱资源使用更加充分，提升频谱的使用效率，增加用户带宽。
3、	更加开放的物理硬件平台，能够满足不同标准的要求，实现系统的平滑 升级
4、	能够给用户提供非常优质的移动网络的接入服务，改善用户的使用体验。
基于C-RAN的众多优势和特点，在未来的无线互联网的发展中必将起着非 常重要的作用。然而，整个C-RAN架构的实现也将会面临诸多挑战和困难，需 要投入更多的研究和技术。其中，C-RAN架构中的基带资源池，协作式无线网 络和实时云架构等不同组成部分实现的技术关键都在于处理资源之间的协作与 调度。而实现不同处理资源之间的协作和调度，达到处理资源共享的关键技术是 能够使实时信号处理业务在不同的资源单元中实时的高效的完成迁移。这一关键 技术的研究与实现将会是C-RAN不同组成部分实现的基础。因此本文将会对这 一关键性的技术进行详细的介绍，设计以及实现，并将这一技术应用于实际的通 信业务场景中，为C-RAN的实现提供技术上的支持和保证。
1.3资源调度的关键——动态迁移技术
1.3.1动态迁移技术的概念
迁移分为系统整体的迁移和针对某单个工作负载的迁移。系统整体迁移，指 将硬件平台上的系统中的所有负载（也包含操作系统本身）完整地迁移到另一个 物理硬件平台之上。而单个工作负载的迁移，是将一台物理机器上的某个正在运 行的工作负载转移到另一台物理机器上继续运行。本文主要研究的是针对实时信 号处理的动态迁移技术，属于单个工作负载的迁移技术。
迁移又包括静态迁移与动态迁移两种不同形式。静态迁移与动态迁移的最大 区别在于，静态迁移有非常明显的一段时间内，提供服务的客户机（这里的客户 机是广义的，可以代表某个实时信号处理任务，软件处理服务甚至虚拟的操作系 统等。当其代表某个实时信号处理程序时，该迁移就归属于单个工作负载的迁移， 也就是本文研究的主要内容，下同）将会出现中断，导致终端用户的服务会出现 严重的中断；而动态迁移则没有明显的服务暂停现象。
动态迁移是指在保证客户机上应用服务正常运行的同时，让客户机在不同的 宿主机之间进行迁移，有硬盘存储和内存都复制的动态迁移，也有仅仅复制内存 镜像的动态迁移。不一样的是，为了确保在迁移操作进行中客户机服务的可用性， 迁移过程只能有很短的宕机间隔。动态迁移允许系统管理员将客户机在不同的物 理机上进行迁移，同时不会断开与访问客户机的服务的终端客户或应用程序的连 接。一个成功的动态迁移，需要保证客户机的网络连接，工作状态，服务内容等 在迁移到目的主机之后依然保证不变，而且迁移过程中的服务暂停时间非常短。
1.3.2动态迁移技术的意义
由于云计算和虚拟化的不断发展，对于处理资源的动态调配的要求越来越多 而且要求也越来越高。而为了满足对于计算处理资源的动态调度与分配的要求，
3
相应的计算处理单元的动态迁移技术也需要相应的发展。尤其当需要实现对于实 时性要求非常高的实时信号处理程序的动态调度与分配时，则对动态迁移技术的 实时性具有更高的要求。
而传统的基于虚拟机的迁移技术，只适合于-般的实时性要求不高的应用场 合；对于实时性要求非常高的通信领域的实时信号处理相关的计算进行迁移时， 则需要设计实现一种具有高实时性的动态迁移技术，来满足实时信号处理程序在 整个动态迁移过程中对于实时性的要求。
具有高实时性的动态迁移技术的实现，对于通信领域内的实时信号处理的实 时地动态调度与分配提供了实现的可能。从而可以根据一定的需要和条件来在不 同的计算处理单元之间调度实时信号处理程序，实现计算处理资源的合理分配而 到达负载均衡；也能够实现实时信号处理程序的集中处理，避免一些低效率的计 算处理单元，提高资源利用率;也能够通过一些主动的实时信号处理的动态迁移, 起到对实时信号处理的备份作用，提高可靠性和安全性。
1.3.3动态迁移技术的应用前景
将来随着动态迁移技术的不断发展与成熟，动态迁移技术的各方面性能的不 断提高，动态迁移技术的应用前景肯定会是非常广阔的，尤其是高实时性的动态 迁移技术的发展使实时信号处理程序的动态迁移成为可能，将会给通信领域中实 时性需求很高的数据处理业务产生巨大的影响。下面简要介绍几点动态迁移技术 的应用场景，以说明该技术的广阔的应用前景。
•负载均衡：当一台物理服务器的负载较高时，可以将其上运行的计算处理 单元动态迁移到负载较低的宿主机服务器中，以保证计算处理单元能够正 常进行相应的服务。当物理资源长期处于超负荷状态时，对服务器稳定性 能和服务质量都有损害的，这时就需要动态迁移来对其进行适当的负载均 衡。
•解除硬件依赖：当系统管理员需要在宿主机上升级、添加、移除某些硬件 设备的时候，可以将该宿主机上运行的计算处理单元非常安全高效地动态 迁移到其他宿主机上。在系统管理员升级硬件系统之时，使用动态迁移, 可以让终端用户完全感知不到服务有任何的暂停现象。
•节约能源：在目前的数据中心的成本支出中，其中有一项重要的费用是电 能的开销。当有较多的服务器的资源使用效率都比较偏低的时候，可以通 过动态迁移将宿主机上的计算处理单元集中迁移到其中几台服务器上，而 在某些宿主机上的计算处理单元完全迁移走之后，就可以将关闭电源以节 省电能的消耗，从而降低数据中心的运营成本。
•实现客户机地理位置上的远程迁移：假设某公司的运行某类应用服务的业 务处理单元本来仅部署在上海电信的互联网数据中心中，然后发现来自北 京及其周边地区的网通用户访问量非常大，但是由于距离和网纟各互联带宽 拥堵（如电信和网通之间的带宽），北方用户使用该服务的网络延迟较大， 这个时候系统管理员可以将上海的互联网数据中心中的部分业务处理单 元通过动态迁移部署到位于北京的北京网通互联网数据中心中，这样可以 让该终端客户使用该服务的质量更高。
由于动态迁移技术的应用前景非常广阔，所以对动态迁移技术的研究就显得
非常重要，尤其是针对通信领域中的实时信号处理程序的高实时性的动态迁移技 术。
1.4本人的主要研究内容
由于实时信号处理对实时性的要求非常高，这就对动态迁移技术的实时性提 出了很高的要求，也是很大的挑战。因此，需要专门设计针对实时信号处理的高 实时性的动态迁移技术，来满足通信领域中的高实时性的需要。本人主要针对动 态迁移技术做了如下工作：
（一）	学习和研究现有的迁移技术，并介绍了一些虚拟机系统和基于这些虚拟 机的虚拟化技术，然后介绍了虚拟机的动态迁移技术。指出了现有的基 于虚拟机系统的动态迁移技术在实时性方面的不足，引出需要设计针对 实时性业务处理的动态迁移方案。
（二）	研究实现了 USRP数据流的热迁移技术。在实验室的现有条件，为了能 够在实验室的条件下，实现针对实时信号处理的动态迁移这一关键技术， 本人将从天线端进行收发和采样处理数据的射频端处理设备采用SDR
（Software Defined Radio,软件无线电）中的 USRP （Universal Software Radio Peripheral,通用软件无线电平台）设备，而与射频端进行数据通 信计算处理资源则釆用IBM的一款服务器设备。为了实现实时信号处理 的动态迁移，首先要实现数据流快速高效地迁移。因此，本文针对USRP 设备提出设计了将USRP数据流快速高效的进行迁移的热迁移技术方案, 以满足实时信号处理动态迁移的需要。
（三）	研究实现了实时信号处理程序在不同的IBM Server之间进行迁移的技术。 由于实时信号处理的高实时性的要求，所以必须尽可能的保证整个动态 迁移的过程都是高效快速的进行，使迁移过程中，数据的丢失尽可能小， 不影响数据业务的正常处理。鉴于这些要求，必须针对实时信号处理设 计一套专门的具有高效率和高实时性的动态迁移系统方案，本文对该动 态迁移核心系统进行了详细的设计，提出了一套动态迁移系统方案，并 用ping数据业务对该动态迁移系统进行了实验验证。
（四）	研究实现了基于动态迁移技术的中继业务方案。在完成了针对实时信号 处理程序的动态迁移的核心技术之后，为了说明该动态迁移技术具有较 高的可扩展性和广阔的应用前景，设计实现了基于该动态迁移核心技术 的通信中继业务，使得中继计算单元具有更高的可靠性和更高效的资源 利用效率。
现阶段，针对实时信号处理程序的具有高实时性的动态迁移技术的研究很少, 所以本人针对该技术的研究具有一定的开创性。因此针对以上研究内容，本课题 的创新之处主要有一下几点：
1.在USRP设备的基础上，通过整合其不同的驱动接口程序，提岀并且实 现了 USRP数据流的热切换技术。
2.设计了一整套基于IBM Server的实时信号处理程序的动态迁移系统方案， 实现了实时信号处理的高实时性和高效率的在不同的IBM Server之间进 行迁移。这一关键技术的实现，为高实时性的通信领域的资源整合，负
5
载均衡等技术的实现提供了支持。
3.提出并实现了将实时信号处理的动态迁移系统应用到中继业务中，实现 中继计算处理单元在中继业务点内的不同Server之间的动态迁移技术方 案，实现中继业务的计算资源的整合和负载均衡等，提高中继计算资源 的利用率和计算单元的可靠性。
1.5论文的组织结构
论文总共分为五章，本文的组织架构如下：
第一章为绪论，主要介绍的本课题的研究背景意义，应用背景以及迁移技术 简介，并简要介绍了一下本文的主要研究内容和创新点。
第二章介绍了基于虚拟机系统的虚拟化技术，并研究了基于该虚拟化技术的 动态迁移技术，指出了其缺陷所在，需要突破。
第三章主要研究了 USRP设备的数据流迁移技术，并利用该技术详细设计了 一套应用于实时信号处理的动态迁移系统方案，并通过实际测试验证了该系统的 性能。
第四章主要研究了将第三章中设计的动态迁移系统应用于中继业务中，验证 了该动态迁移系统的可扩展性，展现了动态迁移技术的广阔的发展前景。
第五章是对本文的总结与展望，也指出了进一步的发展方向。
第二章基于虚拟机的迁移技术研究
为了实现资源的合理调用和分配，最大效率的利用有限的计算处理资源，则 需要对不同的计算处理单元进行整合和调度迁移，使它们处于最优的工作组合状 态下对待处理资源进行处理。计算处理单元分布在不同的物理平台上，对外表现 出相当于一个巨大的云计算处理资源池，对输入进来的待处理资源进行处理，而 由哪一台或者哪几台来具体处理该资源则根据处理资源的分配来内部进行调度。
构建云计算处理资源池的关键技术则是虚拟化技术，而虚拟化的核心计算则 是对计算处理单元在云计算资源池中的迁移技术。在资源优化配置，利用率提高 和安全可靠性等方面，虚拟化技术都比直接使用硬件平台有着巨大的优势。
2.1基于虚拟机的虚拟化技术
虚拟化是一个为了提高资源优化配置，使资源管理更加简化的解决方案，是 构建云计算基础平台重要的关键技术之一。目前被广泛用来实现和应用的虚拟化 技术都是基于虚拟机系统原理实现的。首先对主流的虚拟机系统进行简要描述囲。
2.1.1典型的虚拟机系统
1）	Xen
Xen是一个完全免费的开源的虚拟机系统监控平台，发起于英国剑桥大学。 它的目标是能够在单机硬件平台上运行上百个满特征的操作系统。
特权操作系统（Domain 0）可以是Linux、Solaris以及NetBSD等。Xen系 统最早的虚拟化思想是类虚拟化：利用直接对Linux内核进行修改来使内存资源 和处理器虚拟化。Xenl.O和Xen2.0先后两个版本完成了针对操作系统级的类虚 拟化技术，即图2-1中所示的Domain 0和DomainNo基于Xen的虚拟化技术的 虚拟机无需特殊的附加硬件支持，所达到的性能即可以与真实物理机平台相媲美。
由于Xen虚拟化技术的蓬勃发展，Xen系统也将完全的硬件虚拟化技术包含 进来,Xen3.0己经支持利用Intel VT和AMD SVM硬件技术实现的完全虚拟化， 如图2-1中所示的VMX Domain。Hypervisor利用硬件平台资源己有的功能来完 成内存、处理器和I/O接口的完全虚拟化。
Xen的特点总结如下：
•开放源代码
•可移植性强
•具有与真实物理机平台相近的性能，具有独有的对类虚拟化技术的支持
•由于操作系统需要进行显式地修改才能在Xen上运行，因此Xen的易用 行比较差。


图2-lXen架构图【5】
2)VMware
VMware是在x86平台上运行的，是一款现今被广泛接受使用和认可的虚拟 机系统软件。VMware具有非常多的虚拟化类别的产品，涉及到从服务器的级别 到桌面级别的各种系统产品，这些产品能够在Windows、Linux和Mac OS X等 不同的操作系统上运作。此外，基于Hypervisor架构的VMware ESX Server可以 直接运行在裸机上。
VMware ESX Server是VMware的龙头产品，是一款具有监视功能的虚拟机 产品，不利用宿主机操作系统，而是在直接应用在硬件平台上。ESX Serve基于 Hypervisor模型，在性能和安全性方面都得到了优化。VMware ESX Serve支持 完全虚拟化，可以运行在Windows、Linux、Solaris和Novell Netware等客户机 操作系统。在新版本的ESX Server中已经开始釆用直接基于硬件平台的虚拟化 技术，分别支持Intel VT和AMD SVM技术。
VMware Workstation是VMware面向桌面的主打产品。Workstation是一款应 用在宿主机模型上的产品，其所在的宿主机能够安装Linux或Windows等系统。 VMware Workstation 支持完全虚拟化，可以运行在 Windows> Linux、Solaris> Novell Netware和FreeBSD等客户机操作系统。
由于VMware是最早开发虚拟化产品的，所以在产品种类和功能上的优势也 比较明显。VMware产品具有以下几个特点：
•功能丰富：几乎大部分的虚拟化功能都有相应的VMware产品与之对应。
•配置和使用方便：VMware开发了非常易于使用的配套工具和友好的操 作界面。
•稳定，适合企业使用。
3)Hyper-V
Hyper-V是一款操作系统管理应用程序的虚拟化技术，其功能是针对虚拟机 的建立与运作来进行相应的操作，并实现硬件平台资源的虚拟化工作。Hyper-V 的架构图如图2-2所示。Hyper-V是处在虚拟机系统和物理平台中间的一层，不 包括其他的第三方程序，因此特别简洁。这样的设计可以让硬件平台与虚拟机系 统仅仅使用非常简洁的中间层进行联系(不需要多层转换)，因而该虚拟化技术 的具有很好的工作效率，能够非常有效地使用物理资源，进而能够让该虚拟机系 统具有与物理机操作系统很相似的性能。
Physical; Hdrdwai?©

图2-2Hyper-V架构图⑶
Hyper-V 可以支持 Windows Server 2003 SP2、Novell SUSE Linux Enterprise Server 10 SP1、Windows Vista SP1 (x86)和 Windows XP SP3 (x86)等。此外 Hyper-V还能够实现远程管理，可以在客户机打开Hyper-V控制台对远端的服务 进行操作。
4) KVM
KVM (Kernel-based Virtual Machine)是一款完全免费开放的实现虚拟化的 虚拟机系统软件，而且在后来被加入到Linux2.6.20内核系统中，成为Linux内 核系统模块的一部分。
KVM的架构如图2-3所示oKVM实现的硬件平台虚拟化方案是利用Intel VT 技术来完成的。同Xen一样，KVM也利用QEMU中间层来实现物理硬件资源 的虚拟化。


图2-3KVM架构图可
从架构上讲，由于最开始Linux的开发人员并没有专门的实现对虚拟化技术 的兼容性，而KVM的实现方式又是基于内核来完成的，因此便有说法认为KVM 是宿主机模型。然而，由于虚拟化技术的发展，大量的虚拟化技术和功能集成到 Linux内核系统中，因此有人开始认为Linux内核系统越来越像Hypervisor监视 平台，所以，KVM也可以被认为Hypervisor监控模型。
因此，KVM只能够运行在操作系统为Linux的宿主机平台上，但是其所运 行的客户机所安装的操作系统非常丰富，可以是Linux> Windows> Solaris和BSD 等。
能够与Linux内核系统具有相当高的兼容性，使得KVM能够保留下Linux 内核系统的相当多的功能，这是KVM虚拟化软件的最突岀的一点。此外，作为 一款免费的开放系统软件，KVM同时具有相当好的可扩展行和迁移性能。
2.1.2虚拟化技术
狭义的虚拟化技术指的是将那些资源计算处理单元运行在虚拟出来的系统 平台上，而突破实际的物理硬件平台的限制的技术。虚拟化技术指把整个物理硬 件基础包括处理器、内存和外设作为处理资源，在物理硬件平台上，虚拟化出多 个虚拟系统平台，每个系统平台都能够进行实现自身的操作而相互之间没有干扰。 在基于硬件物理平台的虚拟化技术中，往往需要通过加入一个叫做虚拟机监控器 (Virtual Machine Monitor, VMM)的虚拟出来的中间层，该层也叫做Hypervisor。 Hypervisor所在的物理硬件操作系统被叫做宿主机。经过虚拟化技术得到的虚拟 的资源处理基础一般被叫做客户机，其上所安装的系统也相应的被叫做客户机操 作系统。
将虚拟化类型通过VMM的不同来进行区分，可以将虚拟化技术分为完全虚 拟化、半虚拟化和操作系统级虚拟化等不同的几种类别⑶。
a)完全虚拟化
完全虚拟化是目前在实践生产使用中最广泛被人们所接受的技术。通过完全 虚拟化技术得到的虚拟的计算处理基础资源和真是的物理硬件基础资源是相同
10
的，其上所允许的客户机操作系统不需要做其他改到就能够正常运转。在这种模 式下，客户机操作系统并不清楚自身正在一个虚拟出来的处理资源上运转，会像 处理正常的硬件处理资源上一样来处理虚拟处理器、虚拟内存和虚拟I/O设备。 在虚拟机（“guest”，客户机）和硬件之间，安装了一个“Hypervisor （超级管理 器）”，它也叫做一个虚拟机监控器。
这样的Hypervisor也称为裸机Hypervisor,因为他们控制着物理硬件。 Hypervisor给主机的所有真实的物理处理资源提供了一个模拟层。guest（客户机） 操作系统无需做修改，就能直接对虚拟化的硬件发请求，客户机操作系统内核想 要执行的任何有特权的指令都有经过Hypervisor翻译，才能得到正确的处理。
裸机虚拟化算得上是最安全的一种虚拟化技术，因为客户机的操作系统和底 层的硬件设备之间被完全隔离开了。此外，客户机操作系统的内核不要求做任何 修改，所以可以在不同的底层体系结构之间移植客户机操作系统。只要有虚拟化 软件，那么客户机就能在任何体系结构的处理器上运行，不过，翻译CPU指令 会带来一定的性能损失。
VMware ESX就是一种目前非常流行的实现完全虚拟化技术的例子。这类的 基于完全虚拟化技术的系统的一般架构如图2-4所示。

b）半虚拟化
半虚拟化是被Xen所釆用的策略，而免费开放的虚拟社区的领头羊又是Xeno 半虚拟化和完全虚拟化一样，也支持客户在单台硬件平台上并行运行多种不同的 操作系统。不过，必须修改每个操作系统内核以支持“ hypercall",即对某些敏 感的CPU指令进行翻译。但是，客户机上的软件程序不必修改，就可以方便地 在Xen平台上运转。同完全虚拟化相类似,在半虚拟化中也需要用一个Hypervisor 层来进行监控。
半虚拟化系统中的翻译层没有完全虚拟化系统中的翻译层开销大，所以半虚 拟化确实能够在性能上获得名义上的好处。不过，由于需要修改客户机操作系统， 所以这是非常不好的一点，Xen的半虚拟化之所以在Linux和其他开源内核之外 很少有应用，主要就是由于这个缘故造成的。
图2-5所展示的是一个半虚拟化的系统环境。它看上去和图2-4的完全虚拟 化系统很相似，但是客户机操作系统要通过一个定义好的接口才能与Hypervisor 打交道，而且第一个客户机系统有特权。



c）操作系统级虚拟化
操作系统级别的虚拟化技术跟前面介绍的两种不同的模型相比，都具有非常 大的区别。操作系统级的虚拟化并不在物理操作系统里创建多个虚拟机环境，而 是让一个操作系统创建多个彼此独立的应用环境，这些应用环境都访问同一个内 核。操作系统级的虚拟化技术更适合于被想象成为是内核部分中的一个功能模块, 而不是独立岀来的一层软件抽象。
由于该技术的实现中并没有真实存在的翻译功能模块或者是虚拟出来的中 间层，所以操作系统级别的虚拟化技术的开销会是非常小的。因此，该技术的大 部分实现模型都是能够达到本原的性能级别。不巧的是，这种类型的虚拟化不能 使用多种操作系统，因为所有的客户机（在这里通常称它们为“container”）都 共享一个内核。AIX 的"workload partition”和 Solaris 的 acontainer 和 zone"都 是操作系统级的虚拟化技术的例子。
操作系统级别的虚拟化结构如图2-6所示。
12


图2-6操作系统级虚拟化的体系结构⑺
2.2虚拟机的动态迁移技术
基于虚拟机的动态迁移技术是要实现在能够保证虚拟机处于正常运转过程 中，把虚拟机上所安装的操作系统和其上面正在进行的正常业务处理当成迁移的 单元从源端的服务主机上迁移到目的端的服务主机，并且在该虚拟机中正常进行 的业务处理操作要一直能够保证对用户发来的业务需求进行处理和反馈；在迁移 操作完成之后，之前在源服务器上所进行业务处理操作能够在迁移后的服务主机 上重新建立起来。同时，为了能够让迁移后的虚拟机与迁移之前的虚拟机保持一 致的状态，而且能够继续往下执行，就必须传递必要的网络状态信息、存储状态 信息和关键的运行状态信息。在针对这些数据和状态信息的所进行迁移操作中， 最困难而复杂的部分当属是对不断发生变化的数据和内存状态的信息的所进行 迁移操作。下面将对以上三种迁移类型进行介绍【8】。
2.2.1网络的迁移
迁移操作完成之后，源主机中的虚拟机将会被删除，虚拟机所有的网络通信 都应该不再需要源主机的中转，而可以直接与目的主机进行。目的端虚拟机会拥 有迁移之前源端虚拟机的网络状态，以接收新进来的任务或者信息等。在对虚拟 机系统进行迁移操作之前，要对源端主机的系统状态信息进行必要的封装，其中 包括系统的网络链接状态和IP地址信息等相关的网络协议的状态信息。但是， 针对多种多样的复杂网络环境，可以制定釆取相应的解决方案⑼。例如：在局域 网内，需要将源端主机系统的网络IP地址跟目的端主机系统的MAC硬件地址 进行绑定，源端主机需要向目的端主机主动地发送ARP重定向包来进行绑定， 完成之后，新来的数据包将会被直接发送到目的端的主机。在广域网的情形下， 由于在迁移操作完成以后，虚拟机的网络IP地址将会重新获取，如果IP地址出 现变化，将会导致在迁移发生之前的建立起来的网络将会断开。因此这个问题需 要通过配置虚拟网卡，设置虚拟的网络端口 IP地址的方法来进行处理。
13
2.2.2存储的迁移
由于虚拟出来的存储单元中存在大量数据信息，所以针对存储单元的迁移操 作将会占用大量的时间和网络带宽，而导致迁移效率极低，而且也很少有需要物 理主机之间迁移大量存储设备的应用场景。为了提高迁移效率，减少迁移过程中 时间，空间与带宽方面的浪费，最常用的处理方法是利用NAS( Network Attached Storage)来充当存储单元来保存数据，实现文件系统的共享，减少迁移的开销。 由于在迁移的过程中，共享的数据和文件系统不需要进行迁移，所以可以节省大 量的时间和带宽，从而会大幅度的提升迁移的效率。因此，为了减少迁移操作所 花费的时间，存储单元必须使用实现文件系统共享。
2.2.3内存的迁移
内存的迁移是虚拟机动态迁移中最难而且最重要的部分，不仅因为存储内存 数据的结构本身很复杂，而且这些内存数据本身也非常重要，决定了迁移之后， 客户机能否恢复。从理论上讲，在完成整个虚拟机的整体的动态迁移操作，其中 需要对内存部分进行专门的迁移操作，该操作流程大体上会包括三个不同部分： Push阶段，Stop-and-Copy阶段和Pull阶段。接下来会对三个不同部分作简单描 述阴:
•Push阶段：在“推送”阶段，如果在虚拟机运行的过程中，虚拟机的内 存页面如果满足迁移的要求，那么就会把这些符合要求的内存页通过网 络从源端主机迁移到目的端主机。但是在迁移的过程中，虚拟机的内存 页面可能会继续被改写，因此需要采用某种适当的策略把那些仍然符合 迁移要求的内存页面发送到目的端主机。
•Stop-and-Copy阶段：在“停机复制”阶段，在源端主机上运行的虚拟机 将会根据不同的迁移策略把源端主机上的相应的虚拟机内存数据发送到 目的端主机。例如在预拷贝算法中，在“停机复制”阶段，发送那些剩 余的未同步的内存脏页。在传送完毕这些内存脏页之后，目的端系统中 的虚拟主机的状态能够与源端系统中的虚拟主机达到同步，这时候会在 目的端系统上的建立起虚拟机，并将原来的创建的虚拟机所占有的资源 释放掉。
•Pull阶段：在“拉取”阶段，如果发现在“停机复制”阶段建立在目的 端主机上并且己经启动的虚拟机出现有内存页面被访问但还没有被复制 传送过来，那么在此阶段，会从源端主机将对应位置上的该页面复制过 来。
2.3本章小结
本章详细的介绍了现今的商业市场上有关虚拟机系统和基于此的动态迁移 方面的应用现状。虽然虚拟机系统和基于虚拟机系统而建立起来的动态迁移机制 己经相对比较成熟，而且在虚拟化和云计算方面都有一定的应用，而且也呈现向 上发展的趋势，但是，这些应用方面的拥有一个共同的特点，就是对迁移的实时 性有要求，但不是十分严格。因此，当处于这样的应用情景下，利用虚拟机系统
14
的实现的动态迁移机制就已经足以胜任了。
当迁移的客户机代表的是通信领域中某个实时信号处理单元的时候，由于通 信系统中对实时性的要求要远远高于其他领域，甚至到秒级和毫秒级。因此，为 了达到通信系统中对实时性的要求，必须抛开虚拟机系统这层延时比较大的环节, 而直接对实时信号处理单元进行相应的迁移处理，将会大大提高迁移的效率和迁 移的实时性。本文接下来将全面介绍应用于通信系统中的具有高实时性的动态迁 移方案。
15

第三章 实时信号处理的动态迁移技术的设计与实现
3.1实时信号处理的动态迁移技术概述
基于虚拟机系统来对计算处理单元进行动态迁移的技术巳经发展的相对比 较程序，市场上也有很多相应的产品和应用存在，例如分布式集群，云计算处理 资源池等。但是，这些商业级的产品和应用的一个共同特点就是对实时性要求不 是很严格。只要能够迁移成功，保证迁移数据的正确性和可恢复性，在几十秒或 者分钟的量级上都是可以接受的。因此现有的成熟技术和方案都是针对实时性不 高的应用进行设计和实现的。
在通信领域，对实时性的要求非常严格，甚至精确到毫秒级别。在这样严格 的实时性要求下，如果还应用虚拟机系统来进行动态迁移方案的设计，势必会有 相当大的延时，将会达到用户无法容忍的程度。因此需要设计一套全新的，抛开 虚拟机系统，具有实时性的动态迁移技术方案，使迁移过程中用户的宕机时间控 制在几秒之内，达到用户可以忍受的程度。
因此为了实现上述的动态迁移的目标，本动态迁移技术方案利用SDR的无 线射频终端进行通信数据业务的接收和转发，利用IBM Server来运行实时信号 处理单元，对通信业务数据进行处理，使实时信号处理单元能够在不同的IBM Server进行动态迁移。方案实现图如图3-1所示，



图3-1实时信号处理单元的动态迁移技术方案
图3-1中所示，通信业务数据从数据业务发起端通过射频端和无线通信链路 传输到数据业务接收端，在数据业务接收端，通过千兆交换机将不同的IBM Server连接在一起，共同组成一个计算处理资源池。实时信号处理程序可以在计 算处理资源池中根据一定的策略进行动态迁移，从而达到资源利用最大化。
为了通过该方案实现计算处理单元的动态迁移，主要需要解决一下两个技术 难点：
16
1、	通过SDR设备的通信业务数据要能够进行热切换，实现可以同 Serverl进行数据业务交互切换到同Server2进行数据业务交互，以保 证迁移前后数据不中断，使通信业务数据的丢失尽可能得少。
2、	实现实时信号处理程序能够在合理的，且耗时最小的情况下，由 Serverl计算处理资源上迁移到Server2计算处理资源上，能够在 Server2尽快的恢复迁移之前的处理业务，使中间的业务中断宕机时 间在可忍受的时间范围内。
本章将针对上述两个技术难点进行全面而详细的动态迁移技术的讨论和研 究，提出相应的解决方案并实现。将根据不同的部分提出不同的动态迁移概念， 并加以分析与实现，其分为三个组成部分：
1）	数据流动态迁移：为了保证迁移的高实时性和通信数据业务的连续性， 需要在迁移的过程中，将数据流从迁移发起端无间断的迁移到迁移冃的 端。
2）	控制信息流动态迁移：为了使实时信号处理业务能够在迁移目的端快速 恢复起来，实现迁移前的业务处理功能。因此，需要将业务处理中固定 的或者短时不会发生变化的控制信息以及一些系统信息打包迁移到迁移 目的端，供迁移目的端快速恢复业务时使用，以缩短迁移耗费时间，提 高迁移效率。
3）	信号处理任务动态迁移：信号处理任务是整个业务处理的核心程序，该 任务的迁移必将致使信号处理业务出现中断。因此，对信号处理任务的 动态迁移方案必须进行精心的设计，使信号处理任务能够尽快的在迁移 目的端恢复起来，减少业务处理的中断时间，以提高动态迁移的总体性 能和效率。
3.2基于SDR设备的数据流动态迁移技术研究
在实现实时信号处理单元的动态迁移技术之前，首先要能够实现射频端的通 信业务数据流的实时热迁移，实现数据流不中断的从一个实时信号处理单元动态 迁移到另一个实时信号处理单元上去，从而保证迁移前后数据业务不中断，使通 信业务数据丢失的尽可能少。
为了射频端的通信业务数据流的实现热切换，需要根据实时信号处理的动态 迁移方案对SDR设备内部驱动进行一定改造与设计。由于本文主要使用的是 SDR家族中的USRP设备，所以实时信号处理的动态迁移技术都是在USRP设 备的基础上实现完成的，因此本文的所有数据流的切换都是针对USRP设备实现 的，下面的介绍也仅针对USRP设备。
3.2.1 USRP的工作原理研究
USRP是Universal Software Radio Peripheral的简称'⑴，是目前主流的通用软 件无线电平台，可以用于无线电技术的研究与开发。USRP由射频子板和母板构 成，射频子板用于实现模拟射频信号的放大与发送的功能，根据不同的需求，有 不同的射频子板可供选择；而母板主要用来完成ADC/DAC的变换，以及DDC （数字下变频）/DUC （数字上变频）的功能，将I/O数据传送给通用处理器来
17

处理。USRP母板也有多种系列，本文釆用的是USRP系列中性能最好的，处理 能力也是最强的母板，属于高端产品的USRP N210母板（下面在不发生混淆的 情况下，简称为USRP）。
3.2.1.1 USRP N210 母板
如图3-2所示，整个USRPN210系列的母板被分成几个主要的功能模块
RAM




Clock
generat
——Gtf
图3-2USRPN210母板架构图
•FPGA模块：FPGA模块在USRP系统中的作用是非常关键的，其能够 完成高速的运算。FPGA模块所完成主要工作是执行高速的数学运算， 并利用DDC模块把信号从中频频段搬移到基带上，然后对信号进行采 样以降低数据的传输速率，使其可以在千兆以太网上进行传送，并适应 于计算处理服务器端的计算能力。在发送路径行，FPGA模块的情况几 乎与接收路径上的一样，除了其中利用DUC （数字上变频器）来实现。
•DAC/ADC模块：USRP有4个高速的12位模拟数字转换器，采样速率 为64M/s,因此USRP可以数字化的带宽为32MHz；在带通滤波上，A/D 转换器可以完成大约200MHz的采样信号。在传输路径上有4路高速的 14位数模转换器，它们的时钟频率为128MHz,因此可以计算岀来奈奎 斯特频率为64MHz。为了让滤波容易一些，使用低于该数值的频率，因 此提供了直流到44MHz的输出频率。
•片外存储器模块：USRP的片外存储器分为三种，一种是FLASH芯片, 用来存数FPGA的相关配置文件和ZPU的firmware程序；另外一种是 EEPROM,用来母板的一些基本信息；最后一种是SDRAM,用来作为 发送链路I/O数据的缓冲FIFOo
18
•以太网物理层模块：以太网物理层模块使用ET1011C2千兆以太网芯片， 这样就要求必须使用千兆以上带宽的链路与USRP N210母板进行连接。 和FPGA模块连接的接口是GMII接口。
•辅助I/O数据接口： USRP母板上有一个高速的64位的数字I/O接口， 其中32位被用于IO_TX, 32位被用于IO_RXo
USRPN210母板的整个工作流程为：USRP上点之后，FPGA会从外部的 FLASH存储器中读取相应的配置文件和ZPU的firmware相关程序，然后进行初始 化操作，完成对母板和子板上的其他相关芯片的配置，接下来通过千兆以太网和 IBM Server建立起I/O数据传输链路。当USRP母板通过千兆以太网接收到从IBM Server端发送过来的I/Q数据后，对其进行DUC操作和滤波处理，再传送到DAC 进行相应的数模变换，然后利用射频子板模块对DAC输出的模拟信号进行混频、 放大等处理操作，最后从空口天线端发射出去。反过来，当USRP接收到IBM Server发起的接收操作请求，从ADC模块得到射频子板模块传送过来的中频信号, 然后再进行DDC操作和滤波处理，接下来进行打包组帧，然后将该数字信号通 过千兆以太网发送到IBM Server端。另外，FPGA还需要不断的维护相应的发送 接收的定时器，以便于IBM Server能够通过相应的时间戳的机制来实现空口同步。
USRPN210的具体工作流程示意图如图3-3所示。
19

USRP设备上电



▼
FPGA定时器 打时间戳		USRP设备 与IBM Server•进 行数据链路链接	V千兆以太网A	IBM Server
w

DUC操作与
滤波处理




子板模块处理
和ADC转换
天线发射与
接收
((?))
图3-3USRPN210母板工作流程示意图
FPGA子系统是USRP设备中最重要，也是完成主要功能的模块。FPGA模块内 部包含了一个SOPC系统，该系统内部包含了不同的功能模块来完成各自的功能, 其中主要的子模块有以太网子模块，ZPU子模块，DSP链路子模块和VITA控制
20
子模块等，各个子模块之间的通信是通过wishbone来完成的。通过各个不同子 模块之间的相互协作，FPGA子系统完成了整个系统的主要配置工作和收发数据 工作。USRPN210 FPGA子系统数据链路架构如图3-4所示。




图3-4USRPN210 FPGA子系统数据链路图
3.2.1.2 USRP 与 UHD 驱动
UHD (Universal Hardware Driver)是用在 USRP 和 IBM Server 之间的驱动接 口程序，它主要是利用C++和Boost库进行编写的。UHD通过将发送和接收数 据、相关的读写配置等操作封装成为API,然后调用操作系统的函数来实现。图 3-5说明了 UHD在系统中的位置。
21


UHD API
UDP Socket,
libsub
or others
Gigabit Ethernet,
USB 2.0 or
Memory interface
图3-5基于UHD的USRP的系统架构图
UHD通过不同的API接口函数，实现对USRP的配置，完成与USRP设备 之间的同步，实现USRP设备与IBM Server之间的数据通信等功能。利用UHD 来进行USRP设备的开发，都是通过调用UHD提供的丰富的API接口函数来完 成的，为了拥有较高的执行效率，其中所有的数据处理传输的开发工作都是利用 C/C++来实现的〔"I。
在IBM Server中，通过UHD启动整个USRP程序进程的大概工作流程如图 3-6所示：
22

开辟本地缓存 buffer和定义元 数据




探测USRP设备
I
与USRP设备进
行链接
I
通过UHD对
USRP的相关参
数进行配置
1	r
TX	调用send() 发送数据		调用recv() 接收数据	RX

图3-6UHD启动USRP进程的主要工作流程示意图
•首先通过UHD的API接口函数在IBM Server构造一个USRP的实体， 构造函数命令如下：
uhd:: usrp:: multi_usrp:: sptr usrp=uhd::usrp::multi_usrp: :make(args); 其中args表示USRP的相关参数。
•发送和接收数据之前，要先开辟一段缓存空间用来存放待发送处理或者 待接收处理的数据，同时还需要定义元数据用来存储发送接收数据起始 位置和错误类型标识。
•在USRP实体与USRP设备建立数据链路链接之前，需要先探测确定 USRP设备是否存在，只有当探测到USRP设备的存在的时候，才能建 立起USRP实体与USRP设备之间的数据链路链接，探测USRP设备的 指令为：
uhd_find_devices
•通过调用UHD的API接口函数逼立起USRP实体与USRP设备之间的
23
数据链路链接，建立起USRP实体与USRP设备链接的API接口函数如 下： uhd::usrp::device::sptrusrp_device:=uhd::usrp::multi_usrp::get_device(void);
•然后再调用UHD的API /口函数对USRP设备晶工作参薮进行配置。
USRP上电的时候，会进行一些固件的配置，这些配置是USRP内部自 行完成，无需额外操作。但是，有一些参数如中心频率、釆样率、增益 等则需要通过调用UHD的API接口函数来进行相应的配置。表3-1列 举了部分可配置的主要参数"】：
表3-1UHD可配置的部分参数及功能
配置参数	参数说明
-freq	中心频率
-ant	天线，选择TX/RX
-args	地址参数(192.168.10.2)
—gain	子板收发增益(单位dB)
-ref	参考时钟(内部、外部、MIMO)
—bw	子板带宽
-rate	PC机接收到的釆样速率的要求

•初始化工作都做好之后，然后就可以进行正常的发送与接收数据操作了。 UHD中执行发送数据与接收数据API接口函数分别为send()和recv()。
3.2.2USRP数据流的热切换技术研究
3.2.2.1USRP数据流热切换的意义
根据USRP的工作原理可知，当USRP上电之后，会首先进行一系列的初始 化操作，然后再与IBM Server之间建立I/O数据传输链路连接。由于USRP的初 始化操作要进行一系列比较复杂的操作来完成相应的硬件配置等，所以会耗费巨 大的时间。经过大量的测试，无论是上电之后的第一次初始化还是重置的再次初 始化，都能够达到2~3s左右的耗时，而且USRP进行了一次初始化之后，相应 的初始化配置几乎不会再发生改变。
在动态迁移的过程中，其中USRP实现的只是将数据流链接从一个IBM Server迁移到另外一个IBM Server上去，而且迁移中所使用的IBM Server的配 置都是完全一样的。在迁移过程中，USRP设备一直保持通电状态，所以完全无 需再对USRP进行二次初始化操作，只需实现将USRP设备与迁移发起端服务器 的数据链路断开，而与迁移目的端服务器建立起数据链路即可，这样将会避免掉 USRP设备二次初始化所耗费的巨大时间，从而大大提高了动态迁移整体的实时 性和迁移效率。
3.2.2.2USRP数据流的热切换技术实现
为了实现USRP数据流的热切换，缩短USRP在动态迁移过程中所耗费的时 间，因此就需要去除掉USRP在迁移中的二次初始化的耗时，让USRP设备只在 上电的之后进行一次初始化操作，之后再不必重新进行初始化操作。
24
USRP设备在上电的时候，本身固件会进行初始化，这种初始化操作，只在 上电的时候才进行一次，所以只要保证USRP设备一直在通电状态就可以避免该 初始化的再次进行。
另外一种初始化是在IBM Server每次启动USRP进程都会通过调用UHD的 API接口函数对USRP设备进行相应的初始化配置。为了在迁移过程中避免重复 初始化，需要对USRP进程的启动操作流程进行改造，使其在启动USRP进程的 过程中，不再对USRP设备进行初始化配置操作。
首先将UHD中对USRP的工作参数进行初始化配置的相关API接口函数提 取出来，进行封装，封装函数内部包含一系列通过UHD的API接口函数对USRP 设备的初始化配置操作，其函数定义如下：
uhd_device::initO
一些主要的配置参数的API接口函数如表3-2所示。
表3-2主要的UHD配置参数的API接口函数
API接口函数	功能描述
set_master_clock_rate()	设置主参考时钟频率
setrxrateO	设置PC机接收端釆样速率
se七rx_freqO	设置接收端中心频率
set_rx_gainO	设置接收增益
set_rx_antenna()	选择接收天线
set_rx_bandwidth()	设置接收带宽
set_tx_rate()	设置PC机发送端采样速率
set_tx_freq()	设置发送端中心频率
set_tx_gainQ	设置发送增益
set_tx_antenna()	选择发送天线
set tx bandwidth()	设置发送带宽
其中各API接口函数都有不同的相应的形参值和返回值，在表中省略了，可 以根据实际业务场景需要进行不同的配置设定。
然后只在第一次启动USRP进程的过程中调用一次该封装的初始化函数。在 迁移目的端启动USRP进程时，将其中的初始化部分的封装函数屏蔽。当USRP 实体与USRP设备通过API接口函数建立起链接之后，直接开始进行正常的数 据业务通信。实时信号处理程序从IBM Serverl上迁移到IBM Server?上的过程 中，USRP数据流的热切换流程示意图如图3-7所示。
25

IBM Serverl ['-启动一
\uSRP进程
与USRP进
行数据通信
图3-7USRP数据流热切换流程示意图
图3-7中所示的USRP数据流的热切换的具体流程说明如下：
1）	在迁移发起端（即IBM Serverl）,启动USRP进程。USRP进程的启动 如图3-6所示。USRP进程启动完成之后，就建立起了迁移发起端上的 USRP进程与USRP设备之间得数据链路链接，即图3-7中所示的红色的 数据链路1。这样，迁移发起端上的实时信号处理程序就可以通过USRP 进程与USRP设备进行数据通信。
2）	当迁移发起端发起了动态迁移业务之后，向迁移目的端（即IBM Server2） 发送迁移指令，迁移目的端首先完成图3-7中黄色矩形框中的操作流程， 创建一个USRP实体，并开辟相应的缓存buffer空间和定义元数据。在 动态迁移系统中，简化成为创建USRP实体。
3）	迁移目的端的USRP实体创建成功之后，需要断开迁移发起端中的USRP 进程与USRP设备之间的数据链路，即断开图3-7中的红色的数据链路1, 具体的断开操作涉及到一些修改端口 IP等操作，在动态迁移系统的设计 中再详细介绍。
4）	迁移发起端与USRP设备之间的数据链路断开之后，迁移目的端进行图 3-7中橙色矩形框中的操作流程，调用UHD的API接口函数get_device 建立起USRP进程与USRP设备之间的数据链路链接，即图3-7中的蓝 色的数据链路2。在后续的动态迁移系统中，简化成为建立USRP数据 链接。在调用API接口函数建立数据链接之前，还涉及到一些修改端口 IP等操作，在动态迁移系统的设计中再详细介绍。
26

在动态迁移过程中，经过上述的USRP数据流的热切换之后，IBM Server2 上的实时信号处理程序就可以通过蓝色的数据链路2与USRP设备进行数据链路 通信了。至此，就完成了在动态迁移过程中，USRP设备的数据流的热切换。
3.2.3 USRP数据流热切换在动态迁移中的必要性
由于USRP设备的数据流的热切换的实现，节省了 USRP再次初始化所耗费 的巨大时间，这将大大降低迁移目的端的USRP进程与USRP设备建立数据链路 的时间，从而降低的动态迁移过程的整体时间消耗，提高了整个迁移过程的实时 性以及迁移效率。因此，为了满足实时信号处理的实时性要求，提高动态迁移的 实时性，实现USRP设备数据流的热切换技术就成为了必然。
3.3实时信号处理的动态迁移系统整体设计
3.3.1实时信号处理的动态迁移系统架构设计
针对通信业务中具有高实时性要求的实时信号处理业务，设计的动态迁移方 案架构图如图3-8所示。
端口 eth3|C^]
IBM Server2
数据业务接收端
图3-8实时信号处理的动态迁移系统方案示意图
针对动态迁移系统方案有以下几点需要说明：
•本系统设计中的实时信号处理程序使用的是基于LTE通信协议的D2D 信号处理程序【削。该实时信号处理程序既能够在基站覆盖下，与基站进 行同步，使不同D2D终端以基站为同步源，实现不同终端之间的数据业 务通信；又能够在非基站覆盖下，在不同D2D终端之间自主分配主从关 系，建立局部通信网，实现不同D2D终端之间的数据业务通信。
• 将IBM Server与USRP设备进行物理连接的端口都配置为eth3,如图3-8 所示。由于USRP设备的特殊要求，只能够识别与其相连，且端口 ip为 192.168.10.1的IBM Server建立链接和进行数据通信。在迁移系统中需 要针对USRP设备的这个特性进行专门的设计。
27
•数据业务接收端（IBM Server 1或者IBM Server2）利用虚拟网卡ip地址 作为与其他终端进行通信的地址。因此，在迁移过程中，需要将迁移发 起端的虚拟网卡ip地址发送到迁移目的端，以保证迁移前后，数据业务 接收端具有相同的地址配置，使数据业务处理能够继续进行。在动态迁 移系统中，虚拟网卡的ip地址获取与配置利用“ifconfig”指令来完成， 并将获取的ip地址保存在变量“tun_ip”中。
获取虚拟网卡ip指令：ifconfig tun 配置虚拟网卡ip指令：ifconfig tun tun ip
•数据业务发起端（IBM Server3）通过ping数据业寡与数据业务接收端进 行通信，以验证动态迁移的性能。
•整个动态迁移系统控制架构是通过Python脚本编写而成卩％,由迁移发起 端和迁移目的端两部分控制程序组成。下两节将会进行详细的介绍。
•动态迁移系统中，不同Server之间通过ZMQ通信机制在局域网内进行 通信与控制信令传输㈤〕。
本系统首先建立起两个D2D终端（即IBM Server3与IBM Serverl两个计算 处理单元）之间的数据通信业务链路的链接，并利用IBM Server3发起ping数据 业务，与IBM Serverl进行数据业务通信。在ping数据业务通信进行中，利用该 动态迁移系统将D2D实时信号处理程序从IBM Serverl迁移到IBM Server2上， 并继续进行与IBM Server3之间的ping数据业务通信。在迁移过程中，IBM Server3上的业务会出现不影响用户体验的短暂的暂停现象，随之马上又恢复正 常业务通信。对于IBM Server3并不会觉察到与自身进行数据业务通信IBM Server已经发生变化。
3.3.2动态迁移系统中端口 ip配置方案设计
由于USRP只能识别与之相连的端口 ip为192.168.10.1的IBM Server,所以 要保证多个通过千兆交换机与USRP设备进行物理相连的端口（即图3-8中的 eth3端口）中，只能有一个端口的ip为192.168.10.1,否则会产生ip冲突，导致 出现多个eth3端口 ip都为192.168.10.1的不同IBM Server中的USRP实体请求 与USRP设备进行数据连接的情况，这样USRP将无法决定到底跟哪一个IBM Server中的USRP实体进行连接，进而就会导致IBM Server中的USRP实体连 接失败，导致整个USRP进程失败。所以在同一时间只能有一个eth3端口 ip为 192.168.10.1的IBM Server中的USRP实体请求与USRP设备进行数据连接，这 时USRP设备才会确定与唯一的USRP实体建立起数据链路连接。
由于USRP设备的这个特性，所以在动态迁移系统中，就需要动态的修改与 IBM Server的eth3端口的ip,以实现USRP实体与USRP设备之间数据链路的 建立与断开来满足迁移的需要。
在动态迁移过程中，动态迁移系统首先会将迁移发起端的IBM Server 1中通 过千兆交换机与USRP设备相连的eth3端口的ip配置为192.168.10.1,其他暂时 不希望与USRP设备建立数据链路链接的IBM Server （即迁移目的端的IBM Server2）的eth3端口 ip配置为其他子网字段（本系统采用192.168.20.*子网字 段）。
当迁移发生时，动态迁移系统会将当前与USRP设备进行连接的IBM Serverl
28
的eth3端口 ip修改为其他子网字段(192.168.20.*),然后把迁移目的端IBM Server2的eth3端口 ip修改为192.168.10.1,这样可以避免在与USRP设备有物 理连接的端口中同时出现多个ip为192.168.10.1的情况，而解决了由于ip冲突 的导致USRP设备错乱的问题。具体的修改ip逻辑顺序等细节问题将会在动态 迁移系统的设计一节进行详细的介绍。
3.3.3实时信号处理控制信息流动态迁移的设计
为了使针对D2D信号处理的程序能够在迁移冃的端(IBM Server2)上尽快 的恢复之前的数据处理业务，所以在D2D程序从IBM Serverl迁移到IBM Server2 的过程中，首先将尽可能多的，短时间内基本不会发生改变的控制信息先发送到 IBM Server?,这样会帮助D2D程序快速地恢复起来，建立起与业务发起端(IBM Server3)之间的业务数据链路链接，缩短IBM Server3上的数据处理业务中断时 间，提高动态迁移的总体效率。
在迁移发起端的动态迁移系统中，创建一个单独的线程定期从D2D信号处 理程序中收集相关的控制信息，并维护更新迁移发起端迁移系统中相应的标志信 息变量。针对该D2D信号处理程序，动态迁移系统需要维护的主要标志信息的 变量如表3-3所示[2珥，当使用其他实时信号处理程序，将根据其特点维护不同的 控制信息变量表。
表3-3动态迁移系统中D2D程序控制信息变量表
标志信息变量	变量描述
workmode	工作模式：0-主机模式，1-从机模式，2-基站模式
phy_cell_ID	物理层小区id,范围：0-503
group ID	小区组id,范围：0〜167
cell ID	小区组内id,范围：0~2
Cplype	CP (循环前缀类型)：0为normal, 1为extended
BW	系统带宽：5,10,20,单位为MHz
RBIdx	系统资源RB个数
duplexMode	通信制式：0-TDD, 1-FDD
numAntennaTX	发送天线个数：1,2,4
numAntennaRX	接收天线个数：1,2,4
DLULAssign	上下行子帧配置
trans_mode	传输模式
lype_mode	编码方式：1-QPSK, 2-16QAM, 3-64QAM
RNTI	RNTI类型
表3-3中描述的参数变量及;	圭值，在动态迁移系统中被保存在Python的字典结
构变量Control中，其保存形式如下：
Control [ucontrol_val_name,^] = control_val_value
当动态迁移发起时，动态迁iW会首先将系统矫维弟的这些与D2D实时 信号处理程序控制信息相关的标志变量从迁移发起端IBM Serverl发送到迁移目 的端IBM Server2, IBM Server2上也会有相应的变量来存储这些信息，然后在启 动D2D实时信号处理程序进程的时候,将这些控制信息变量传送到D2D程序中， 使D2D程序可以直接利用这些已有的控制信息来更快地恢复数据业务处理能力,
29
避免D2D实时信号处理程序重新在接收数据中检测和计算这些控制信息所耗费 的时间，从而缩短业务发起终端(IBM Server3)的数据业务中断的时间，提高 整个动态迁移过程的实时性和迁移效率。
在向迁移目的端发送控制信息的同时，也将发送控制信息标志位，用来表征 此次启动的D2D进程是否包含控制信息。标志位表示如下：
control_flag=0 表示启动不包含控制信息D2D进程 control_flag=l表示启动包含控制信息的D2D进程 在动态迁移系统设计二苇中将会详细介绍控制信息的迁移流程。
3.3.4动态迁移系统中执行函数简介
动态迁移系统中，在迁移发起端和迁移目的端都会经常用到一些执行命令， 因此利用python编写了一些公用的执行函数如下：
•def cmdProcess(cmd,cmd_label):
该函数执行输入命令Zmd,并返回执行结果是成功(True)还是失败(False) 输入cmd是所要执行的系统命令，类型为字符串；cmd_label是执行命令 的说明，用于打印，类型为字符串。
•def getProcess(cmdname):
该函数获取输入参数cmdname的进程id,如果执行成功则返回进程id, 失败返回Falseo 输入参数cmdname为所要获取进程id的进程名，类型为字符串。
•def killProcess(cmdname):
该函数杀死输入进程。成功返回True,失败返回False
输入参数cmdname为进程名，类型为字符串。
3.4迁移发起端实时信号任务动态迁移的设计
在迁移发起端IBM Server!,通过Python脚本编写动态迁移系统的迁移发起 端的主控程序，用来调度迁移发起端的不同部分之间相互配合以完成不同的功能, 并通过ZMQ通信机制与迁移目的端进行通信，使动态迁移系统的两部控制程序 之间相互协同工作，完成整个动态迁移任务。
迁移发起端动态迁移系统的主控程序会根据接收到的外部命令(本系统釆用 标准输入终端输入)来跳转执行不同的功能模块，下面将详细介绍主控程序的处 理操作。
3.4.1动态迁移系统的主控程序处理流程
启动迁移发起端(IBMServerl)上的主控程序之后，主控程序会等待标准输 入终端输入控制指令。当接收到输入的控制指令之后，主控程序会根据相应的控 制指令跳转到相应的控制模块进行相应的处理工作。下面介绍控制命令及其相应 的功能实现模块。
当终端输入DS (Double Start的缩写)控制指令之后，主控程序会跳转到启 动IBM Serverl上的实时信号处理程序相关的D2D进程和USRP进程的处理模 块，并进行相应的启动进程处理操作。
30
当终端输入KL （Kill Location的缩写）控制指令之后，主控程序会跳转到结 束IBM Serverl上的实时信号处理程序相关的D2D进程和USRP进程的处理模 块，并进行相应的结束进程处理工作。
当终端输入M或者Migration控制指令之后，主控程序会跳转到启动D2D实 时信号处理程序由IBM Serverl迁移到IBM Server?上的功能模块，并进行相应 的迁移处理操作。
当终端输入MB （Migration Back的缩写）控制指令之后，主控程序会跳转 到启动D2D实时信号处理程序由IBM Server?迁移回到IBM Serverl的功能模块, 并进行相应的迁移处理操作。
当终端输入DSR （Double Start Remote的缩写）控制指令之后，主控程序会 跳转到启动迁移目的端（IBM Server2）上的实时信号处理程序相关的D2D进程 和USRP进程的处理模块，并进行相应的启动处理操作。
当终端输入KR （Kill Remote的缩写）控制指令之后，主控程序会跳转到结 束IBM Server2的实时信号处理程序相关的D2D进程和USRP进程的处理模块， 并进行相应的结束进程操作。
当终端输入h或者help控制指令之后，主控程序会打印相关的控制指令说明 信息到标准输出，以提示如何进行操作。
当终端数据end控制指令之后，主控程序将会执行结束IBM Server2上动态 迁移系统的主控程序操作。
当终端输入q或者quit控制指令之后，主控程序结束工作，退出。
迁移发起端主控程序的处理流程过程如图3-9所示，每个模块的处理流程将 在后续分别进行描述。

图3-9迁移发起端动态迁移系统主控程序的处理流程示意图
3.4.2启动实时信号处理相关进程模块
当主控程序接收到标准输入终端输入的控制指令为“DS”时，主控程序会跳 转到启动迁移发起端实时信号处理程序的相关D2D进程和USRP进程的处理模 块，并进行一系列的启动处理操作。
首先修改与交换机相连的网络端口 eth3的ip地址为192.168.10.1,以便能够 与USRP进行数据通信连接。如果修改成功，则可以进行下一步操作；如果修改 失败，则直接结束本次启动工作流程。
当eth3端口的ip修改成功之后，将会调用cmdProcess函数来启动实时信号 处理程序D2D进程。启动D2D进程结束后，将会检测D2D进程是否启动成功。 如果启动成功，则可以进行下一步操作；如果启动失败，则会将之前修改的eth3 端口的ip再修改回192.168.20.2,并直接结束本次启动处理流程。
在D2D进程启动成功之后，然后就开始不断的通过“uhd_find_devices”指 令探测USRP设备的存在。在探测的同时，由于IBMServerl禾断曲向外发送探 测指令，所以也能够让交换机更快的建立起IBM Serverl的ip地址和mac地址 的对应关系，以便能够更快的让网内的设备“知道”自己的存在。如果在规定的 次数内通过“uhd_find_devices”指令探测到USRP设备的存在，就可以建立起 IBM Serverl中的USRP实体与USRP设备之间的数据链路链接；如果在规定的 次数内一直没有检测到USRP设备的存在，则意味着本次启动工作将无法进行, 则将eth3端口的ip修改为192.168.20.2,并同时结束前面已经成功启动的D2D 进程,然后跳出本次启动工作流程。这里的最大检测次数是通过不断测试得出的，
32
定为8次。	-
当确定USRP设备存在并保证可以与之建立通信连接之后，将调用 cmdProcess函数来启动USRP进程（此处的USRP进程启动，是完整的包含初始 化操作的进程启动，启动过程如3.2.1.2小节所介绍），函数处理结束后，将检测 USRP进程是否启动成功。如果USRP进程启动成功，则将打印启动成功报告, 然后创建一个新线程对D2D程序的控制信息进行釆集；如果USRP进程启动失 败，则会先将eth3端口的ip改回到192.168.20.2,然后再结束之前启动的D2D 进程，并结束本次启动处理流程。
在创建的新线程中，动态迁移系统将会每隔10s启动一次采集数据操作，用 来更新系统中用来保存D2D程序控制信息的Control字典变量，使Control中保 存的不同变量对应的参数值保持最新。
迁移发起端IBM Serverl中实时信号处理程序相关进程启动操作流程示意图 如图3-10所小。
33


图3-10迁移发起端实时信号处理程序进程启动流程
3.4.3结束实时信号处理相关进程模块
当主控程序接收到输入终端输入的控制指令为“KL”时，主控程序将会跳转
34 到结束本地IBM Ser/erl实时信号处理程序的相关进程——D2D进程和USRP进 程的处理模块，并进行一系列的结束进程操作。
首先分别调用killProcess函数，结束正在IBM Serverl中运行的与实时信号 处理程序相关的进程一一D2D进程和USRP进程，然后再调用cmdProcess函数， 将IBM Serverl的eth3端口的ip改为192.168.20.2o等待三个函数都执行完毕之 后，再检测三个函数的执行结果，只有当三个函数都返回执行成功的结果之后， 才算是成功执行本次结束相关进程的操作，并打印结束实时信号处理相关进程成 功的报告，然后停止另外一个线程中釆集D2D程序控制信息的操作，并结束该 线程，最后跳出本次执行模块；如果三个函数中有至少一个函数返回执行失败的 结果，那么本次结束相关进程的操作就是失败的，打印失败报告，同时也停止另 外一个线程中釆集D2D程序控制信息的操作，并结束该线程，然后跳出本次执 苻模块，等待下一步的相关命令。
迁移发起端IBM Server中，结束实时信号处理相关的D2D进程和USRP进 程的处理流程示意图如图3-11所示。

图3-11迁移发起端结束实时信号处理相关进程的处理流程

3.4.4实时信号处理迁移功能模块
当主控程序接收到输入终端输入的控制指令为“M"或者"Migration”时,
35
主控程序将会跳转到启动将实时信号处理D2D程序向迁移目的端迁移的处理模 块，并逬行一系列的迁移操作工作。
首先迁移发起端利用ifconfig指令获取本地的虚拟网卡ip地址，并将该地址 保存在动态迁移系统中的tun_ip变量中，然后通过ZMQ通信机制向迁移目的端 发送配置虚拟网卡ip地址的遂制指令一一“set_tun_ip”和需要配置的虚拟网卡 ip值一一“tun_ip”，然后等待迁移目的端返回亙置盘拟网卡ip操作结果。如果 迁移目的端的虚拟网卡ip配置成功，则迁移发起端将会继续迁移的后续操作； 如果迁移目的端返回虚拟网卡ip配置失败的结果，则结束配置迁移操作，并打 印迁移失败报告。
在接收到迁移目的端虚拟网卡ip配置成功的返回结果之后，迁移发起端的主 控程序会继续利用ZMQ机制向迁移目的端发送启动接收控制信令的实时信号处 理程序相关的D2D进程的控制指令—— “start_d2d”，同时也将通过ZMQ机制 将保存在动态迁移系统的“Control”字典变量中的相关控制信息和标志信息参数 ucontrol_flag=l ”一并发送到迁移目的端，表示启动包含控制信息的D2D进程， 然后等而移目的端返回的启动进程处理的相关结果。如果迁移目的端的实时信 号处理程序相关的D2D进程启动成功，则迁移发起端可以继续进行下一步操作； 如果迁移目的端返回D2D进程启动失败的结果，则迁移发起端将会终止本次的 实时信号处理程序的迁移操作，并打印迁移操作失败报告，然后跳出本次执行操 作模块。
在通过ZMQ机制接收到迁移目的端启动D2D进程成功的结果之后，迁移发 起端的迁移系统主控程序将会继续通过ZMQ机制向迁移目的端发送创建USRP 实体的控制指令一一“make_usrp",然后根据迁移目的端返回的处理结果来决定 是否继续下一步的操作。如棄接收到迁移目的端返回的创建USRP实体操作成功 的结果，迁移发起端将会继续执行迁移操作的流程；如果迁移目的端返回了创建 USRP实体操作失败的结果，则主控程序将终止迁移操作的后续流程，并向迁移 目的端发送“kill_d2d”控制指令，以结束前面在迁移目的端己经成功启动的D2D 进程，然后打印迁移失败报告，跳出迁移功能模块。
当迁移发起端通过ZMQ机制接收到创建USRP实体成功的信息之后，迁移 发起端将开始调用killProcess函数，结束本地正在运行的实时信号处理相关的 D2D进程和USRP进程。如果迁移发起端本地的实时信号处理相关进程结束操 作执行成功，则主控程序会进一步将eth3端口的ip修改为192.168.20.2,以避免 ip冲突的问题，然后在进行迁移的后续操作；如果本地的实时信号处理相关进程 结束操作失败，那么主控程序则会终止本次迁移操作，并且主控程序向迁移目的 端发送“double_kill”控制指令，令迁移目的端同时结束之前在迁移目的端已经 成功启动的D2D进程和USRP实体进程，然后打印迁移失败报告，退出本次迁 移操作模块。无论结束本地实时信号处理相关的D2D进程和USRP进程是否执 行成功，都将停止另一线程中釆集D2D程序控制信息的操作，并结束该线程。
当迁移发起端实时信号处理相关的D2D进程和USRP进程都被成功地结束 T，并且eth3网络端口的ip也被配置为非USRP设备子网内的适当值之后，主 控程序将会执行迁移的最后一步，利用ZMQ机制向迁移目的端发送建立USRP 实体与USRP设备之间数据链路连接的控制指令一一“link_usrp”，然后等待迁 移目的端返回的连接操作处理结果。如果通过ZMQ机制接板到迁移目的端返回
36
USRP实体与USRP设备己经建立起数据链路连接的结果，主控程序将会打印迁 移成功报告，并成功的完成此次迁移工作，结束本次迁移操作；如果主控程序接 收到迁移目的端返回的建立数据链路连接失败的结果，主控程序则需要通过 ZMQ机制向迁移目的端发送将前面己经在迁移目的端成功启动的D2D进程和 USRP实体进程结束的控制指令——“double_kill”，然后打印迁移失败报告，并 以失败结束本次迁移操作，退出迁移模块。
在迁移发起端IBM Serverl中，将实时信号处理程序从迁移发起端迁移到迁 移目的端的整个迁移流程中，迁移发起端所进行的相关迁移操作流程如图3-12 所示。
37

屢时信号处理程序腐 迁移发起端迁移到迁 〈移目的端处理模块丿
获取本地虚拟网卡ip, 并向迁移目的端发送 配置虚拟网卡ip指令
配置成功
向迁移目的端发送
Control控制信息、标志
位和启动D2D进程指令，
并等待返回处理结果


涣时信号处理程序蚀
囈处理模塡反行结矿
图3-12实时信号处理动态辻移过程中迁移发起端的相关处理流程
38
3.4.5实时信号处理迁回功能模块
当主控程序接收到输入终端输入的控制指令为“MB”时，主控程序将会跳 转到将实时信号处理相关的D2D进程和USRP进程从迁移目的端迁回到迁移发 起端的处理模块，并进行一系列的迁移操作工作。
首先动态迁移系统中迁移发起端主控程序将通过ZMQ机制分别向迁移目的 端发送获取迁移目的端虚拟网卡ip地址的“get_tun_ip”指令和获取迁移目的端 Control变量信息的“get_control”指令。然后莓待盘回结果。只要虚拟网卡ip 地址和Control变量信息市有…个的返回信息出现异常，则终止本次迁回操作， 打印迁回失败报告，并退出。
当接收到正确的虚拟网卡ip和Control变量信息，迁移发起端主控程序会将 虚拟网卡ip地址保存在tun_ip变量中，并调用ifconfig指令，将迁移发起端的虚 拟网卡ip地址配置为tun_ip；将返回的Control变量信息保存在迁移发起端的 Control变量中。
接下来，主控程序调用cmdProcess函数启动实时信号处理程序的D2D进程， 并将Control变量中的相关控制信息以参数的形式传入到D2D程序中。然后检测 启动结果。如果D2D进程启动成功，将会进行后续的迁回操作；如果启动失败， 则立即终止本次迁回操作，并打印迁回失败报告，退岀。
在启动完D2D进程之后，会调用UHD的API接口 uhd::usrp::multi usrp::make 来创建一个USRP实体，并同时■进行开辟本地缓存buffer,定义元数据操作。如 果创建失败，则将会调用killProcess函数结束前面启动成功的D2D进程，然后 打印迁回失败报告，并退出。如果创建成功，则进一步通过ZMQ机制向迁移目 的端发送结束USRP进程指令“kill_usrp”。
如果迁移目的端返回结束USRP进程成功，则主控程序将迁移发起端的eth3 端口 ip配置为192.168.10.1,然后调用uhd_find_devices指令探测USRP设备； 如果迁移目的端返回结束进程失败，则终止豆回為作，并调用killProcess函数分 别结束之前启动的D2D进程和USRP实体进程。
如果在规定的次数内仍然没有探测到USRP设备，则宣告失败，终止迁回操 作，调用killProcess函数结束已经启动的D2D进程和USRP实体进程，并将eth3 端口 ip修改为192.168.20.2,打印失败报告，退出。如果探测到USRP设备的存 在，则将调用uhd::usrp::multi_usrp::get_device建立USRP实体与USRP设备之间 的数据链路链接。	一 ~
USRP实体与USRP设备之间的数据链路建立起来之后，就可以正常就行数 据业务通信，迁回操作已经基本成功。但这时主控程序还需要向迁移目的端发送 kill_d2d指令，结束仍在运行的D2D进程。如果结束进程成功，则最后打印迁回 成场信息，退出本次处理模块；如果结束进程失败，则打印相关结束进程失败信 息，但本次成功仍然算是成功的，所以也打印迁回成功信息，然后退出本次处理 模块。
在实时信号处理程序从迁移目的端迁移回到迁移发起端的整个迁移流程中， 迁移发起端所进行的的相关操作流程如图3-13所示。
39


图3-13实时信号处理从迁移目的端迁回到迁移发起端的操作流程

3.4.6启动迁移目的端实时信号处理相关进程模块
当主控程序接收到标准输入终端输入的控制指令为“DSR”时，主控程序会 跳转到启动迁移目的端的实时信号处理程序的相关进程一一D2D进程和USRP 进程的处理模块，并进行一系列的启动处理操作。
首先迁移发起端IBM Serverl的主控程序会通过ZMQ机制向迁移目的端发 送启动实时信号处理程序相关的D2D进程指令一一“start_d2d”指令，同时利 用ZMQ机制发送参数“control_flag=0",表示正常启动不亘含控制信息的D2D 进程。然后等待迁移目的端启动D2D进程处理结果。当迁移目的端通过ZMQ 机制返回启动D2D进程操作成功的结果时，主控程序会进一步通过ZMQ机制 向迁移目的端发送启动实时信号处理程序相关的USRP进程指令一一 “start_usip",此处发送的是正常启动USRP进程指令，包含USRP初始化操作； 如果鶴发起端通过ZMQ机制接收到迁移目的端返回的D2D进程启动失败的 结果，主控程序会结束本次启动迁移目的端实时信号处理流程操作，并打印报告 表明本次启动迁移目的端实时信号处理程序处理流程失败。
在主控程序发送启动USRP进程指令之后，如果主控程序通过ZMQ机制接 收到迁移目的端返回的启动USRP进程操作成功的结果，则表明此次启动迁移目 的端实时信号处理程序操作成功，打印启动操作处理成功报告，并退出本次处理 流程;如果迁移目的端通过ZMQ机制返回的是启动USRP进程操作失败的结果， 主控程序将启动结束本次处理流程操作，通过ZMQ机制向迁移目的端发送结束 D2D进程指令，然后打印本次模块流程处理操作失败报告，并退出本次模块处 理。
通过ZMQ机制启动迁移目的端IBM Server2实时信号处理程序相关进程模 块的流程示意图如图3-14所示。
41


图3-14启动迁移目的端实时信号处理相关进程的处理流程
3.4.7结束迁移目的端实时信号处理相关进程模块

当主控程序接收到输入终端输入的控制指令为“KR”时，主控程序将会跳 转到结束迁移目的端的实时信号处理程序的相关进程一一D2D进程和USRP进 程的处理模块，并进行一系列的结束进程相关处理操作。
首先迁移发起端的主控程序通过ZMQ机制向迁移目的端发送同时结束实时 信号处理程序相关的D2D进程和USRP进程的指令—— “double_kill”指令，然 后等待迁移目的端返回相应的结束进程处理结果。
当主控程序通过ZMQ机制接收到迁移目的端返回的结束实时信号处理程序 的D2D进程和USRP进程操作成功的结果时，表明本次结束迁移目的端的实时 信号处理相关进程操作执行成功，主控程序会打印本次模块处理流程执行成功报 告，并退出本次模块处理流程；如果主控程序通过ZMQ机制接收到的是迁移目 的端返回的结束实时信号处理程序D2D进程和USRP进程操作失败的结果，那 么则表明本次模块操作执行失败，主控程序打印本次模块调用流程处理失败报告, 并退出此次模块处理流程。
通过ZMQ机制结束迁移目的端实时信号处理相关进程模块的处理流程示意 图如图3-15所示。
42


魔束迁移目的端D2D匪 程和USRP进程处理模块*
X	执行完毕	7
图3-15结束迁移目的端实时信号处理相关进程模块的处理流程
3.4.8主控程序的其他处理模块
当主控程序接收到输入终端输入的控制指令为“end”是，主控程序会通过 ZMQ机制向迁移目的端发送结束整个迁移目的端主控程序的指令一一 “end”， 然后通过ZMQ机制接收迁移目的端返回的结束整个迁移目的端主控程序操作结 果。当主控程序接收到迁移目的端返回的整个主控程序结束操作执行成功的结果 时，打印结束迁移目的端成功报告，并结束本次处理模块；当主控程序接收到迁 移目的端的返回结果是结束整个主控程序操作失败的结果时，迁移发起端主控程 序会打印迁移目的端主控程序结束失败报告，并退出本次处理模块。
当主控程序接收到输入终端输入的控制指令为“q”或者“quit”时，主控程 序会通过ZMQ机制首先向迁移目的端发送结束迁移目的端主控处理程序的“end” 指令，然后再通过ZMQ机制接收迁移目的端的返回结果。如果迁移目的端返回 结束整个主控程序操作成功，则迁移发起端主控程序会打印结束迁移目的端主控 程序成功，并结束本地的整个主控程序，退出；当迁移目的端返回整个主控程序 结束操作执行失败时，迁移发起端主控程序会打印相应的结束主控程序失败报告， 并也结束本地的整个处理程序，退出。
当主控程序接收到终端输入的控制指令是“h”或者"help”时，主控程序会 打印提示信息，提示不同输入指令的含义。
当主控程序接收到终端输入的控制指令是未知的指令，主控程序会打印提示 输入指令未知，并要求重新输入正确的指令。
迁移发起端主控程序的其他模块处理流程示意图如图3-16所示。
43

当输入h或
者 help
*
打印终端输入
指令提示有息
图3-16迁移发起端中其他模块的处理流程
3.5迁移目的端实时信号任务动态迁移的设计
在迁移目的端IBM Server2,通过Python脚本编写的动态迁移系统的迁移目 的端的主控程序，用来调度迁移目的端的不同部分之间相互配合以实现不同的功 能，并通过ZMQ通信机制与迁移发起端的主控程序进行通信，配合迁移发起端 的动态迁移系统主控程序，以完成整个动态迁移的任务。
迁移目的端动态迁移系统会通过在ZMQ机制接收到来自于迁移发起端的一 系列控制指令，根据这些控制指令，配合迁移发起端完成将实时信号处理程序从 迁移发起端迁移到迁移目的端的，并使实时信号处理程序在迁移目的端迅速恢复 之前的业务处理功能等一系列操作。进而保证整个动态迁移任务的完成。
3.5.1动态迁移系统的主控程序处理流程
启动迁移目的端的主控程序之后，主控程序会通过ZMQ消息传输机制，等 待接收发起迁移端的控制指令,然后根据不同的指令进行跳转到相应的模块并进 行相应的处理工作。
当接收到“start_d2d”指令之后，主控程序会跳转到启动实时信号处理的D2D 进程的处理模块，齐进行相应的启动进程处理操作。
当接收到“start_usrp”指令之后，主控程序会跳转到启动USRP进程的处理 模块，并进行相应而启动进程处理操作。
当接收到“make_usrp”指令之后，主控程序会跳转到创建USRP实体的处 理模块，并进行相应前创建处理操作。
当接收到“link_usrp"指令之后，主控程序会跳转到建立USRP实体与USRP 设备之间数据链路遂接的处理模块，并进行相应的链接处理操作。
当接收到“double_kill^,指令之后，主控程序会跳转到结束D2D进程和USRP 进程的处理模块，并蛭行相应的结束进程处理工作。
当接收到“kill_usrp”指令之后，主控程序会跳转到结束USRP进程的处理
44
模块，并进行相应的结束迸程处理工作。
当接收到"kill_d2d"指令之后，主控程序会跳转到结束D2D进程的处理模 块，并进行相应的爲束进程处理工作。
当接收到“get_tun_ip”指令之后，主控程序会进入获取虚拟网卡ip操作处 理模块，并进行相至函乍。
当接收到“set_tun_ip”指令之后，主控程序会进入配置虚拟网卡ip操作处 理模块，并进行相关羸作。
当接收到“get_control”指令之后，主控程序会进入获取迁移目的端Control 变量处理模块，并純行相关操作。
当接收到“end”指令之后，主控程序会结束工作，退岀。
迁移目的端主控程序的处理流程过程如图3-17所示，每个模块的处理流程将 在后续分别进行描述。

图3-17迂移目的端动态迁移系统主控程序的处理流程示意图

3.5.2启动USRP进程模块
当主控程序通过ZMQ机制接收到迁移发起端发送的“start_usrp”控制指令 之后，程序会跳转到启动与USRP设备进行数据通信的USRP届程的处理模块, 并进行相应的启动处理工作。
首先主控程序会将IBM Server2与千兆交换机相连的网络端口 eth3的ip地址 配置为192.168.10.1,使能够满足与USRP进行通信连接的要求。如果修改成功， 则通过“uhd_find_devices”指令不断探测USRP设备的存在，将进一步试图与 之建立连接；如棄修改失败，则向迁移发起端发送ip地址修改失败的消息，并
45
结束本次处理过程。
当不断的通过“uhd_find_devices"指令探测USRP设备存在的同时，由于 IBMServer2不断的向夕［发錄测指令，这样也能够让交换机更快的“知道"该 Server的ip地址已经改变，使交换机修改该server的ip地址与mac地址的对应 关系。
如果在规定次数内检测到USRP设备的存在，将开始启动USRP进程操作; 但是如果超过规定次数仍没有检测到USRP设备的存在，就将eth3端口的ip改 回192.168.20.2,并向迁移发起端发送USRP进程启动失败的消息，结束此次启 动操作。
在确定USRP设备存在时，主控程序将调用cmdProcess函数启动USRP进程 （此处的USRP进程启动，是完整的包含初始化操作的进程启动，启动过程如 3.2丄2小节所介绍）。在启动结束后，将检测启动USRP进程操作是否成功，如 果启动成功，则迁移发起端发送USRP进程启动成功消息，并成功结束本次启动 处理过程；如果启动进程失败，则先将eth3端口的ip改回到192.168.20.2,然后 再向迁移发起端发送USRP进程启动失败的消息，同时结束本次启动处理过程。
迁移目的端中，启动USRP进程的处理流程如图3-18所示，

图3-18迁移目的端USRP进程启动操作流程

46
3.5.3创建USRP实体模块
当主控程序通过ZMQ机制接收到迁移发起端发送的“make_usrp”控制指令 之后，程序会跳转到创建USRP实体的处理模块，并进行相应面创建处理工作。
迁移目的端中的动态迁移系统的主控程序首先将调用UHD的API接口函数 uhd::usrp::multi usrp::make来创建一个USRP实体。如果USRP实体创建成功， 则将会进行下二步操作，开辟本地的缓存buffer用来存放待发送或者待接收的数 据，然后定义元数据用来存储发送数据或者接收数据的起始位置和错误类型标识。 如果上述各步操作都执行成功，则将会向迁移发起端发送USRP实体创建成功的 消息；但只要上述操作中，有一步操作执行失败，则会终止此次创建USRP实体 操作，不再继续下面的操作，向迁移发起端发送USRP实体创建失败的消息。在 向迁移发起端发送完创建结果消息后，结束此次创建操作。
迁移目的端USRP实体的创建操作具体流程如图3-19所示。

图3-19迁移目的端USRP实体创建流程

3.5.4 USRP实体与USRP设备数据链路建立模块
当主控程序通过ZMQ机制接收到迁移发起端发送的“link_usrp”控制指令 之后，程序会跳转到建立USRP实体与USRP设备之间数据链冨的处理模块，并 进行相应的链接处理操作。
47
迁移目的端主控程序首先将与千兆交换机相连的eth3端口的• ip地址配置成 192.168.10.1,使满足USRP设备的通信要求。如果端口 ip配置成功，则通过 “uhd_find_devices”指令不断检测USRP设备；如果端口 ip配置失败，则终止 本次蘿接曲建立操作，并向迁移发起端发送操作失败的消息。
如果在规定次数内检测到了 USRP设备的存在，则将会继续进行下一步，调 用uhd::usrp::multiusrp::get_device建立起已创建的USRP实体与USRP设备之间 的数据链路连接;果超囱定次数仍没有检测到USRP设备的存在，则将会终 止此次链接操作，并将eth3端口的ip修改回192.168.20.2,并向迁移发起端发送 USRP进程启动失败的消息。
迁移目的端中，建立USRP实体与USRP设备之间数据链路链接的操作流程 如图3-20所示。

图3-20建立USRP实体与USRP设备之间数据链路链接的操作流程

3.5.5启动实时信号处理D2D进程模块
当主控程序通过ZMQ传输机制接收到迁移发起端发送的“start_d2d”控制 指令之后，程序会跳转到启动实时信号处理程序的处理模块，并进行而应的启动 处理工作。
首先根据迁移目的端动态迁移系统主控程序通过ZMQ机制接收到的 control flag的值来决定下一步操作。
48
当接收到的control flag的值为0时，主控程序调用cmdProcess函数启动实 时信号处理的D2D进麻。在启动结束后，检测D2D进程启动是否成功。如果启 动成功，将会继续后续操作；如果启动失败，则向迁移发起端发送D2D进程启 动失败的消息，同时结束本次处理过程。
当接收到的control_flag的值为1时，主控程序接收迁移发起端发送过来的 包含D2D程序相关控面信息，并保存在迁移目的端动态迁移系统的Control变量 中。随后调用cmdProcess函数启动D2D进程，并将相关控制信息以参数传递的 形式传入到D2D程序中，以供D2D程序使用。在启动操作完成之后，检测D2D 进程是否启动成功。如果启动成功，则继续进行后续操作；如果启动失败，则向 迁移发起端发送D2D进程启动失败的消息，同时结束本次处理过程。
当D2D进程启动成功，主控程序会向迁移发起端发送D2D进程启动成功消 息，并结束本次启动进程操作。同时系统也会创建新线程用于釆集启动的D2D 程序的相关控制信息，保存在变量Control中，并且该线程每个10s就会启动一 次采集信息操作，用于不断更新系统中Control变量的值。
迁移目的端中实时信号处理D2D进程启动操作流程如图3-21所示。

图3-21迁移目的端D2D进程启动操作流程

3.5.6结束USRP进程模块
当主控程序通过ZMQ机制接收到由迁移发起端发送过来的“kill_usrp”控制 指令之后，主控程序会跳转到结束USRP进程的模块，并进行相应的结束进程相
49
关处理操作。
首先将迁移目的端的eth3端口的ip修改为192.168.20.2o如果ip修改成功, 则进行后续操作；如果ip修改失败，则向迁移发起端发送ip修改失败的消息， 并结束本次处理。
在ip修改成功之后，调用killProcess函数，该函数将执行结束USRP进程的 操作，如果函数成功结束了 USRP进程，则向迁移发起端发送结束USRP进程成 功的消息，结束本次结束进程操作；如果函数执行失败，则向迁移发起端发送结 束进程失败的消息，终止本次操作。
迁移目的端结束USRP进程的处理流程如图3-22所示。

图3-22迁移目的端结束USRP进程处理流程

3.5.7结束实时信号处理D2D进程模块
当主控程序通过ZMQ机制接收到由迁移发起端发送过来的“kill_d2d”控制 指令之后，主控程序会跳转到结束实时信号处理的D2D进程的处理履块，并进 行相应的结束进程相关处理操作。
首先调用killProcess函数，该函数将会执行结束实时信号处理的D2D进程的 操作。如果该函数成功结束了 D2D进程，那么主控程序会立即向迁移发起端发 送结束D2D进程成功的消息；如果函数执行失败，那么主控程序则会发送结束 D2D进程失败的消息到迁移发起端。
当主控程序向迁移发起端发送完执行结果消息之后，会结束在迁移目的端动 态迁移系统中运行的釆集D2D程序控制信息的线程，然后退出本次处理模块。
50

迁移目的端动态迁移系统中，结束实时信号处理的D2D进程模块的操作处 理流程如图3-23所示。

图3-23迁移目的端结束D2D进程处理流程

3.5.8结束实时信号处理相关进程模块
当主控程序通过ZMQ机制接收到由迁移发起端发送过来的“double_kill”控 制指令之后,主控程序会跳转到结束实时信号处理程序相关的D2D进程而USRP 进程的处理模块，并进行相应的结束进程相关处理操作。
首先主控程序会调用killProcess函数，分别结束正在迁移目的端运行实时信 号处理相关的D2D进程和USRP进程，然后再将迁移目的端的eth3端口的ip改 为192.168.20.2o上述操作都执行完毕之后，再检测各操作是否执行成功，只有 在各部分操作都执行成功，主控程序才通过ZMQ机制向迁移发起端发送结束 D2D进程和USRP进程操作成功的消息；否则，则向迁移发起端发送结束相关 进程操作失败的消息。
在向迁移发起端发送完执行结果消息之后，主控程序将结束在在迁移目的端 釆集D2D程序控制信息的线程，然后退出本次处理模块。
动态迁移系统中，结束迁移目的端实时信号处理相关的D2D进程和USRP 进程模块的处理流程如图3-24所示。
51


图3-24迁移目的端结束D2D进程和USRi»进程处理流程

3.5.9获取和配置虚拟网卡ip地址模块
当主控程序通过ZMQ机制接收到由迁移发起端发送过来的“get_tun_ip”控 制指令之后，主控程序将通过调用“ifbonfig”指令，获取迁移目的端羸虚拟网 卡ip地址，并将其保存在tun_ip变量中。如果获取ip地址成功，则通过ZMQ 机制将该ip地址发送到迁移宏起端，如果获取失败，则向迁移发起端发送获取 失败消息。
当主控程序通过ZMQ机制接收到由迁移发起端发送过来的“set_tun_ip”控 制指令之后，迁移系统将接收到迁移发起端发送过来的ip地址值，异蒋液地址 保存在tun_ip变量中，然后调用“ifconfig”指令配置迁移目的端的虚拟网卡地 址为tun_ip的值，并通过ZMQ机制向迁移发起端发送虚拟网卡ip配置结果。
动縁移系统中，获取和配置迁移目的端的虚拟网卡ip地址的操作流程如图 3-25所示。
52
接收迁移发送端发 送的ip地址，并保 存在tun ip变量中
调用 ifconfigSE置
虚拟网卡ip地址一配置失败
向迁移发起 端发送配置 ipife址央败
图3-25获取和配置虚拟网卡ip地址的操作流在
3.5.10主控程序的其他处理模块
当主控程序通过ZMQ机制接收到由迁移发起端发送过来的“get_comrol"控 制指令之后，主控程序会将迁移目的端的Control变量信息通过ZMQ机制发送 到迁移发起端，然后退出。
当主控程序通过ZMQ传输机制接收到由迁移发起端发送过来的“end”控制 指令之后，主控程序会退出，结束迁移目的端动态迁移系统主控程序
当主控程序通过ZMQ传输机制接收到非识别的控制指令之后，会向迁移发 起端发送接收错误指令的消息，并跳出。
当主控程序中的一些执行命令遇到异常，抛出时，except模块会接收处理， 并向迁移发起端发送命令执行异常消息，然后主控程序跳出，等待接收下一次控 制指令。
迁移目的端主控程序的其他模块处理流程如图3-26所示。
53


图3-26迁移目的端主控程序其他模块处理流程

3.6实时信号处理动态迁移系统验证
本小节将对上述几节设计的实时信号处理的动态迁移系统进行测试，并通过 与基于虚拟机系统进行迁移方案的性能进行对比来分析其性能。本实验所利用的 虚拟机软件环境是在支持Inter VT-x技术的IBM Server安装Xen3.1.0的 Fedora7(2.6.l8)系统，并在Xen3.1.0上进行全虚拟化虚拟机迁移。
3.6.1实时信号处理的动态迁移系统方案搭建
实际测试方案搭建中，动态迁移系统中的实时信号处理计算资源单元利用图 3-27所示的IBM Server设备和USRP设备组成的。实时信号处理程序运行在IBM Server中，利用USRP设备通过天线收发数据。


图3-27IBM Sei-ver和USRP设备实际图
动态迁移系统方案的示意图如图3-28所示，所有的IBM Server都连接到局
54
域网内，以便以系统测试和观察。IBM Server!和IBM Server2通过千兆交换机 组成了一个实时信号处理的动态迁移系统。






图3-28动态迁移系统方案示意图
动态迁移系统方案实际搭建场景如图3-29所示。

图3-29动态迁移系统方案搭建场景'
在迁移系统启动前，首先将数据业务发起端（即IBM Server3）的虚拟网卡 ip配置为192.168.40.8,将数据业务接收端（即IBM Server!或者IBM Server2） 的虚拟网卡配置为192.168.40.9。然后启动IBM Server3上实时信号处理程序， 最后启动IBM Server!和IBM Server2上的动态迁移系统。
3.6.2实时信号处理的动态迁移系统性能测试
针对动态迁移系统测试，首先在IBM Server!输入DS指令，启动实时信号 处理相关进程，然后在IBM Server3上利用指令ping 192.168.40.9 -i 0.1,向IBM
55
Server发送数据，建立起IBM Server3与IBM Server!之间的通信数据链接，并 进行数据业务通信。然后在IBM Server 1输入M或者Migration指令，启动动态 迁移操作。迁移完成后，IBM Server3会失去IBM Server 1的数据链接，而与IBM Server2建立起数据链路链接。也可以进行实时信号处理从IBM Server2向IBM ServerI的迁移，相应的流程与操作类似，指令为DSR与MB,在此不再赘述。 如图3-30中所示，先进行IBM Server3与IBM Server 1之间的ping数据业务， 然后进行动态迁移，实现IBM Server 1上的D2D程序向IBM Server2上迁移，实 现IBM Server3与IBM Server?之间的ping数据业务。迁移过程中会系统会记录 各个环节的时间花费，同时通过观察ping数据业务的丢包的数量也可以大致计 算出数据业务中断时间，如式3-1所示。
Time numberx 100ms	(3-1)
其中7?糜表示业务中断时!'□], number表示丢包数量，每个包的花费时间为lOOmSo

图3-30ping数据业务迁移实现图


表3-4动态迁移过程中各处理环节耗费时间
动态迁移系统各个环节	时间花费(单位：ms)
获取迁移发起端虚拟网卡ip, 并配置迁移目的端虚拟网卡	6
向迁移目的端传送控制信息并 启动D2D程序进程	53
创建USRP实体	55
修改迁移目的端eth3端口	6
链接USRP实体与USRP设备	1743
实时信号处理程序同步	106
动态迁移过程总花费时间	1969
数据业务丢包个数	19个
通过对搭建的实时信号处理的动态迁移系统进行测试。测量出动态迁移系统

56
各个环节的花费时间如表3-4所示。数据业务中断时间为修改eth3端口和链接 USRP实体与USRP设备所花费的总时间以及实时信号处理业务同步所需时间花 费，为1855ms,丢失19个数据包，几乎不会对通信业务造成影响。
通过图3-31可知，对于实时信号处理这种具有高实时性要求，并且数据处理 量大，占用较高的CPU资源和内存资源的任务，实时动态迁移系统方案无论是 在总迁移时间花费，还是在数据处理业务中断时间方面相比于基于虚拟机系统的 迁移方案都具有非常明显实时性的优势。实时动态迁移系统迁移过程中，数据业 务只有不到2s的中断现象，这对用户不会产生明显的影响；而基于虚拟机系统 的迁移技术，则会给用户带来无法容忍长达20s以上的业务中断现象。因此，针 对于实时信号处理，该动态迁移系统具有巨大的应用优势“


3.7本章小结
针对于实时信号处理的动态迁移技术，本章首先需要解决实时信号处理所需 要的数据流的快速高效的切换迁移问题。针对该问题，本章充分研究了 USRP设 备，在USRP设备基础上实现了通信业务数据流的实时切换。然后在USRP设备 基础上，利用IBM Server作为实时信号处理程序的载体，设计一整套针对实时 信号处理的动态迁移系统，并利用ping数据业务，对该动态迁移系统性能进行 测试。将该套动态迁移系统方案与基于虚拟机系统的迁移技术进行比较，可以很 明显的看出该套动态迁移系统在实时性方面具有巨大的性能优势。因此，相比于 虚拟机系统的迁移技术，该套动态迁移系统对于实时性要求很高的通信领域的实 时信号处理任务，能够很好地完成迁移任务。可以预见，在高实时性要求的实时 信号处理的动态迁移方面将会具有非常大的应用发展空间和潜力。
57
第四章中继处理的动态迁移技术方案设计与实现
4.1中继处理业务的应用
在现实的通信场景中，尤其是无线通信应用中，由于通信距离较远导致的衰 减或者由于高山等障碍物的阻挡，发送终端和接收终端之间往往没有直连路径到 达，通常需要借助于几个中继处理业务点的转发才能够将发送端的数据传输到接 收端，以供接收端进行处理"，如图4-1所示，终端1和终端3之间要进行数据 通信，但是两个终端之间没有可以直达的路径，但是两个终端都可以到达终端2, 这时终端2充当了中继作用，终端1和3之间的数据通信借助于终端2的数据处 理与转发来完成。


图4-1中继应用示意图
由于在实际应用中，需要进行通信的两个终端用户之间往往没有直达路径， 要经过多次中继处理才能建立起两终端之间的通信链路，所以中继处理业务的应 用范围是非常广阔的，因此对于中继处理的可靠性和资源处理效率的研究就很有 必要I諷。通过将中继点的计算处理资源进行虚拟化，通过动态迁移技术，合理 调度分配实时信号处理中继业务在不同计算处理资源中，即能提高计算处理资源 的使用效率，避免使用效率很低的计算资源浪费，也能够使计算处理资源的负载 达到均衡，避免实时信号处理任务极度分配不均，影响处理能力。
本章将上一章设计的实时信号出的动态迁移系统应用到通信中继处理业务 中，使实时信号处理的中继处理业务能够在中继点内的计算处理资源内实现迁移。 这一技术的实现，将使实时信号处理任务在中继内得到合理分配调度，提高整个 中继的计算资源使用效率，使中继内的计算处理资源达到负载均衡。
58
4.2中继处理的动态迁移技术设计与实现
4.2.1中继处理的动态迁移系统方案设计
在实时信号处理动态迁移系统基础上，中继处理业务的动态迁移技术方案如 图4-2所示，图中绿色矩形框内就是实时信号处理的动态迁移系统架构图，将其 应用于中继处理中，实现中继处理的动态迁移。



图4-2中继处理业务的动态迁移系统方案架构
针对中继处理的动态迁移技术方案，进行中继处理业务的实时信号处理程序 是基于LTE通信协议实现的具有中继业务转发处理能力的D2D程序〔29】，该程序 能够对将目的终端的ip地址打包到业务数据中，也能够解析接收到的数据中包 含的目的终端ip地址，通过查找自身的路由表信息，将该数据转发到下一跳ip 对应的终端，进而最终到达目的终端。具体路由表信息将在后面小节进行介绍。
4.2.2中继处理业务的路由表信息迁移
为了利用上一章的动态迁移技术对中继处理业务进行迁移，还需要在基本的 动态迁移系统基础上，加上中继处理业务特有的一些信息的维护与迁移。最主要 的就是中继处理业务数据链路通信所需要的路由表信息。
4.2.2.1路由表信息的建立
中继终端为了实现将接收到的数据信息通过自身的处理转发到目的终端，需 要知道数据信息所要到达的最终目的终端的ip地址，同时还需要知道要将数据 信息转发到该目的终端所需要经过的下一个中继终端（也可以直接是目的终端） 的ip地址，也就是下一跳ip地址。
针对本章的具有中继转发业务处理能力的实时信号处程序中的，通过维护一 个结构体来保存相应的路由信息，结构体的元素变量内容如表4-1所示。其中， destID保存着数据信息所要到达的最终的目的终端的ip地址，nexthop保存着数 据信息要到达最终目的终端所要经历的下一跳终端的ip地址。ip地址以索引代 号的形式，这样利用节省保存空间与查找，每个索引对应着唯一的ip地址，所 以通过索引就可以找到相应的ip地址。
表4-1纟吉构体route_t中元素变量内容
destID	目的终端ip地址
nexthop	下一跳终端ip地址
启动D2D程序后，中继终端系统通过不断的对外探测，发现该终端所能够 到达的所有终端的ip地址⑶】，这些终端既包括该中继终端有直连路径能够直接 到达的，也包括可以通过一个或多个中继终端的转发可以到达的。中继终端将这 些探测到的可以到达的终端的ip地址和到达这些终端所要经历的下一跳终端的 ip分别保存在route t结构体的成员变量destID和nexthop中。至此，建立起中 继终端中实时信号虹理进行数据转发所需要的路由表信息。当中继终端D2D程 序接收到通信业务数据信息时，会从中解读出该通信业务数据所要到达的最终目 的终端的ip地址，然后通过路由表信息找到相应的下一跳终端ip地址，并将该 通信业务数据转发到该下一跳终端。
整个路由表的建立过程将会花费可能高达数秒甚至十几秒的时间，但是保存 在route_t结构体中的信息只会随着中继终端可到达的终端的变化而添加或者删 除相应而ip地址信息，这个过程的变化将会是缓慢和不频繁的。因此，在迁移 过程中，需要将该路由表信息从迁移发起端迁移到迁移目的端，而避免在迁移目 的端重新建立该路由表，这将会大大缩减中继终端中转业务的恢复建立，提高迁 移的效率和实时性。
4.2.2.2路由表信息的迁移
通过上一小节知道，路由表的相关信息的建立是需要花费大量的时间的，因 此只在中继终端中继处理业务启动的时候才建立路由表信息，而在中继处理业务 的动态迁移过程中，迁移目的端将利用迁移发起端已经建立好的路由表信息来快 速建立起自己的路由表信息，从而避免路由表初始建立所花费的大量时间。
为了在中继处理业务的动态迁移中能够将路由表信息从迁移发起端迁移到 迁移目的端，需要进行三个阶段。
1、	在迁移发起端，动态迁移系统需要不断的将D2D程序中的route_t结 构体中的路由表信息采集出来，保存在动态迁移系统中，并需要定时 更新维护，为后续迁移操作所使用。动态迁移中保存该结构体的信息 的变量定义为Python的列表类型的routeJist,列表元素为Python的 元组类型，每个元组由destID和nexthop变量对组成，如下：
route_Iist=[(destIDO,nexthopO),(destID 1,nexthop 1),	];
2、	动态迁移系死将路由表信息从迁移发起端发送到迁移目的端，并在迁 移目的端保存。
3、	在迁移目的端，动态迁移系统在启动D2D程序的时候将利用保存的 路由表信息将D2D程序中的route_t结构体配置好。
为了实现上述三个阶段，实现中继处理业糸■的动态迁移系统需要在上一章实 现的基本的动态迁移系统技术基础之上进行一些特制的修改。
60
4.2.3中继处理的动态迁移方案处理流程	，
要想在实时信号处理动态迁移系统的基础上实现中继处理业务的动态迁移, 只需要在现有的实时信号处理动态迁移系统中，加入对上一小节介绍的路由表信 息的迁移操作，就可以完成中继处理业务的动态迁移技术。针对路由表的修改也 将分为迁移发起端和迁移目的端两部分来完成。
4.2.3.1迁移发起端的中继处理迁移流程
为了实现中继处理业务所需的路由表信息迁移，需要在迁移发起端动态迁移 系统中再创建一个新线程用于釆集D2D程序中的route_t结构体信息，保存在动 态迁移系统中。
在动态迁移系统主控程序接收到启动实时信号处理相关进程指令DS之后， 首先启动相关D2D进程和USRP进程，启动过程参见上一章。在进程启动完毕 之后,主控程序会创建一个新线程，该线程用来定期从D2D程序中获取路由表 信息，并更新迁移系统中的routeJist变量信息，保证迁移系统维护一套最新的 路由表信息，图4-3中绿色方框内描述了该线程的操作流程。
当主控程序接收到迁移指令M或者Migration时，首先通过ZMQ机制向迁 移目的端发送配置迁移系统路由表变量的控制指令一一set_route,同时将变量 routejist也发送到迁移目的端以供使用。如果迁移目的端返百配置路由表变量失 败的宿息，则停止本次中继处理迁移操作，退出迁移模块。如果返回配置成功的 消息，则主控程序将会进行迁移操作的其他流程，直到迁移处理完毕为止，具体 流程参加上一章相关内容，完成迁移操作之后，主控程序结束用于更新系统路由 表信息的线程，然后退出。
迁移发起端动态迁移系统中，将中继处理业务迁移到迁移目的端的处理流程 如图4-3所小o
61

图4-3中继处理业务动态迂移过程中迁移发起端处理流程
4.2.3.2迁移目的端的中继处理迁移流程
在迁移目的端恢复迁移发起端的中继处理业务之前，需要先通过迁移发起端 发送过来的路由表信息来配置迁移系统中的路由表信息变量，以避免重建路由表 信息所花费的大量时间。
在迁移发起之前，迁移目的端会首先接收到迁移发起端通过ZMQ机制发送 过来的要求配置迁移系统路由表信息变量的控制指令一一set_route。迁移目的端 接收到该指令后，会继续接收相应的路由表信息，然后利用该信息配置迁移目的 端迁移系统的变量routejist,保存在迁移系统中以供迁移时使用。配置完成后会 向迁移发起端返回配置展功与否的结果。
迁移发起时，迁移目的端首先接收到迁移发起端发送的控制指令- start_d2d,然后主控程序会按照动态迁移系统中的流程启动D2D进程。当D2D 启动成功时，会利用迁移系统的routejist变量对D2D程序中的route_t结构体进 行配置，并返回route_t结构体配置结果信息给迁移发起端。如果D2D进程启动 失败，则终止此次操出，退出。
在本次操作都成功执行完毕之后，主控程序会创建一个新线程来定期采集
62

D2D程序的route_t结构体信息，以更新迁移系统中的变量，使迁移系统维护最 新的路由表信息，如图4-4中绿色方框所示。
迁移目的端动态迁移系统中，完成中继处理业务迁移任务的相应处理流程如 图4-4所示。
动态迁移主 控程序启动
(	set route	1—start d2d―i
接收迁移发起
端的路由表信	启动D2D进程一启动失败-
息睪量
启动成功
V	▼


疏冨誰| GO
!取路由表信息丨



图4-4迁移目的端中继处理业务迁移任务相关处理流程
4.3中继处理的动态迁移技术方案验证
4.3.1中继处理动态迁移系统方案搭建
在实验室环境下，根据上一节设计的中继处理业务的动态迁移技术方案搭建
63

实际的动态迁移场景，对上述技术方案进行验证，并测试其迁移性能。图4-5为 搭建的中继处理业务的动态迁移系统方案示意图。图中发射点需要将IBM Server3上产生的视频数据通过D2D相关程序传送到空口发送到接收点，然后在 IBM Server4中的D2D相应程序从接收天线空口数据中解析岀该视频数据，并发 送到播放器进行播放。但由于发射点与接收点之间由于障碍物的阻挡，没有直连 路径，所以收发之间需要通过中继转发点进行数据的转发才能进行通信数据链接。
中继转发点的D2D相关程序将接收到的数据进行解析，然后根据目的地址 ip转发到相应的下一跳接收点去。首先由动态迁移系统主控程序根据相应指令启 动IBM Serverl上的D2D实时信号处理相关进程来完成对通信数据业务的解析 与转发，实现IBM Server4与IBM Server3之间的视频数据业务的收发，使IBM Server4上能够成功解析播放出IBM Server3上产生发送出来的视频数据信息。 然后主控程序发出迁移指令，开始中继处理的迁移过程，将D2D程序从IBM Serverl上迁移到IBM Server2上，并恢复之前的视频数据解析转发业务，保证 IBM Server3与IBM Server4之间的通信链路畅通，使IBM Server4上的视频播放 业务不受影响。



图4-5中继处理动态迁移方案测试场景示意图
图4-6为在实验室条件下搭建的实际系统测试场景图。
4.3.2中继处理动态迁移系统性能测试
通过观察IBM Server4中的视频播放业务在迁移过程中的中断情况，可以反 映出中继处理动态迁移系统的实际应用性能，对用户实际体验的影响，再结合迁 移系统的测量参数值来分析中继处理业务的迁移性能。对搭建的中继处理业务动 态迁移实际系统的迁移过程的性能测试如表4-2所示。
表4-2中继处理动态迁移系统时间花费情况
视频业务中断时间(ms)	动态迁移过程总时间(ms)
实时信号处理动 态迁移系统	1855	1969
中继处理动态迁 移系统	1858	2173

2200
2100
2000
| 1900
1800
1700
1600
图4-7中继处理动态迁移与基本的实时信号动态辻移时间花费对比
图4-7描述了中继处理业务动态迁移过程与基本实时信号处理动态迁移过程
65
的时间花费对比情况。虽然整个迁移过程时间多出了几百毫秒，但是业务中断的 时间基本没有变化，而且迁移过程多出的时间基本是花费在获取，传递和配置路 由表信息上，这些操作都是在建立数据连接操作之前的预处理过程中，所以基本 不影响业务的中断时间。因此，可以看出将实时信号处理动态迁移系统的应用在 其他广阔的场景中，并不影响该系统的实际性能，所以该系统将拥有很强的应用 前景。
4.4本章小结
通过将实时信号处理的动态迁移系统应用到在无线通信领域用非常广的 中继处理业务中，并对其性能进行了测试。动态迁移技术为中继转发点内有限的 中继转发计算处理资源进行整合提供了可能，能够使计算处理资源得到有效合理 的分配，既可以避免个别计算资源的过度负载导致的处理性能下降，又可以避免 个别计算处理的过度空闲导致计算处理资源的极度浪费，大大提高了中继处理的 处理效率和处理能力。同时也说明了实时信号处理动态迁移技术的良好的可扩展 性，为该技术的应用提供了广阔的前景。
66
第五章总结与展望
5.1论文总结
随着无线通信技术的发展，以及人们对在无线通信技术发展起来的各种数据 业务的不断需求，特别是对移动网络业务需求的爆炸式增长，导致计算处理资源 的局部性紧张的现象。由于移动网络业务用户具有高流动的特性，导致移动网络 数据业务需求分别的极度不均，造成一部分区域的用户业务需求量非常大，计算 处理资源高负载运作，往往会影响数据业务处理性能，降低用户的体验。而另一 部分区域，由于人流的减少，导致用户业务需求急速下降，大部分计算处理资源 处于闲置或者低效率运转，造成了计算资源的极大浪费，增加了能源的耗费。而 且随着移动互联网的不断发展，这种一方面资源极度紧张，影响处理性能，另一 方面资源闲置，极度浪费的现象将会越来越突出严重。
针对这一突出现象，自然会想到如何将这些资源整合，动态调度起来，让闲 置的资源能够有效的利用起来，增加高负载业务的计算资源量。这样既提高了资 源的利用率，又避免的资源紧张的情况。为了解决这一突出问题，需要将计算资 源整合起来，实现计算资源的虚拟化，形成一个大的计算资源池，来根据业务量 的需求来调度相应的计算资源的技术。而实现该技术核心的内容是实现实时信号 处理业务能够在不同的计算处理资源之间进行动态的迁移，而不影响正常的数据 处理业务的运行。
本文正是针对实时信号处理的动态迁移这一核心技术的研究，设计实现通信 数据业务的实时信号处理功能在不同的计算资源载体上的具有高实时性的动态 迁移技术，为解决上述问题提供了技术上的可能。针对这一技术的研究，本文的 主要工作总结如下：
1.针对现有的虚拟机系统，研究了基于虚拟机系统的虚拟化技术，并在此 基础上研究了实时信号处理基于虚拟机系统的迁移技术。指出了该技术 在通信系统要求的实时性方面的不足。引出对新技术的需求。
2.针对SDR平台，研究了 USRP设备的工作原理，实现了在实时信号处理 动态迁移过程中，通过USRP设备的数据流的迁移技术。并在USRP设 备数据流迁移技术的基础上，实现了通信数据业务实时信号处理在不同 的计算处理资源IBM Server之间的动态迁移技术，并在实验室条件下， 搭建了该动态迁移系统，对其进行迁移过程性能测试。通过与基于虚拟 机系统的迁移技术相比，该技术具有实时性方面的巨大优势，能够很好 的满足通信数据业务对实时行的要求。
3.在实时信号处理的动态迁移技术基础上，又进一步将该技术应用于通信 领域中广泛存在的中继处理业务中。实现了中继处理业务在不同的资源 处理单元之间的动态迁移，为避免中继处理业务局部负载过高，性能下 降和提高中继处理业务的资源利用率提供了支持。并搭建该实际系统进 行测试，可以得出，实时信号处理的动态迁移技术的不同推广应用并不 会影响动态迁移技术的性能，从而验证了该技术的应用空间。
67
5.2展望
虽然资源虚拟化己经提岀多年，但是针对实时信号处理的动态迁移技术的发 展很缓慢，还有诸多的问题需要解决。本文只是针对实时信号处理动态迁移技术 在实时性方面进行了一部分实现。因此，后续还有大量的工作需要完成：
(1)本文是针对实时性和扩展性方面的设计实现了实时信号处理动态迁移技 术，后续还需要在实时信号处理动态迁移技术的稳定性，复杂性，其他 领域的更多应用性方面进行加强和探索。
(2)本文研究了具有高实时性的实时信号处理动态迁移技术，下一步将继续 利用该技术整合大量的计算处理资源，实现计算处理资源的虚拟化，形 成一个庞大的计算处理资源池，具有强大的云计算能力，以能处理不同 量级的通信业务。
(3)在本文的高实时性的实时信号处理动态迁移技术的基础上，下一步将继 续研究合理有效的资源调度分配算法来根据实际需要将通信业务的实时 信号处理模块在不同的计算处理资源之间进行迁移，提高计算处理资源 池的分配调度能力，以实现业务处理能力的最大化和资源使用效率的最 大化。
