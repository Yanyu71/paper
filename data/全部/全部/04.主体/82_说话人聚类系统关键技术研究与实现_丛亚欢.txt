第一章绪论
1.1研究意义与背景
随着机器学习、深度学习的快速发展，语音技术的相关研究受到广泛关注， 引起新的研究热潮。人们对于人机交互的期望达到新的高度，各类语音产品竞相 出现，其代表性产品有微软小冰、谷歌助手、亚马逊echo、苹果Siri,天猫精灵等 等。各大科技公司在语音家居，语音车载，智能设备等语音应用领域激烈角逐。 随着人们对语音产品的需求增大、对语音技术的要求增强。语音技术也成为未来 的智能语音设备的重要技术支撑。其中代表性的语音技术主要有语音识别，语音 合成，声纹识别，声音类别检测等，成为当前语音技术的重要的研究方向。
说话人聚类是语音信号处理的一个重要手段，它将一个语音数据中属于同一 说话人的语音片段聚成一类并生成序列标签标记它们。主要概括为“谁，何时， 说话”问题。说话人聚类技术主要应用在会议语音记录分类、语音识别预处理、 声音类别检测、说话人识别等诸多研究方向，因此具有重要的研究意义［1］。
此外，文献［1］表明，说话人聚类有助于在会议（ICSI［2］, AMI［3］）和家庭环 境（CHiME5［4］）下的多说话人对话场景中提高自动语音识别（ASR）性能。并 与说话人识别领域的研究相互促进。说话人聚类任务的特点是在给定任意音频记 录的情况下识别说话者，而又不知道共有多少个说话者。与说话人聚类有关的其 他任务如下［5］：
•说话人验证（SV）： 一个二进制决策任务，其目的是确定录音是否属于某个 人。
•说话人识别（SI）： —项多类分类任务，确定某个录音属于多个说话人中的 谁。
由于每个录音涉及多个说话者，因此在聚类之前需要自动分割，也称为说话 人聚类。说话人聚类是一个完全不受监督的问题（说话者的数量和每个说话者的 段数未知），因此很容易注意到，相对于说话人验证和说话人识别而言，它被认 为具有更高的复杂性。说话人聚类的复杂性可与计算机视觉中的图像分割问题 相提并论，在该问题中通常无法确定要找到的区域数。
另外，文献⑺中提到，说话人聚类算法大都集中在单说话人场景下的会议记 录，通话记录等，不能很好的代表现实的应用条件，如何在复杂的场景下实现说 话人聚类成为当前的研究难点，其中多说话人同时说话（overlap）的“鸡尾酒会” 问题成为当前说话人聚类研究的主要困难之一。
本文研究当前先进的说话人聚类方法，提出并实现适用于单通道的说话人聚 类系统。也由于比麦克风阵列获得更少的位置信息，单通道下的说话人聚类问题 的研究更加困难。但是单通道的语音数据在日常生活中的更为常见，使得单通道 语音信号下的说话人聚类系统具有更加现实的应用意义。
本文提出的说话人聚类系统基于说话人数量估计与uis-rnn聚类结合的方法。 其中说话人数量估计任务中，本文提出使用GST模型结构的数量估计方法，较目 前先进的基于CRNN的说话人数量方法count-net［36］有明显提升。此外，比较只支 持固定长度语音信号的count-net方法，本文提出的基于GST的说话人数量估计方 法支持变长的语音信号输入。在说话人聚类任务中，本文使用2018年由google提 出的首个实现全监督的说话人聚类方法：uis-mno并在其基础上提出几项改进， 包括提出新的说话人转换概率估计，结合说话人数量估计，增加了再聚类的决策 机制。最后结合说话人数量估计方法，预先检测多说话人同时说话(overlap)状 态，在原有只支持非overlap场景下的uis-mn方法的基础上，有效地解决了多说话 人同时说话场景下的聚类问题。
1.2国内外研究现状
语音作为人类最重要的沟通方式，人们对于语音的研究由来已久。语音信号 中包含丰富的信息，包括语言内容，说话者情绪，说话者身份信息等，其中对说 话者身份的信息也早在十九世纪初就已开始［11］研究。说话人聚类的目标正是根 据说话者的语音中的身份信息，辨别不同说话人之间的差异，以此实现对音频中 的多说话人进行聚类。
说话人聚类系统主要包括说话人分割和说话人聚类，是说话人聚类领域研究 研究人员的重点研究方向。因此本文针对这两部分分别介绍其研究现状。
说话人分割旨在将音频流分成听觉上相同的段，因此理想情况下，每个段仅 包含一个说话人，可以将其大致分为两类：基于度量的和基于模型的分段方式。 基于度量的分段是使用评估音频内容的距离函数评估在音频流上移动的相邻分 析窗口之间的相似性。距离函数的局部最大值(超过阈值)被视为变化点。目前 存在多种距离度量。例如Kullback-Leibler (KL)散度［12］,以及BIC［13］度量标 准等。
在基于模型的分割中，从训练语料库中导出一组模型并针对不同的说话者类 别进行训练。使用这些模型对传入语音流进行分类，此外，先验知识是初始化说 话人聚类模型的先决条件。20世纪90年代之后，高斯模型(Gaussian Mixture Model, GMM) ［14］出现，Reynolds对高斯混合模型做了较为详尽的介绍之后，高
2 斯混合模型以其高效性、简洁性、易实现性、较好的鲁棒性，成为对说话人建模 的首选算法。此后DA Reynolds的团队，针对GMM中数据量需求难以满足的问题, 提岀了解决UBM方法：将非目标用户数据(声纹识别领域称为背景数据)混合起 来充分训练出一个GMM,这个混合GMM叫“通用背景模型”(Universal Background Model, UBM) [15]。
此后，科学家Kenny提出了联合因子分析(Joint Factor Analysis, JFA)的理 论分析框架，将说话人所处的空间和信道所处的空间做了独立不相关的假设，需 要同时估计出一段语音在特征音空间上的映射和特征信道上的映射，再消除特征 信道上的干扰，以此实现更好的声纹环境鲁棒性。2009年，Kenny的学生， NJDehak,提出了用一个超向量子空间对两种信息同时建模的方法，这个既能模 拟说话人差异性又模拟信道差异性的空间称为全因子空间(Total Factor Matrix) [16],每段语音在这个空间上的映射坐标称作身份向量(Identity Vector, i-vector ) □ I-vector的出现使得说话人识别的研究抽象为一个数值分析与数据分析的问题。 随着深度学习的快速发展，最近的一些研究[17]表明，通过用神经网络生成的 embedding (也称为d-vector[30])代替传统的i-vector,可以显着提高说话人区分 性能。这主要是因为基于神经网络的方法可以使用大型数据集来训练，因此该模 型在不同的场景下对各种说话人的口音和声学条件具有足够的鲁棒性。此后，更 多基于深度学习方式生成矢量特征相继出现：基于多层感知器(MLP)bottle-neck feature[ 18],基于深度神经网络的x-vector[ 19]等。
说话人聚类是指对说话者语音片段的无监督分类。也就是在语音信号中识 别同一说话者的所有语音片段，并为其分配标签。经典的层次聚类方法 (Agglomerative Hierarchical Clustering, AHC)由 Liu和Kubala提出[20]。每个细 分的片段都被视为专属于一个发言人。通过比较所有小语音片段之间的距离，可 以找到最接近的音频段对。自组织图(SOM)聚类方法也是一种强大的说话人聚 类工具。[21]提出了一种基于SOM的说话人聚类算法。假定说话人的数目是已知 的。数据分为短段。每个部分都被认为仅属于一个说话者。使用阈值将音频记录 初步分割为语音和非语音片段，非语音段用于训练非语音SOM,每个说话者均由 Kohonen SOM建模。
此外，还有基于图论的谱聚类算法(spectralcluster) [22],谱聚类算法将带 权无向图划分为两个或两个以上的最优子图，使子图内部尽量相似，而子图间距 离尽量远，以达到聚类的冃的。这样，谱聚类能够识别任意形状的样本空间且收 敛于全局最优解，其基本思想是利用样本数据的相似矩阵(拉普拉斯矩阵)进行特 征分解后得到特征向量，并对特征向量进行聚类。
另外，传统的说话人聚类方法还包括基于GMM [23]和HMM [24]的聚类方 法。后期随着深度学习在机器视觉领域获得成功，说话人聚类方法中开始出现结 合深度学习的建模方法，如最近新提出的uis-mn［25］,该方法实现全监督的聚类 训练方式，成为当前广受关注的聚类方法。
在过去的很长一点时间，说话人聚类领域主要针对非重叠声（overlappedspeech） 的聚类问题。随着技术的进步和研究的需求提升，对于重叠声音事件 （overlap对话场景）的分割聚类研究也逐渐引起研究者的重视。
2013年Delphine Charlet等人［6］针对说话人聚类中重叠场景的影响进行研究， 应用了2007年S. Otterson等人提出的重叠处理的基本策略，在EIAPE［10］评估中 进行试验，说话人聚类实验表明，改进重叠事件的检测与聚类是改进说话人聚类 问题的有效方式。
研究人员已经在2017年Jelinek Summer Workshop on Speech and Language Technology （JSALT）语音和语言技术夏季研讨会（JSALT） ［9］中讨论了当前的说 话人聚类问题中提到。已经发现，当具有更广泛的声学条件和大量噪声以及高度 的说话人重叠时，现有的说话人聚类系统表现不佳。并提岀第一个DIHARD挑战 ［7］,以开发最先进的说话人聚类系统来解决声音重叠状态下的说话人分割聚类 问题。
然而，根据本文目前的调研结果显示，虽然基于简单场景下（非overlap）的 说话人聚类已经获得较理想的结果，但是针对重叠对话的场景的说话人聚类方法 的研究还处在起步阶段。因此本文提出基于现有的非重叠场景的说话人聚类方法 uis-mn,与说话人数量估计方法相结合，实现了重叠场景下的说话人聚类系统。 并提出了几项改进，提升了原有方法的聚类效果，此外本文提出新的说话人估计 方法，基于GST模型结构的说话人数量估计模型。
1.3论文研究内容
本文主要对语音重叠场景下的说话人聚类算法进行研究与实现，在现有的非 重叠场景下的先进说话人聚类方法，uis-rnn方法［25］上，提出改进方法，包括改 进的说话人转换概率估计方法，增加了再聚类方法，提升说话人聚类效果。此外 本文提出新的说话人数量估计方法，基于GST结构［49］的说话人数量估计模型， 改进了估计效果，并支持变长语音数据。本文结合该说话人数量估计结果，在uis- rnn 的基础上， 解决了多说话人同时说话的复杂场景（“鸡尾酒会”或overlap场景） 下的说话人聚类问题。
大多数现有的说话人聚类系统可以分成以下几个组件：
（1）语音分割模块；
4
（2）	说话人矢量特征表示模块；
（3）	说话人聚类模块；
（4）	一个细分模块。
本文结合说话人数量估计模块，将说话人聚类系统共分为五个主要部分，包 括语音活动检测，语音特征提取，说话人特征矢量表示，说话人数量估计，以及 聚类与细分聚类（resegment）。其中各组成部分涉及的技术有，基于GMM的语音 活动检测（VAD）, MFCC特征提取，基于深度神经网络的d-vector说话人矢量特 征，说话人数量估计模型，以及全监督的说话人聚类方法uis-rnn。本文在 Librispeech[54]和Youtube平台的语音数据中进行实验，并通过结果验证本文所提 方法的有效性。
仁4主要创新工作
本文在对说话人聚类技术的研究与实现过程中，主要创新点如下：
1. 在研究全监督的说话人聚类方法uis-mn的基础上，针对其说话人转换概 率估计问题，提出新的概率估计方法。
2. 研究现有先进的说话人估计任务与方法count-net[36],并提出基于GST模 型结构的说话人数量估计方法。
3. 在uis-mn的基础上，结合说话人数量估计的结果，增加online方式的聚类 细分机制。
4. 由于uis-mn说话人聚类方法[25]只支持非重叠场景下说话人的聚类，即 同一时间只有最多一个说话人说话，不能解决重叠场景的语音信号的多 标签聚类问题，因此本文提出了结合说话人数量估计方法（包括重叠场 景），在uis-mn方法的基础上，实现了重叠场景下的多说话人聚类系统。
1 ■ 5论文结构
本文共分为七个章节，各章节主要内容如下：
第一章绪论。本章节主要介绍说话人技术的研究背景与研究意义，并对当前 主要的研究方法进行梳理，对其中的重要技术环节进行简单介绍，最后介绍本文 的主要工作内容与创新方法。
第二章语音信号预处理。本章主要介绍预处理的相关方法，也常称为语音前 端处理，主要包括语音活动检测，语音特征提取，说话人矢量表征d-vector的相关 研究与方法。
第三章说话人聚类方法uis-nm,主要介绍基于深度学习的说话人聚类方法 uis-mn,分析其优点与不足之处，并介绍了本文的改进方法。
第四章介绍说话人数量估计的相关问题,介绍基于深度学习的说话人数量估 计方法：Count Neto提出新的说话人估计方法：基于GST模型结构的说话人数量 估计方法。
第五章整体介绍说话人聚类系统的实现，包括预处理流程，说话人数量估计, 以及分别使用整体说话人数量估计结果与分语音片段获取说话人数量估计结果 （overlap数量检测），最后说明如何使用这两个结果分别实现说话人聚类细分和 重叠场景下的说话人聚类。
第六章系统结果，分别介绍本文提出的说话人数量估计方法与说话人聚类系 统与baseline的结果比较，并将说话人聚类系统分重叠场景与非重叠场景两组实 验，分别展示本文提出的系统的性能。
第七章对本文研究内容进行总结，对本文的改进方法与相应结果进行概括， 对相关不足进行说明，最后介绍未来的研究计划。


第二章语音信号预处理
2.1基于GMM的语音活动检测
语音活动检测(Voice Active Detection, VAD)是对语音信号与非语音信号进 行识别，并将语音信息出现的起止点作为检测结果。VAD成为语音识别，说话人 识别，语音类别检测等诸多语音研究领域的必要的信号预处理手段，属于语音前 端处理操作。VAD的准确性也直接影响着后端识别等研究问题的性能，如果没有 正确检测语音片段，则随后的识别过程通常是没有意义的。此外VAD能够显著减 少后端识别的计算量，因此被广泛研究。下面主要介绍基于混合高斯分布的语音 活动检测方法。
高斯混合模型(Gaussian Mixed Model, GMM)是指包含多个高斯分布函数 的线性组合，具有形式如下公式(2-1)形式的概率分布模型［26］。'公式(2-2)称 为第k个高斯分模型。
P(y\0) = yK cck0(y\ek)	(2T)
厶7 = 0
其中互是系数，ak > 0, /=0互=1； 0(y|0fc)是高斯分布密度，匪=(%诡),

理论上混合高斯分布可以拟合出任意类型的分布，通常用于解决同一集合下 的数据包含多个不同的分布的情况(或者是同一类分布但参数不一样，或者是不 同类型的分布，比如正态分布和伯努利分布)。混合高斯分布因其拟合任意复杂 的、多种形式的分布能力而广为人知［27］o
混合高斯分布最明显的特征就是它的多模态(K>1)特性，不同于高斯分布 的单模特性质(K=l),这使得混合高斯模型足以描述很多具有多模态性质的物 理数据(包括语音数据)，而单高斯分布则无法实现。数据中的多模态性质可能 来自于多种潜在因素，每一个因素决定分布中一个特定的混合成分。如果单个因 素能够被识别出来，那么混合分布就可以由多个因素独立分布组成的集合。
多元混合高斯分布的应用是提升语音研究性能的一个关键因素(在深度学习 之前)。在多数应用中，根据问题的本质，混合成分的数量K被选择为一个先验 值。虽然有很多种方法尝试去回避寻找“正确值”这个的困难问题，但是主流的 做法仍然是直接选取先验值。

混合高斯分布作为描述基于傅里叶频谱语言特征的统计模型，在传统语音识 别系统的声学建模中发挥了重要作用。原始语音数据经过短时傅里叶变换形式或 者取倒谱后会成为特征序列，由于VAD过程中可以忽略时序信息，因此混合高斯 分布成为适合且有效的语音特征拟合方法，在VAD过程中通常使用混合高斯分布 对语音段与静音段分别建模以实现对与语音区域的检测。
混合高斯分布GMM参数可以通过期望最大化算法(expecatation maximization, EM)得到。EM算法可以使GMM在训练数据上生成语音观察特征 的概率最大化。即期望最大化算法可以有效的训练混合高斯分布，以匹配语音特 征。EM算法是一种迭代算法，1977年由Dempster等人总结提出，用于含有隐变 量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计。EM 算法的每次迭代由两个组成部分，分别为：E步：求期望(expecatation)； M步， 求极大(maximization ),所以这一算法成为期望极大算法(expectation maximization algorithm ),简称EM算法。
概率模型有时既含有观测变量(observable variable),又含有隐变量或潜在 变量(latentvariable)。如果概率模型的变量都是观测变量，那么给定数据，可以 直接用极大似然估计方法，或贝叶斯估计法估计模型参数。但是，当模型含有隐 变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型 参数的极大似然估计方法。对应GMM公式(2-1 ),其中0 = @1卫2，…,aK-,elte2eK),我们使用EM算法估计高斯混合模型的参数氐可以 假设观测数据旳，j =	时这样产生的：首先依概率a*选择第上个高斯分
布模型0(?|去)；然后依第k个分模型的概率分布0(y|&)生成观测数据刃。这时观 测数据yP j = 1,2…N,是已知的；反映观测数据旳来自第k个分模型的的数据 是位置的，k = l,2K,以隐变量彷表示，其定义如公式所示。加是0-1随机变 量。有了观测数据刃，以及未观测数据力花，那么完全数据是：
(yj,Y)2> …，YjK，j = 1,2,,N)	(2-3)
由此可以得到完全数据的似然函数，使用EM算法，实现参数的估计。下面 简要描述高斯混合模型参数估计过程中EM算法的训练过程：
算法2.1
输入：观测变量数据Y(yliy2yN),和高斯混合模型; 输出：高斯混合模型参数；
(1) .取参数的初始值开始迭代;
(2) . E步：依据当前模型参数，计算分模型花对观测数据力的响应度;
,ak0(yj|ek)
bk=^=1ak0(yj|0k)
(3) .M步：计算新一轮迭代的模型参数;

(4) .重复第⑵步和第⑶步，直到收敛。
本文使用基于WebRTC标准［28］的混合高斯方法的语音活动检测：WebRTC- vad［28］o webrtc的vad检测原理是根据人声的频谱范围，把输入的频谱分成六个子 带(80Hz〜250Hz, 250Hz~500Hz, 500Hz~lK, 1K〜2K, 2K〜3K, 3K-4K)分别 计算这六个子带的能量。使用高斯模型的概率密度函数做运算，得出一个对数似 然比函数。对数似然比分为全局和局部，全局是六个子带之加权之和，而局部是 指每一个子带则是局部，并根据全局与局部的似然比结构判决当前是语音还是非 语言段。
2. 2 MFCC特征提取
语音特征提取主要指对原始语音时域信号变换至频域，时频域等方法，主要 包括STFT, MFCC, Filter Banko其中MFCC成为说话人识别相关任务中最为广 泛使用的语音信号特征。
Steven B. Davis最早提出应用于语音信号特征参数提取的Mel频率倒谱系 数(Mel-Frequency Cepstral Coefficient, MFCC)。MFCC是根据人类听觉系统对 语音信号频率的感知原理提出的，因此具有相当好的仿生作用，是语音信号处理 领域中常用的特征参数提取方法。
MFCC分析依据的听觉机理有两个［29］。第一，人的主观感知频域的划定并 不是线性的，根据Stenvens和Vblkman (1940)的工作，有公式(2-8)。公式中Enez 是以美尔(mel)为单位的感知频率；f是以Hz为单位的实际频率。Fmei与f的关 系曲线如图2-1所示。将语音信号的频谱变换到感知域中，能更好地模拟听觉过 程的处理。
MFCC依据的第二个听觉机理是临界带(CriticalBand)。临界带的概念由听 觉掩蔽机理定义。用一个中心频率为带宽为刃7的白噪声来掩蔽一个频率为f 的纯音，先将这个白噪声的强度调节到被掩蔽纯音恰好听不见为止，然后将Vf由 大到小逐渐减小，而保持单位频率的噪声强度（噪声谱密度）不变，起初，这个 纯音一直是听不见的，但当yf小到某个临界值时，这个纯音可以被听见，如果再 进一步减小vf,被掩蔽音则会越来越清晰。这里刚刚开始能听到被掩蔽声时的Vf 宽的频带，称为频率/•处的临界带。当掩蔽噪声的带宽窄于临界带的带宽时，能 掩蔽住纯音/■的强度是随噪声带宽的增加而增加的，但当掩蔽噪声的带宽达到临 界带后，继续增加噪声带宽就不再引起掩蔽量的提高了。临界带宽是随其中心频 率而该变的，被掩蔽纯音得频率（即临界带的中心频率）越高，临界带宽也越宽。


掩蔽效应具有临界带的现象可以从听觉生理上找到依据，人耳基底膜具有与 频谱分析器相似的作用。在20-22050HZ范围内的频率可分成25个频率群。频率群 的划分相应于基底膜划分成许多很小的部分，每一部分对应一个频率群，掩蔽效 应就在这些频率群内发生。对应于同一基底膜部分的那些频率的声音，在大脑似 乎是叠加在一起进行评价的，如果他们同时发声，可以互相掩蔽，因此频率群与 临界带之间存在密切的联系。MFCC依据的感知机理就有临界带概念。按临界带 的划分，将语音在频率上划分成一系列的频率群组成滤波器组，即Mel滤波器组。
在常规的时间信号分析中，任何周期分量在相应的频谱（即傅立叶频谱）中 都显示为尖锐的峰值。这是通过对时间信号进行傅立叶变换获得的。在获取该傅 立叶频谱的幅度的对数后，通过余弦变换获取此对数的频谱，由此能够在原始时 间信号中周期性性出现的的地方观察到一个峰值。由于我们对频谱本身进行了变 换，得到的频谱既不在频域也不在时域，因此Bogert等人将其称为倒频率 （quefrency）域，并将时间信号频谱的对数频谱称为倒谱（cepstrum） [30]。
人耳的听觉范围是20Hz至20kHz。然而人耳对频率的感知状态不同与频率间 隔状态并非线性相关。如一个900Hz的信号（类似于麦克风的反馈声音）和一个 1kHz的声音。尽管实际差异是相同的（100Hz）,但是这两种声音之间的感知距离 可能看起来大于前两种。因此使用梅尔音阶试图捕捉这种差异。可以使用以下公 式将以赫兹（f）为单位的频率转换为梅尔刻度：
10
Fmei = 2595 log(l + -^)	(2_8)
MFCC主要流程如下：
(1) .对连续语音信号进行预加重，提升高频部分；
(2) .分帧加窗并做FFT；
(3) .通过Mel滤波器做进行mel变换；
(4) .取对数滤波器组能量的DCT,保留DCT系数2-13,丢弃其余部分；
(5) .标准MFCC只反映语音静态特性，如果需要动态信息，可进行一阶差分
或二阶差分。
其中预处理包括预加重、分帧、加窗函数。预加重的目的是为了补偿高频分 量的损失，提升高频分量，预加重的滤波器常设计为公式(2-9)。公式中a为一 个常数。
H(z) = 1 - az-1	(2-9)
分帧处理是因为语音信号是一个准稳态信号，把它分成较短的帧，在每一帧 内可将其看作稳态信号，可用处理稳态信号的方法来处理。同时为了使一帧与另 一帧之间的参数能较平稳的过渡，在相邻两帧之间互相有部分重叠。本文实验中， 按照主流的做法，将窗长设置为25ms,重叠15ms。此外，加窗函数的目的是减少 频率域中的频谱泄露问题，将对每一帧语音乘以汉明窗或海宁窗，语音信号 经过上述处理后为竝(m)其中下标i表示分帧后的第Z帧。然后对每一帧信号执行 快速傅里叶变换(FFT),从时域数据转变为频域数据。随之对变换到频域的信号 计算谱线能量，并通过Mel滤波器，计算在该Mel滤波器中的能量。最后对Mel滤 波器的能量取对数后计算DCT。这样就计算出了MFCC参数。该参数不仅用于说 话人识别的相关任务中，在语音识别端点检测等研究中也被广泛使用，是语音研 究领域的重要特征。
2. 3 D-vector说话人矢量特征
本节介绍说话人特征矢量方法。在当今说话人特征表征领域，一共存在三种 主流的特征矢量表征方式，分别是i-vector[16], x-vector[19], d-vector[30],其中 d-vector是一中基于深度学习的特征表示方法，由google提出，其效果在一直的测 评上达到甚至超越经典i-vector特征表示方法，成为当前说话人表征的有效方法 之一。
i-vector是代表帧级特征分布模式的特征。i-vector提取实质上是GMM超向量 的降维(尽管在计算i向量时未提取GMM超向量)。d-vector使用DNN方式训练而 11

得。为了提取d-vector,需要对DNN模型进行训练，从该DNN的最后一个隐藏层 中获得。因此，与i-vector框架不同，它没有关于要素分布的任何假设（i-vector框 架假定i-vector或潜变量具有高斯分布）。



不同的蓝色代表来自不同说话人的embedding/utterance［30i
本文所使用的特征矢量表征方法为d-vector的改进版本，基于GE2E （Generalized end-to-end）的d-vector表征矢量。在论文［30］中提出了通用的端到 端（GE2E）损失函数，以更有效地训练说话者验证模型。理论和实验结果均证 实了这种新型损失函数的优势，此外论文［30］中还引入了MultiReader技术来组合 不同的数据源，从而使d-vector的模型能够支持多个关键字和多种语言。结合这 两种技术，d-vector模型产生了更准确的说话人矢量。
GE2E训练数据采取批处理形式，每组数据包含N个说话人，每个说话人选取 M个语音数据，如图2-2所示。每次选择N*M个utterance （在本文中指240帧的mel 特征）组成一^ batch,这些utterance来源于N个不同说话人，每个人选择M个不 同的utterance,用咒巧表示从第i个说话人的第j个utterance。使用［31］提出的基于 LSTM的特征提取网络，竝丿•作为网络输入，经过LSIM结构后连接一个线性层作 为网络d-vector的输出层，而d-vector则是线性输出层的L2归一化的结果。
GE2E是一种相似性度量方法，用于衡量d-vector：	和各自类中心q的相似
度距离，相似度矩阵》讥被定义公式（2-10）,具体计算方式参考［30］o
Sji,k = 3 • cos（e；i，ck） + b	（2-10）
在训练过程中，希望每个utterance的embedding相似于该说话人所有 embedding的质心c,同时又远离其他说话者的质心。如图2-2中的相似度矩阵所 示，我们希望有色区域的相似度值较大，而灰色区域的相似度值较小。图2-3以 不同的方式说明了相同的概念：我们希望蓝色嵌入矢量接近其说话者的质心（蓝 色三角形），而远离其他质心（红色和紫色三角形），尤其是最接近的质心（红色 三角形））。给定嵌入向量句，所有质心q和对应的相似度矩阵®i肿 可以通过公 式（2-11）和公式（2-12）两种方法计算损失函数，为了计算简便本文使用公式 （2-11）作为loss的计算方法。






图2-3说话人类别间质心关系㈤]

此外，论文[30]中提出了MultiReader方法，一致类似的正则化技术，帮助解 决不同数据源下训练同一模型的规范问题，同样，本文在训练d-vector时，使用了 该技术。考虑以下情况：我们关心具有小数据集D1的域中的模型应用程序。同 时，我们在相似但不相同的域中拥有更大的数据集D2。我们希望在D2的帮助下 训练一个对数据集D1表现良好的模型：
厶(Di—； w) = ExeD1[L(x; w)] + aExeD2[LCx; w)]	(2-14)
在正常正则化流程中，通常使用a||w||2对模型进行正则化。但是MultiReader 中使用了ExeD2[LQx; w)]进行正则化。当数据集D1无法容纳足够的数据时，在D1 上训练网络可能会导致过度拟合。要求网络在D2上也要表现得相当好，这有助于 规范网络。可以将其概括为组合K个不同的，可能极不平衡的数据源：%，…,DK° 对每个数据源进行权重加权，表示该数据源的重要性。在训练过程中，在每个步 骤中，从每个数据源中提取一批/多组话语，并将合并的损失计算为，其中， 厶(x/w)是(2-13)中定义的损失函数。
厶(D1，...,DK) = y akEx eD [L(_xk; w)]	(2T5)
*fc=l
13

第三章UIS-RNN说话人聚类
说话人聚类指对语音特征向量的聚类方法，由于未知说话人数量，因此通常 使用无监督的聚类方法，主要包括层次聚类，k-means,谱聚类，以及基于深度学 习的聚类方法。2018年，Aonan Zhang, Quan Wang等人在文献［25］首次提出一种 全监督的训练方式用于解决说话人聚类问题，称为uis-mn,并在NIST SRE 2000 CALLHOME表现了7.6%的DER的水平，获得研究人员的广泛关注。本文的说话 人聚类系统基于先进的方法uis-mn。
3.1 UIS-RNN模型结构
uis-mn (unbounded interleaved-state recurrent neural networks ) 即无界交错状 态递归神经网络，具有以下几个特征：
(1) .每个说话人都由RNN的一个实例建模，实例间共享相同参数；
(2) .可以生成无限ft量的RNN实例，即实现未知数量的说话人建模；
(3) .对应于不同说话人的不同RNN实例状态在时域中交错。
使用2.3节介绍的方法，我们将utterance通过LSTM网络结构提取d-vector说话 人矢量，d-vector随后作为uis-mn网络的输入：X。uis-mn是一个关于(X,Y)的生 成模型，X是embedding： d-vector, Y是该embedding对应说话人类别标签，uis-mn 的表达式如公式(3-1)所示。其中z表示说话人转换概率，当绻=1表示当前卷所 属说话人与验一1时刻非同一说话人。z产0,表示与t-1时刻属于同一说话人。
T
P(X,匕 z) = p(%i,yi)- p(xt> yt>	z[t-u)
t=2

pglut-Q -
sequence generation
p(y』zt，yt-Q -
speaker assignment
P(Zt|Zt_i)
speaker change
该公式进一步可描述为三部分，分别为序列生成(sequencegeneration),说
话人类别标记(speaker assignment),和说话人转换(speaker change),如公式(3-
14
2）所示。
3.1.1说话人转换概率
说话人转换（speakerchange）概率是用于描述当前时刻的输入embeddin所属 说话人与上一时刻的类别关系的概率。因此类别％可以用公式（3-3）表示。关于 zt的计算方法论文［25］中并没有详细介绍，只提供了基本的二项式分布方法。而 在其开源的项目中，其实现方法为对训练数据进行说话人转换统计的概率估计。 即公式（3-4）。
p(yt = yt-i\zt>y[t-i]) = i--zt
uis-mn的说话人转变估计方法并未真实体现现实场景下的说话人转变状态， 本文在通过实验研究发现，该方法在任意时刻的概率为一个常量图3-1橘色线所 示，而这种常量的概率表示并不能有效表达说话人转换的概率分布，因此本文提 出将原论文实现的固定概率表示，改进为与时间相关的概率计算方法,如公式（3- 5）,并对其进行函数拟合（最小二乘法）。说话人转换的概率与时间相关图3-1蓝 色线部分，更符合对话环境中，speaker change的变化规律。
时长为t时说话人转换次数总和
时长为t的utterance总个数
3.1.2说话人类别标记
为了解决未知数量的聚类问题，uis-mn中使用基于距离的中国人餐馆
15

(distance dependent Chinese restaurant process, ddCRP)计算说话人类别的相关 概率。称为说话人类别标记(speakerassignment)。ddCRP是一个贝叶斯非参数模 型，可以潜在地建模无数个说话人，因此能够有效的用于说话人聚类中。正如上 节所介绍的，当三=0,表示与t-1时刻属于同一说话人，因此说话人类别不改变，而 当丟=1,说话人类别的概率估计如公式(3-6)所示。
P(7t = k\zt =	x Nk,t_i	(3-6)
P(7t = Kt_i + l\zt = I,”—]) = a
其中K—i ：= max表示至t-1时刻全部的说话人类别编码的最大值，即 至t-1时刻的最大的说话人数量。该公式表示出现新说话人的概率为一个常数。而 对于已检测到的说话人类别，其概率与其心成正比。这也是ddCRP算法的思 想所在。心,_1为说话人类别k在该时刻之前岀现的全部块数，每一块表示块内只 含一个说话人，边界外为其他与块内不相同的说话人。因此可以表征每一说话人 类别的概率状态。因此Y与Z的联合分布为公式，既包含检测新出现的说话人的 概率，又包括计算当前时刻为已检测到的某一说话人的概率。
小-	贰7-1口篦附灯)	(3-7)
nt=2(^ke[/ft-i]{yt-i} Nk,t-i+a)
3.1.3说话人特征序列生成
说话人特征序列生成(sequence generation)是uis-mn算法中最重要的环节， 也是全监督聚类算法的实现的关键，将传统的判决式转变为生成式方式是uis-rrn 的主要贡献之一。该方法的基本假设是说话人embedding可由RNN的输出组成的 某参数化分布生成。该RNN具有对应于不同说话者的多个实例，并且它们共享相 同的RNN参数集，在uis-mn中使用门控循环单元(GRU) [32]的RNN模型。如图 3-2所示,相同的说话人共同使用一个RNN,每当出现该说话人序列时更新该RNN 状态。而新的说话人出现后，则初始化新的RNN作为该类别的初始状态。 Embedding生成方式在上节公式(3-2)中表示，基于GRU的输岀，该论文假设该 生成模型的输出建模为正态分布(3-8),其中卩由公式(3-9)计算。
(3-8)

至此，实现了(3-2)的整个计算，而其中只有sequence generation是基于神 经网络形式，结构简单，执行效率高。
16


图3-2 Uis-mn模型聚类过程㈤
3. 2 UIS-RNN的预测解码过程
uis-mn使用基于beam search （波束搜索）的解码方式，一种启发式的寻解过 程，将复杂度减少到0（T）,由此完成说话人的聚类匹配过程。beam search通过扩 展有限集中最有希望的节点来探索图。beam search是最佳优先搜索的优化，可减 少其内存需求。最佳优先搜索是一种图形搜索，根据某种启发式算法对所有部分 解（状态）进行排序。但在beam search中，仅保留预定数量的最佳局部解作为候 选。因此，这是一种贪婪算法。"beamsearch"—词由卡内基梅隆大学的拉吉•雷迪 （Raj Reddy）于 1977年提出［33］。
beam search使用广度优先搜索来构建其搜索树。在树的每个级别上，它都会 生成当前级别上所有状态的后继者，并按启发式代价的升序对其进行排序。但是, 它仅在每个级别上存储预定数量的最佳状态0（称为光束宽度）。接下来仅扩展那 些状态。光束宽度越大，修剪的状态越少。波束宽度无限大时，不会修剪任何状 态，并且beam search与宽度优先搜索相同。beam宽度限制了执行搜索所需的内 存。由于可能会修剪目标状态，因此波束搜索会牺牲完整性（保证算法将以解决 方案终止（如果存在）的保证）。波束搜索不是最佳的（也就是说，无法保证它 将找到最佳解决方案）。
通常，beam search返回找到的第一个解决方案。用于机器翻译的beam search 是另一种情况：一旦达到配置的最大搜索深度（即翻译长度），该算法将评估在 搜索过程中发现的各种深度的解，并返回最佳解（概率最大的解）［34］。通过将 beam search与深度优先搜索相结合来完成beam search,从而进行波束堆栈搜索和 深度优先beamsearch,并通过有限差异搜索［9］进行组合，从而使用有限差异回溯
17
进行波束搜索。最终的搜索算法是随时可以找到良好但可能不是最佳解决方案的 算法，例如波束搜索，然后回溯并继续找到改进的解决方案，直到收敛到最佳解 决方案为止[35] o
uis-mn的具体解码过程如下：
算法3.1
数据：X{x1,x2,x3,...,xT）
结果：厂3,坊）
初始化：x0 = O,ho = 0
for t =1,2,...,T do
（y；, z；） = arg max如）（In p （z』
更新必上_1和GRU的hidden状态；
beam search除了较小的复杂度外，其决策的过程更能和多模态决策融合自 然一体，并在本文提出overlap解决方式中，利用beam search的决策过程，将说话 人聚类的应用从单标签聚类扩展至可解决overlap复杂对话场景的多标签聚类问 题，在第五章会详细介绍该实现方式。
18
第四章单通道下的说话人数量估计
在说话人聚类研究与声音分离等领域，说话人数量是一个对语音算法结果产 生重要影响的信息。从理论上讲，估计同时发言的说话人数量与识别他们这一更 困难的问题密切相关，而这是说话人聚类主要解决的问题。凭直觉，如果系统能 够分辨谁在何时发言，那么自然也就能够分辨出实际上有多少人在混音中活跃。 此策略称为••通过检测计数”。一个理想的说话人聚类系统将能够使用此策略充分 解决说话者人数估计问题。
但是，当一个人仅对同时发言的人数感兴趣时，通过说话人聚类去解决会使 问题过于复杂化。此外，当说话人同时处于活动状态时(如在实际的鸡尾酒会环 境中)，现有的说话人聚类将失败，如uis-mn系统。实际上，重叠的语音片段通常 是说话人聚类的主要错误来源[37]。在具有多个同时发言人的“鸡尾酒会”场景中， 一个典型的假设是并发的发言人数量是已知的，这对于后续处理的有效性至关重 要。然而，在实际应用中，通常无法获得有关并发发言人实际人数的信息。令人 惊讶地是，目前仅存在很少的方法来解决计算发言者人数的任务[38] o
受深度学习方法在各种音频相关任务中的近期令人瞩目的成功的推动， Fabian-Robert Stoter等人致力于开发一种用于直接说话人数量估计的方法，提出 基于单通道的说话人数量估计(Single Channel Speaker Count Estimation)方法 CountNet[36],可有效的检测包含overlap状态下的说话人数量，并能检测最高达 10人同时说话状态下的说话人数量，即使在很小一段语音片段中包含多说话人， 该方法也能达到识别5人以内同时说话情景下的数量。BP overlap detection可以使 用单帧的说话人数量估计的结果，0代表有overlap, 1代表无overlap□并在最多10 位演讲者的混音中显示5秒语音片段的结果。
由于说话人数量估计方法在overlap情形下的有效性，本文提出将说话人数量 估计与说话人聚类任务联合的方法，解决多说话人同时说话问题。然而count-net 仅能支持固定长度的语音输入，无法有效完成不等长的语音数据的说话人整体数 量估计。
因此本文提出基于GST模型的说话人数量估计方法，支持变长数据的输入， 识别结果为有效范围内任意长度语音中包含的说话人整体数量。本文的说话人聚 类系统中对该模型有两种使用方式，一对长语音数据整体使用说话人数量估计， 估计整体说话人聚类的类别数，这对于聚类而言，是至关重要的信息。因此本文 提出在说话人聚类阶段结合CountNet的类别数估计的聚类方法。
此外，使用短语音片段，与utterance长度对应，以估计说话人聚类系统中每
19

个输入对应的同时说话的说话人数量，既overlap detection,用于与uis-mn的beam search过程中，实现重叠场景下的多标签聚类。整体说话人聚类系统将在第五章 中详细介绍。本章介绍当前研究中主流的的说话人聚类方法CountNet的实现方 法，重点介绍本文提出的基于GST结构的说话人数量估计方法。
4.1 CountNet说话人数量估计
传统单说话人聚类系统通常假设同时仅有一个说话人，由此分割语音片段来 区分不同说话人。当像真实的“鸡尾酒会”环境中的声音完全重叠时，这种分割几 乎是不可行的。而且，当说话人重叠时，开发一种算法来检测说话人数量是重要 的任务。因此，在CountNet研究中，CountNet尝试直接估算说话者人数。CountNet 基于最新的CRNN网络的神经网络架构，以推断出5s短音频段中的说话人。
说话人数量估计任务可以描述为估计最大同时发言者数量，kez+,在单通 道信号中，混合语音信号为x,说话人数量估计是通过应用从x到k的映射来实现 的。令x为N个样本的时域信号，表示L个唯一的单个说话人语音信号si的线性混 合。自然地，不是所有的说话者Z = 1,...,L在每个时间实例都处于活动状态。因此, 对于每个样本n,引入一个潜在的二进制语音活动变量vnZ£{0, 1}0然后，说话 人数量估计的任务是估计k:
k = max(^ vnl)
i=i
可以看出，估计kSL的任务与源分离更紧密相关，而L本身的估计对于说话者 不重叠的任务更有用。假设除了并发发言人的最大数量心“以外，没有其他可 用的先验信息，心“表示估计的上限。在图4-1中，以L=k = 3个说话人的“鸡尾 酒会''场景为例进行了说明。

图4"说话人数量估计卩6]
对于说话人数量估计任务，count-net的作者对比了可应用到当前任务的深度
学习的主流的网络架构，并通过结果分析表明CRNN网络结构比单独的RNN结构
20

表现更好，此外，作者分别将说话人数量估计任务视作分类任务与回归问题，并 发现分类的表现由于回归任务。因此提出了基于CRNN的分类算法解决说话人数 量估计问题，命名该模型为CountNet。
CountNet使用卷积递归(Convolutional Recurrent Neural Networks, CRNN ) 架构，并使用分类的方式对说话人数量进行检测。可检测5秒的段语音片段中的 说话人数量，可支持10人以内overlap的数量估计。唯一需要预先知道的信息是最 大说话人数量限定K,除此之外无需其余先验信息。CountNet网络输出为当前语 音片段内包含说话人的数量k,语音数据中说话人可同时说话，也可不持续说话。 即支持真实的复杂对话场景的说话人数量估计。
CRNN作为从输入X到输出y的映射函数加由yM(X)给出，其中通过监督 训练学习最佳参数(权重)Go CRNN的输出不一定是直接源计数k,因此我们引 入q(J作为决策函数，使得
k = q(fe(X))	(4-2)
使用｛X, k｝示例的训练数据库以受监督的方式训练CRNN。在这项工作中， 我们要研究CRNN的输出以及相应的决策函数由于使用分类任务的表现最 好，因此本文仅介绍其分类的相关决策问题。并默认决策方法为分类方法。分类 的决策方式中，输出分布直接视为离散的，放弃了与不同可能值的顺序有关的任 何含义。给定一些特定的输入X,网络使用softmax激活函数生成(心好+ 1 ) 类(包括上=0)的后验输出概率，并选择一个简单地选择最可能的最大后验 (MAP)决策函数，比如q = argmax^o尽管分类概念简单，但有两个缺点。 首先，不同估算值之间的直观排名会丢失：例如p仇=6)可能不取决于p仇= 5)。其次，对可能的最大数量心好进行先验处理。尽管有这些限制，基于分类的 方法已成功地应用于深度神经网络中，比如用于对图像中的对象进行计数。
最近的语音算法研究中，CNN和RNN结合的CRNN方法被很多研究人员使用 [45],[46],[47]o CRNN结构中有很多方式堆叠CNN和RNN模块，在CountNet应用 中，其动机是聚合来自输出卷积神经网络的局部时频特征，并使用LSTM层对长 时间结构进行建模。由于CNN层的输出为3D体积DxFxC,并且循环层的输入仅 采用2D序列，因此需要减小尺寸。自然地，时间维必须保持不变，因此通道维C 与频率维F堆叠在一起，得到DxF C输出。CountNet结构如图4-2所示。
CountNet损失函数釆用负对数似然损失，在给定似然参数;I和真实类别数k, 预测值为y时，损失函数计算如公式(4-3)所示：
2 — k * log(A + y)
21

Poisson Regression
FC 1, exp act
Classification
FC K, softmax act
图4-2 CRNN说话人数量估计模型结构a】
4. 2基于GST模型的说话人数量估计方法
GST (global style tokens)方法是解决语音合成中风格控制算法，由Yuxuan Wang等人在2018年提出［49］。它们生成的软可解释的“标签”可用于以新颖的方式 (例如，变化的速度和说话方式)控制合成，而与文本内容无关。它们还可以用 于样式传递，在整个长格式文本语料库中复制单个音频片段的说话样式。在针对 嘈杂的，未标记的发现数据进行训练时，GST会学习将噪声和说话人身份分解为 因数，从而提供了通往高度可扩展但功能强大的语音合成的途径。GST生成的 embedding也可以看作是一个外部存储器，用于存储从训练数据中提取的样式信 息。参考信号在训练时引导存储器写入，而在推理时引导存储器读取。
此外，GST作者还使用style embedding作为特征，通过线性判别分析对噪声 和说话人进行分类。结果表明，对于噪声分类，GST达到99.2%的准确度。对于 说话人分类，将TED视频ID用作真实标签，并与i-vector方法进行比较，这是目前 说话人验证系统中使用的标准表示形式。对于此任务，测试仪包含431个说话人。
22
在训练和测试短话语(平均持续时间3.75秒)的同时，可以看至!!GST与i-vector具 有可比性，作者推测，GST有可能应用于说话人聚类方法中。此外值得说明的是, GST直观，易于实施，当对语音数据进行训练时，GST模型会产生可解释的 embeddingo尽管GST最初被设计为模仿说话风格，但却是发现数据潜在变化的 通用技术。原论文提出GST可以应用于受益于可解释性，可控制性和鲁棒性的其 他问题领域。










图 4-3 GST (global style tokens)模型结构
本文受上述方法启发，将GST模型结构应用于解决说话人数量估计任务，提 出可以支持变长语音输入的说话人数量估计方法。较基于CountNet的只支持固定 长度的输入模型，更适合语音非固定长度的数据特性。
本文使用GST的输出style embedding作为说话人数量的潜在特征，这些 embedding可用于描述不同说话人数量下特征。随后将style embedding输入 softmax进行分类概率表示，最后使用交叉爛损失函数作为训练目标。结果表明 GST模型学会了将各种噪声和说话者数量因素分解为单独的样式标记。GST模型 包括三个部分，reference encoder, style attention, style embedding o 如图 4-3 所示。
整个信息流经模型如下：
(1) .首先reference encoder ,Skerry-Ryan et al., 2018在［48］论文中提出的参考
编码器将可变长度音频信号的韵律压缩为固定长度矢量，我们称其为参 考嵌入 (reference embedding)。
(2) .参考嵌入传递到样式标记层，在这里它用作注意力模块(attention)的查
23
询（query）向量。此处，注意力学习参考嵌入与一组随机初始化的嵌入中 的每个令牌之间的相似性度量。我们将这套嵌入（也称为全局样式令牌, GST或令牌嵌入）在所有训练序列中共享。
（3）.注意模块输出一组组合权重，这些组合权重表示每个样式标记对编码参 考嵌入的贡献。我们将GST的加权总和（我们称为样式嵌入，style embedding）传递到最后的分类层，计算类别概率。
下面对上述三部分进行详细介绍。
4.2.1参考编码器
参考编码器（reference encoder）用于压缩变长的语音信号为固定长度的矢量 特征，输出结果称为reference embedding0输入语音信号通常使用mel特征。 reference encoder由卷积堆栈和RNN组成，如图4~4所示。


图4-4 reference encoder模块。具有批处理归一化功能的6层卷积的6层堆栈，“递归池”以
总结可变长度序列，最后是可选的完全连接层和激活。[的
reference encoder,以对数梅尔声谱图作为输入，首先将其传递到具有3><3内 核，2x2步幅，批归一化和ReLU激活功能的六个二维卷积层的堆栈，6个卷积层 分别使用32、32、64、64、128和128个输出通道。然后将生成的输出张量整形为 3维（保留输出时间分辨率），为了将CNN层产生的序列压缩到单个固定长度矢 量，使用了具有单个128宽度门控循环单元（GRU）的循环神经网络。将GRU的 最终128维输出作为序列的汇总。为了从GRU的128维输出中计算出最终的最终 固定维度的reference embedding,应用了一个全连接层，将输出投影到所需的维 数，然后加上激活函数（例如softmax, tanh）o激活函数的选择可以限制嵌入中
24
包含的信息，并通过控制其大小来简化学习，最终得到reference embedding输出。
4.2.2特征注意力
注意力机制使2014年由Bahdanau等人［50］提出，主要用来解决Seq2Seq中无 法记住长句子的问题。广义上讲，它旨在将输入序列（源）转换为新序列（目标）, 并且两个序列都可以具有任意长度。转换任务的示例包括在文本或音频中的多种 语言之间进行机器翻译，生成问答对话框，甚至将句子解析为语法树。在某种程 度上，注意力是由我们如何视觉关注图像的不同区域或如何关联一个句子中的单 词所激发的。人类的视觉注意力使我们能够将注意力集中在“高分辨率”的特定区 域，同时以“低分辨率”感知周围的图像，然后调整焦点或进行相应的推断。
Scaled Dot-Product Attention
图4・5 （左）Scaled Dot-Product Attention。（右）由并行运行的几个attention层组成的
Multi-Head Attention」5】］
注意机制已成为各种任务中引人注目的序列建模和转导模型不可或缺的一 部分,从而允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离。 在论文"Attention is all your need" （V^swani 等人，）［51］中提出了 Transformer,它 是一种避免重复发生的模型体系结构，而是完全依赖于注意力机制来绘制输入和 输出之间的全局依存关系。其中一个最主要的结构是：multi head attention□
特征注意力，在本文指style attention,采用了Multi-Head attention结构，用于 获取reference embedding的相似性度量。Multi-head attention功能可以描述为将查 询和一组键值对（key-value）映射到输出，其中查询（Q, query）,键（K, key）, 值（V, values）和输出都是向量。将输出计算为值的加权总和，其中分配给每 个值的权重是通过查询与相应键的兼容性函数来计算的。
Scaled Dot-Product Attention （称为特别注意"点乘积注意力"）（图4-5左侧）。
25 输入由查询(Q)和维度比的键(K)以及维度比的值(V)组成。我们用所有键 计算查询的点积，每个除以“;，并应用softmax函数来获取值的权重。实际上， 我们在一组查询上同时计算注意力函数，将它们打包成矩阵Q,键和值也打包成 矩阵K和V。通过公式(4~4)计算输出矩阵。
AttentionQQ, K, V) = softmax(^=)V	( 4-4 )
此外，使用并行机制执行attention,产生维度为d”的输出值V,将它们连接 起来并再次投影，得到最终值。相比单个执行attention功能，产生d^odez维度 的键K,值V和查询Q,这种并行机制将查询，键和值分别以dk, dk和dv维度分别 学习不同的线性投影线性投影h次更有效的。其中h是参ahead numso这种并行 机制称为Multi-Head attention,如图 4-5右侧所示。Multi-Head attention结构允许 模型在不同位置共同关注来自不同表示子空间的信息。对于一个注意力集中的头 部(MutiHead),平均操作会抑制这种情况，如公式所示。
MultiHeadQQ, K, V) = Concat(headr,..., head^W0	(4-5)
where head： = AttentionQQW^, KW^,
其中投影参数矩阵叫° 6 RdmodelXdk ,	}	£ /JdmodeZxdv ,
IV。G J^^vx^m.odel o
4.2.3特征嵌入
reference embedding输入注意力模块，作为查询向量。在这里注意力机制学 习reference embedding和每个token之间相似程度。token随机初始化，是一组 embedding,即模型名称的由来，称为global style tokens, GSTs。在训练阶段在所 有训练序列中共享。注意力模块输出一组组合权重，这些组合权重表示每个style token对reference embedding的贡献，GSTs的加权总和称为特征嵌入，本文指GST 结构中的输出：style embedding。
style token层由一组style token embeddings和注意力模块组成。原论文种使 用10个样式标记(tokens),认为标记足以表示训练数据中的少量但丰富的韵律维 度。为了匹配文本编码器状态的维数，每个token embedding为256维度。
本文提出基于GST模型结构的说话人估计方法，在说话人数量估计任务中， 将style embedding作为说话人数量特征表示，并将其传递到softmax层，使用交叉 矯损失函数进行分类训练。本文使用10个token的默认配置，每个token的维度是 256,并在数量估计中假设最多9个说话人数量限制，因此包含静音状态，可视为 10分类问题。可合理的假设10个token正对应10个类别的样式信息。比较基于
26
CRNN模型结构的CountNet,基于GST模型结构的说话人估计方法适用于变长的 语音片段。
最后我们将style embedding用作说话人数量特征表征，将说话人数量视为分 类任务，style embedding输入sofitmax,交叉燔为损失函数，训练分类模型。
27
第五章说话人聚类系统实现
说话人聚类系统共分为五个主要部分，包括语音活动检测，语音特征提取， 说话人特征矢量d-vector,说话人数量估计，以及说话人聚类模块。
说话人聚类是指基于说话者语音特征的语音片段的无监督分类，在录音中识 别同一说话者说出的所有语音片段，并为其分配标签。如图5-1所示。本文的单 声道说话人聚类系统基于uis-mn的研究与实现。Uis-rnn实现了全监督的训练方式 用于解决说话人聚类问题，但是uis-mn缺点是对overlap场景下的聚类问题无法给 出有效结果。overlap场景是常见的语音场景，由于overlap带来特征训练鲁棒性的 困难，对多标签聚类问题提出调整需求，因此对于overlap的问题既是现实所需也 是当前的研究难点。
说话人B

图5-1说话人聚类任务

本文基于当前先进的说话人聚类方法uis-mn,提出支持多说话人同时讲话的 复杂场景下的说话人聚类系统。本文的主要贡献总结有以下几点：首先，在原始 uis-mn的方法中，本文提出改进的说话转换的概率估计方法，解决了原方法中说 话人转换概率为随时间不变的常数而不符合实际对话常见状态的问题，改进为随 时间变化的概率值估计。其次在说话人数量估计问题中，提出了使用GST的说话 人数量估计方法，支持变长的语音数据输入，并在240ms的短语音段下，实现有 效的overlap detection估计任务。第三，本文基于uis-mn提出了支持overlap的说话 人聚类方法。结合GST结构的说话人数量估计模块，本文使用其实现overlap detection任务，结合uis-mn的聚类过程，实现了对多说话人同时讲话场景的说话 人聚类问题。最后，本文结合GST,作为说话人聚类的类别数估计，在uis-mn的 聚类任务中，增加resegment模块，提升聚类的准确度。相关算法的原理以及改进 方法已在前文介绍，本章对本文提出的说话人聚类系统的实现过程进行总体介 绍。
5.1 UIS-RNN的细分聚类方法
28
说话人聚类问题的难点之一就是未知说话人具体数量，例如常见的聚类方法 k-means,谱聚类等，聚类数量的估计的准确度直接严重影响着聚类结果的表现。 此外，在其他语音研究方向中，说话人数量也是重要的先验信息，如语音分离等 任务，很多分离算法中，说话人的数量直接决定着语音分离的质量，因此很多研 究假设已知说话人数量，以此减小问题的复杂程度。然而，在实际的应用中，更 多的是未知说话人数量的情形下的应用场景，由此说话数量估计问题成为单独的 研究领域，此外类似问题还包括overlap的检测等。
传统说话人聚类方法再聚类之后，会根据结果进行边界调整，称为 resegmento Resegment常常能达到提高聚类结果准确度的效果，是基于传统方法 的说话聚类系统中的常见结构之一。然而，在基于深度学习的网络说话人聚类方 法uis-mn中，系统决策时并没有resegment环节。本人实验研究中发现，预先估计 说话人数量，并对说话人聚类应用resegment能够提升聚类结构的准确度。
本文在说话人数估计的任务中，分析当前主流的网络结构和当前的说话人数 量估计方法，提出了将multi heads attention的GST结构应用于说话人数量估计任 务中，获得了比baseline更好的性能，因此可作为预处理模块，估计说话人数量, 用于说话人聚类问题，增加说话人聚类问题的先验信息。结合基于音频的说话人 数量方法GST的基础上，在uis-mn的beam search决策过程，使用GST估计说话人 数量，并作为beam search决策的先验信息，进行再聚类操作。

图5-2聚类细分resegment

此外在多模态的实验中，使用基于人脸检测的说话人数量估计作为uis-mn的 再聚类依据，也获得有效的结果提升。因此对于多模态的聚类任务，本文所改进 的uis-mn系统能够利用说话人数量信息提升聚类的结果。
类似于说话人聚类的语音数据处理步骤，Resegment的实现方式为将原始语 音信号提取MFCC特征，直接作为说话人数量估计模块GST的输入，由于GST网 络支持变长数据的数量估计，因此对不同长度的语音都可以预测其所包含说话人 数量k。uis-mn的聚类过程中beam search执行uis-mn的每的说话人维持一个人
29 RNN网络生长，不同说话人之间维持不同的RNN,因此每一个RNN状态代表着 一个说话人，而根据第三章介绍，RNN的输出结果是类别的均值，因此可用视为 i亥类的类中心，不同的RNN的hidden时间交错，对应输入信号的随时间变化的说 话人身份。因此beam search在计算损失函数的同时维持着多个RNN状态，用于 记录当前时刻的说话人聚类结果和聚类状态。在已知说话数量的前提下，本文提 出在uis-mn的在线预测过程中，一旦当前RNN状态个数大于说话人数量k时，执 行resegmento具体执彳亍方式：
1, 计算当前RNN的均值之间的MSE loss,用于衡量不同类别的距离；
2, 对于MSEloss最低，即类别最接近的两个RNN状态之间，执行resegment 进行预测的类别结果合并，直到当前剩余RNN状态数量不再大于说话人 数量估计值：匕
本文实现了online方式的resegment,在beam search的决策过程中，既避免了 聚类数量多于限定类别数量造成的聚类误差，又在online形式的resegment的过 程中避免了聚类的误差累积，因此提高了聚类的准确度。其效果如图5-2所示， 当在说话人聚类的过程中，发现聚类的类别数超过预先估计的说话人总数，触发 resegment,对类的RNN状态下的均值计算MSE损失，合并其中损失值最小的类 别，损失值表明了不同说话人类中心的距离，直到类别数小于等于预先估计的说 话人总数。
此外，根据模型的结果于时间说话人数量估计的准确度，可设定beam search 的RNN状态数量阈值为可调参数，由此避免对说话人数量估计结果的绝对依赖, 避免了其带来过大的误差。本系统支持该方式，但由于说话人数量估计模块GST 在当前数据集的表现已到达较高的准确度，因此为了较少系统的复杂程度与调参 过程，本文的说话人聚类系统中并没有使用该参数方式，直接使用GST的结果作 为触发在线预测过程中的resegment^节的触发条件。
5. 2支持overlap的UIS-RNN说话人聚类方法
传统说话人聚类的研究主要针对单说话人聚类问题，即同一时间最多只有一 个人讲话的场景，然后多人同时讲话（overlap）的场景在日常对话中经常出现， 因此将单说话人聚类问题拓展到多说话人聚类是自然而然的研究需要，也更加能 满足复杂场景下的说话人聚类问题。然而由于大多数聚类方法为了增加模型的鲁 棒性，使用了单说话人的简单数据，其聚类结果也是单标签聚类，即一个说话人 特性向量只属于唯一一个类别标签，因此不能适应多说话人聚类的要求。
当前基于overlap的说话人聚类问题仍然处于研究的初期，比如以接近复杂场
30
景下的说话人聚类问题为目标的DIHARD比赛至今才是第二年。而其中的overlap 问题在多篇研究结果中仍然是说话人聚类错误率的主要因素。本文在分析当前的 研究中，仅发现少有针对overlap场景的说话人聚类方法［53］。在文献［53］中，作者 提到，重叠语音的挑战并非微不足道，因为它可能需要彻底重新思考差异化过程, 因为我们当前的系统根本不允许多个说话者对同一语音框架负责。因此可见基于
overlap的说话人聚类问题的研究难度。
在第三章所介绍的说话人聚类系统uis-mn中，其在数据的选择阶段丢弃了 overlap状态的数据，其结果虽然获得较高的表现，但在overlap问题上没有具体的 解决方案，其聚类结果属于单标签聚类问题。然而在对overlap数据上执行uis-mn 的聚类过程中，发现uis-mn具有对overlap中的某一人预测的准确度，如两个人同 时对话的情形下，uis-mn能准确的预测其中一个，而uis-mn本身只支持单标签聚 类的限制使其无法完成多标签预测问题，因此，可以说明uisn-mn说话人聚类方 法在overlap问题上表现出了解决多说话人同时说话的聚类问题的潜在能力。
本文在确认uis-rrm聚类效果的基础上，提岀将说话人数量估计模块与uis-mn 聚类方法相结合，解决带有overlap对话场景的说话人聚类问题。说话人数量估计 模块在用于resegment问题之外，同时用于overlap detection任务，区别在于输入语 音段的片段长度不同。在overlap聚类上，使用240ms的短语音片段作为说话人数 量估计模型的输入，而非完整的全部语音信号（resegment任务中，使用完整的语 音信号作为说话人数量估计模块，以估计总共的类别数量）。因此该结果可认为 是当前语音片段内的overlap数量检测结果，同样，以240ms的语音片段提取civector 向量作为 uis-mn 聚类模型的输入， 对该语音片段进行聚类。在此过程中，可

在第三章中与上节中已经对uis-mn的聚类决策算法beam search进行了详细 介绍，因此可以了解beam search的决策环节的损失函数计算以及RNN状态更新与 状态保存过程。本文在实现决策算法时，将说话人数量估计结果作为一种输入信 息，根据overlap detection结果，对当前类别聚类状态进行判决，不再执行最小loss
31
所对应的类别标签为当前语音片段所对应的说话人类别，而是根据uis-rnn的损失 值与各个说话人的RNN状态结果预先判定overlap状态并计算其对应的概率值。
其后融合overlap detection结果最终判定当前语音片段中overlap的数量以及其所 对应的全部类别标签。以此完成多标签聚类过程，解决overlap状态下的多标签聚 类任务，效果展示如图5-3o最后整体的系统流程如图5-4所示。
1州*树屮	涮协川
UFCCie征表示
d-vector生成	说话人数詔估计
Overlap数量	整体说话人数量

BI
图54说话人聚类系统
32
第六章说话人聚类系统结果
本文实验分为两部分，第一部分实验为说话人数量估计任务，比较基于 CRNN的count-net模型结构的baseline方法和改进的方法-基于multi heads attention 的GST模型结构的说话人估计方法。第二部分实验用于比较说话人聚类任务，论 文［25］提出的uis-mn方法为baseline,本文中提出的改进后的说话人聚类系统，包 括说话人转变概率估计与rsegment。此外，说话人聚类实验中，本文提出的结合 说话人数量估计解决overlap场景下的说话人聚类方法，在多说话人（overlap情形 下）聚类效果上与baseline进行对比。
6.1实验数据与评估指标
6.1.1实验数据集
迄今为止，许多可用的语音数据集都只包含一个说话人处于活动状态。如第 一节所示，包含重叠语音段的数据集缺少准确的注释，因为混合中语音开始和偏 移的注释对人类来说相对复杂，或者缺乏可控的听觉环境，例如电视/广播场景。 由于没有完全重叠的（overlap）说话人讲话场景的真实数据集，因此我们选择手 动生成合成带有overlap的混合语音数据，即模拟“鸡尾酒会场景”的数据。
我们认识到，在模拟的“鸡尾酒会”环境中，混合语音数据缺乏人际交流的对 话内容，但提供了受控的语音环境，有助于理解说话人数量估计模型（如baseline 中的CRNN结构和本文提出的基于GST结构的方法）如何解决计数估算问题与说 话人聚类模型对于overlap数据的处理能力。我们选择了一个语音语料库，该语 料库优先选择大量不同的说话者，而不是发声的次数，从而产生了大量独特的混 合语音数据。这部分数据的选择与处理借鉴论文［36］中的方法。为了进行训练， 我们选择了与论文［36］相同的数据集LibriSpeechclean-360［54］,其中包括921名说 话者（439位女性和482位男性说话者）的363小时英语语音，并以16 kHz采样。
为了生成单个训练样本{X, k},我们从语料库中提取了一组独特的L个说话 者。对于每个说话人，选择其一个随机语音，重新采样为16 kHz并应用WebRTC VAD,在第二章中介绍的基于GMM模型WebRTC规范的VAD方法。本文在生成 overlap语音时，在心好内随机选取人数，即为真实类别标签（ground truth） ko根 据该人数设定，从数据集中随机选取speaker,之后，从该说话人的语音数据中随 机选择一个，为合成语音中的一个说话人数据。接下来WebRTCVAD估计用于消
33
除发声记录开始和结束时的静音。随机选择混合位置，混合不同说话人的数据， 直到达到所设定的说话人人数。对所有说话人重复进行此过程，以便创建L个时 域信号。信号经过混合和峰值归一化以避免削波。
此外本文实验中，说话人聚类任务的训练数据集包含超过14万个音频剪辑， 总共约100个小时，所有视频片段均来自YouTube。此外，该数据集是中文数据 集。
本文使用LibriSpeech数据集手动合成overlap多说话人数据，并在训练说话人 数量估计模型时，加入上述的中文数据集。除了增加数据量的作用外，该数据集 包含加入了真实的生活背景音等，增加了数据集中的语音环境的多样性，此外 LibriSpeech为英文数据集，加入中文数据集，增加了数量估计任务中，模型对语 言的鲁棒性。由于本文需要使用说话人数量估计模型检测overlap数量，且时长片 段最小需要达240ms,因此额外合成了不同长度的overlap语言数据，用于增强 countnet模型对于不同时长的语言数据的检测能力，尤其是非常短小的语音片段。
最后另一份需要说明的测试集是LibriCount数据集。在countnet论文中，作者 合成并公开了LibriCount数据集，LibriCount数据集包含［0..10］个说话者的模拟鸡 尾酒会环境，共包含5720个持续时间为5秒的多说话人混合语音数据。因此在测 试countet模型时，我们也使用LibriCount作为另一个用于与countnet-baselien模型 对比测试数据集，进行说话人数量估计任务的测试数据。
6.1.2评估指标
本文同论文［36］—样，使用MAE作为说话人数量估计评估指标，在说话人数 量估计任务中，模型输出y被视为分类问题，我们将最终输出k评估为离散回归问 题。因此，我们釆用平均绝对误差(MAE),它也常用于其他与计数相关的任务。 由于MAE取决于真实计数k,因此我们还将每个类的MAE具体表示为公式(6-1), 然后将其平均化为各个类别，即公式(6-2)所示。
(6-1)
在评估说话人聚类任务中，本文使用DER (Diarization Error Rate) ［55］作为 说话人聚类性能表现的评估指标。该误码率是语音/非语音错误和说话人类别错 误的总和。语音/非语音错误是遗漏和错误警报错误的总和。说话人类别错误是
34 每当说话者的语音片段归属于另一个语音片段时发生的聚类错误。此指标已在多 个NIST丰富转录评估活动中使用。
DER用于评估自动语音识别系统。DER定义为输入信号中被差分输出错误标 记的百分比。在持续时间dur(s)的第s个片段中，川呵和是分别由注释指示和 由系统假设的说话者数量，是在第s个片段中与注释和假设正确匹配的说 话者数量，如公式(6-3)所描述。
小 _ Ss=l	(max(Nref Miyp) ^correct)	(6-3)
UcK =	^7
Xs=idur(s).Nref
对DER的贡献来自三个因素，即语音丢失率(MSR),错误警报语音率(FASR) 和Speaker Error o当语音段被标记为非语音时，该错误将归因于MSR。FASR是 当检测到无语音却被标记作为语音段时的错误。SpeakerError错误归因于说话 者聚类和细分。如果未检测到说话人变化，音段分割过大，错误聚集，则可能导 致这种错误。这三个错误的总和构成DER,即公式(6-4)所描述。
DER = MSR + FASR + SpeakerError	(6 一4)
6. 2说话人数量估计结果
本文为了解决多说话人同时讲话(overlap)的'鸡尾酒会'场景下的说话人 聚类问题，引入了说话人数量估计方法，作为overalp数量估计方法。同时使用该 模块作为resegment环节中的总体说话人数量估计模块。此外，本文受基于multihead attention方法的GST模型启发，在说话人数量估计任务中GST模型结构，提 出基于GST模型结构的说话人数量估计方法：GST-Count-Neto
此外，目前主流的说话人数量估计模型结构是基于CRNN的Count-Net模型， 为了便于区分，本文中将其命名为CRNN-Count-Net,本节中对本文提出的GST- Count-Net和CRNN-Count-Net进行对比实验，分析两个模型结构下的说话人数量 估计的实验结果。本次实验中，设置GST^Count-Net的attention结构的head num=10,对应于本文说话人数量估计任务中最大的说话人数量限制上机购=9, 包括无说话人共10个类别。
由于在论文［36］的实验结果已表明，将说话人数量估计任务当作分类任务的 表现优于将其当作回归任务，因此本文仅针对分类判决任务的情况下，多说话人 数量估计的两种模型结构CRNN和GST进行对比实验，即输出y的方法不同，而通 过y计算k的方法相同，简言乙决策函数q(・)初風实验中，唯一的先验知识是 受限的最大说话人数量局“乂，本实验中假设kmax=lQo模型的输入信息为40位
35

MFCC特征，本实验结果在Ground truth为k=[0 ... 10]时，计算具有相等功率的 多说话人混合语音信号(包括多说话人同时讲话的overlap场景)的平均平均绝对 误差(average mean absolute error, MAE)。测试数据集采用论文中已公开的overlap 数据测试集LibriCountl 0。

实验结果如图6-1所示，GST-Count-Net模型与CRNN-Count-Net模型都能够 在说话人数k小于5的情况下得到小于0.5的MAE结果，GST-Count-Net可得到小于 0.4的MAE值。接<k= {1, 2 ... 7}之间CRNN-Count-Net的MAE是线性增加。而 GST-Count-Net却获得了更好的结果，并未受说话人数量增加而是估计的准确度 下降，反而取得更低的MAE值，在K>7的情况下能获得小于0.2的MAE误差，可 见GST-Count-Net在多说人存在的场景下能够可靠地估计说话人总数。此外基于 GST的数量估计模型在k>4的情况下，都表现出了优于CRNN结构的估计结果。

图6-3长时语音数据的GST-Count-Net结果

GSFCount-Net模型结构的另一优势是支持变长数据的说话人数量估计，本 实验中测试GST结构在不同长度的语音数据下的数量估计的结果，由于短时数据 没有真实的overlap数据，因此本文针对小于1秒的数据采用LibriSpeech数据集手 动合成了 overlap,其结果如图6-2所示，表明GST-Count-Net在200ms到1800ms之 间的说话人数量估计结果。结果显示GST-Count-Net对于时长呈现出了语音数据 时长越长估计误差越小的状态趋势，符合人的主观感觉。长度大于1秒后，GST- Count-Net结果已经取得了小于0.3的MAE误差值。此外，本文的说话人聚类任务 中的语音片段长度为240ms,所以为了GST-Count-Net能够在240ms的语言长度下 取得稳定的结果，本文增加了对该长度的语音数据，其估计误差较相邻的其他长 度有明显减低，因此本文初步推测GST-Count-Net受数据长度不平衡的影响，其 结果对训练数据的长度程依赖关系，鲁棒性不够，此外也表明了在更过数据的情 况下，性能提升的潜能。
GSTCount-Net在本文中的另一作用是估计长时语音的总体数量，因此本文 的另一个实验是在真是的变长的数据下，检测GST-Count-Net的结果。如图6-3所 示，在YoutTube数据集下，长度从Is到16s之间的GST-Count-Net结果。与图6-2结 果相似，GST-Count-Net误差随语音长度增加而呈现下降趋势。然而在5秒左右， 误差增加，通过对数据集的统计发现，训练数据中5秒的数据相比其他长度数量 差距明显，再次表明上述的GST-Count-Net结构对语音数据的时长敏感的特点。
通过上述的结果发现，不同时长下的GST-Count-Net的估计误差值(MAE结 果)差别较大，一方面数据过短，最小达200ms,给模型提出了更大的挑战，另 一方面，数据中不同时长的数据表现不均衡，且其中每一个人的说话人时长也存
37
在不均衡现象。但值得说明的是，实验结果表明了本文提出GSFCount-Net数量 估计方法对于变长语音数据的估计的有效性，包括短时语音的overlap的检测与长 时语音的说话人总体数量估计。
6. 3非overlap场景的说话人聚类结果
本文基于baseline［25］的uis-mn方法，在其基础上提出新的说话人转变概率估 计方法，改进了原有的常数概率估计方式。此外，本文提出基于GST结构的说话 人数量估计模型，支持变长语音数据的输入，预先估计每个数据中最大说话人数 量。然后，本文在uis-nrn的beam search决策的过程中，根据当前所划分的类别数 与已估计的最大说话人数量，对uis・rnn说话人聚类结果进行再聚类调整，避免了 聚类类别结果多于以估计的最大类别数量的情形，一定程度上减少了聚类误差。
Sample Wav
的说话人部分分类错误，分基本正确
本文使用uis-mn论文中相同的训练d-vector的技巧，即通过使用可变长度窗 口重新训练了d-vector,其中窗口大小是在训练过程中从［240ms, 1600ms］内的均 匀分布得出的。在测试环节，采用窗长240ms为一个待识别的语音片段，假设每 个240ms语音片段内说话人是一直处于说话状态。在beam search的解码过程，使 用beam size为 10的约束宽度(beam size=10)。
与论文［25］相同，我们有以下几项实验约束条件：
(1) .我们评估单声道音频。
(2) .我们从评估中排除重叠的语音。
(3) .我们容许段边界小于250ms的错误。
(4).我们报告了混淆错误，在文献中通常将其直接称为Diarization Error Rate (DER)o
实验结果如下表所示，其中baseline为原始uis-mn, zt-uis-mn为改进说话人转 换概率估计方法后的uis-mn,而resegment-uismn是在zt-uis-mn基础上增加了resegment环节的说话人聚类模型，各个模型的说话人聚类的DER结果如下表所示。
表6T,各个模型的DER结果比较

结果表明，改进说话人转换概率估计的方法后，聚类误差DER下降2.6%,而 增加了resegment环节后，DER下降达6%左右。当然resegment的提升效果与说话 人总数量估计的准确度直接相关，为了避免总人数估计误差对聚类结果产生过的 的负面影响，在执行re-segment的过程中可通过调整阈值方式稳定聚类结果，本 文当前实验并未采用该方式，直接以说话人数量估计结果为resegment决策值。此 外，由于本文采用的online方式的resegment,可以在聚类的过程中对结果进行及 时纠正，避免了错误的聚类结果的累积。其输出结果的结果与真实标记在三个说 话人情况下对比如图6-4所示。
6.4 Overlap场景下的说话人聚类结果
多说话人聚类是指在同一时间存在多于一个说话人讲话的场景下的聚类问 题，此时的说话人聚类任务中，不再简单的每个语音片段仅分配一个类别标签， 而是存在多于一个标签的聚类问题，即多标签聚类问题。
本文致力于在不改变原有的矢量特征(i-vector, d-vector, x-vector等)提取 的方法基础上，解决overlap(或鸡尾酒会问题)场景下的说话人聚类问题，一方面 这些特征矢量是研究过程中被不断改进后表现优秀的好的矢量特征表示方式，并 在多种任务表现稳定。因此本文提出的支持overlap复杂场景的说话人聚类方法是 在现有的矢量表征方法的基础上进行。
本文在论文［25］所提的sui-rnn的方法基础上，分析了overlap情形下的其d- vector间的关系，发现其呈现了明显的距离联系，即overlap的d-vector更接近其包 含的说话人的d-vector。因此本文提出在uis-mn的决策过程种，预先估计语音片段 的overlap数量，通过该overlap估计结果对uis-mn的聚类算法进行改进，根据beam search的mse损失函数关系对当前语音片段分配多标签，实现多标签聚类任务。
39 即通过该overlap估计结果对uis-mn的聚类算法进行改进，在其beam search决策算 法中，融合overlap概率估计结果，使得beam search不再是单标签聚类算法，而支 持多说话人多标签聚类问题。
此外，由于原论文中的uis-rnn方法只支持单标签聚类，导致该方法一方面无 法对overlap的语音片段执行多标签聚类误差，另一方面由于带来overlap特征向量 会分配给其最接近的说话人类别，因此在beam search决策中给该类带来误差，影 响其类中心的稳定度，使得非overlap的语音片段的说话人聚类算法结果也明显下 降。
本文提出的overlap下的说话人聚类系统，实现多标签聚类，避免了overlap的 d-vecor被分配给任意说话人类别，改进后的多标签beam search算法对于overlap特 征向量不再进行更新聚类中心均值计算，因此一定程度上减少了 overlap复杂场景 对于非overlap语音片段下的说话人类别聚类结果的影响。实验结果如下表所示， 本实验的测试数据是在手动生成的具有overlap的数据集上测试。该baseline是使 用了本文提出的说话人转换概率估计方式和增加resegment结构后的结果，上节 实验中的最优结果所对应的模型，即非overlap场景下，说话人聚类结果为DER
结果表明，本文提出的针对overlap的说话人聚类方法能有效的对overlap情形 下进行说话人的多标签聚类，并通过额外的overlap检测模块直接对说话人聚类的 决策阶段进行改进，并未改变原有特征处理、特征提取与聚类模型的结构等，因此可直接有效的从单说话人聚类过渡到多说话人聚类。该系统在手动合成的多说 话人的混合语音数据上获得9.76%的DER。
第七章总结与展望
7.1总结
在过去的很长一点时间，说话人聚类领域主要针对非重叠声(overlappedspeech) 情况下的聚类问题。随着技术的进步和研究的需求提升，对于重叠声音 事件(overlap对话场景)的分割聚类研究也逐渐引起研究者的重视。但是overlap 场景的说话人聚类问题依然是当今的研究难题，一方面是由于'鸡尾酒会'场景 下的语音问题研究的难度和复杂度，目前还没有有效的统一的解决方法，另一方 面也是由于对于人耳听觉的感知系统的掌握还不够充分。对于复杂场景的听觉感 知方法还不够明确。
本文致力于实现基于单通道的overlap场景/鸡尾酒会场景下的说话人聚类问 题，在不改变原有的说话人矢量特征表示的情况下，通过对聚类方法的改进实现 支持overlap场景的说话人聚类方法。在论文［25］中，作者提出了一种说话人区分 系统uis-mn,其中常用的聚类模块被可训练的无界交错状态RNN代替。由于可以 通过监督方式学习该系统的所有模块，因此在可以使用带有高质量带时间戳的发 言人标签的训练数据的情况下，它比无监督系统更可靠。此外该方法是一种在线 算法，其性能优于最新的频谱离线聚类算法。
本文通过对uis-mn的决策环节beam search进行改进，提出几种改进方法， 使其在非overlap场景下的说话人聚类效果获得明显提升，并在研究overlap的问题 下，根据分析其d-vector关系，提出基于说话人数量估计方法上，实现overlap场景 下的多标签聚类问题。本文提出的具体改进方法有。第一，提出了新的说话人转 换概率的估计方式，解决了原有估计方法为固定常数不符合说话场景状态问题。 其结果较baseline系统上提升2%的准确度。第二，提出新的说话人数量估计方法, 基于multi-head attention的GST结果的说话人数量估计方法，相比论文［36］中提出 的方法CounfNet,本文方法支持变长的语音输入，并且在说话人大于5的情况下 获得更低的MAE误差。此外，相比较conutnet支持5s的数量估计限制，本文所提 出的基于GST的说话人数量估计方法将其缩短至200ms的长度，仍能获得较低的 MAE结果，可作为有效的overlap detection方法，适用于说话人聚类任务的语音片 段的最小长度需求。同时在长时语音信号16s的语音数据中，GST结构的说话人 数量估计依然能达到较好的性能表现，可完成整体数量估计任务，适用于说话人 聚类，语音分类等研究。
本文实现单声道下的说话人聚类，并在现有的先进方法uis-rnn的基础上提出
说话人数量估计与resegment的聚类调整方法，获得6.18%DER的结果，较原方法 14.84%,有显著提升。此夕卜，本文提出新的基于multi-headattention的GST模型结 构的说话人数量估计方法，结合该方法，实现对说话人数量进行预先估计，并在 overlap的情形下获得有效的overlap的说话人估计值，并联合uis-rnn的beam search 的决策方法，实现了overlap复杂情形下的说话人聚类问题，并获得9.76%的DER 结果，表明了该方法的有效性。
此外，我们介绍了使用数据驱动的方法来估计在模拟的“鸡尾酒会”环境中同 时发言的最大人数的任务，并讨论了如何在深度学习环境中构造此任务。
目前研究现状的基础上，我们研究了将说话人数量估计任务作为分类任务下 的CRNN模型结构和GST模型结构，进行了实验以评估不同的网络体系结构。在 第六章，我们评估了数据的平衡问题，如说话人的人数不同或者说话人持续时间 不同等情况下的估计结果，本文提出的GST-Count-Net方法能够在多说话人的情 况下获得比CRNN-Count-net更高的准确度。此外语音长度越长，GST-Count-Net 估计结果取得2更低的误差。但是GST-Count-Net也呈现了对训练数据时长的敏感 性，对语音数据长度不平衡问题，GST-Count-Net方法的鲁棒性不足。一方面由于 缺失overlap的数据，手动合成的数据不能满足多种要求，另一方面需要从模型方 面进行进一步改进。此外，本实验中另一种数据不平衡现象值得注意，语音信号 中每个说话人讲话时长的不平衡问题，也会对结果产生影响。
深度学习方法依赖大量的数据，然而目前overlap的真实数据集难以获得，本 文实验中说话人重叠数据采用手动合成方式，由此对于语音环境的模拟与真实语 音数据相比可能还需要进一步实验。比如手动合成的overlap语音数据中，每个说 话人的音量差异较小，而在更现实的环境下，说话人的音量通常也会存在明显不 同，如不同方位，距离远近的说话人，也会对说话人数量估计问题带来影响。在 论文［36］中，其参数优化结构方法里也提及该问题，并通过引入介于0.5和2.0之间 的增益因子（随机应用于音源）来模拟这一点，通过实验结果证明增加混合增益 确实会对性能产生影响，说明手动合成的语音与真实overlap语音数据还存在差 异，影响实验结果。
7. 2未来展望
本文提出的单通道说话人聚类系统在目前先进的算法uis-mn基础上解决多 说话人同时对话的场景下的聚类问题，提出新的说话人估计方法GST-Count-Neto 本文GST-Count-Net支持变长语音信号的输入，但是在实验中发现其对数据的时 长比较敏感，在语音长度不平衡下，结果波动明显。此外，在短语音片段小于Is的情况下，其MAE误差值相比长语音数据误差较大，因此需要进一步改进段语音 下的说话人数量估计方法。
说话人数量估计任务中存在多种数据不平衡问题，如语音长度不平衡，说话 人时长不平衡，说话人音量不平衡等问题，如何提升数据不平衡下的说话人数量 估计的性能以及说话人聚类的结果，解决语音数据多种不平衡问题，也是下一步 工作的重点之一。
此外，当前的说话人聚类系统与说话人数量估计系统分开单独训练，未来会 进一步研究两个模块uis-mn和GST-Count-Net的联合训练方式下的效果，并探索 更有效联合训练的损失函数。

参考文献
[1] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, "End-to-end neural speaker diarization with permutation-free objectives.,, in Proc. Interspeech. 2019.


























































































































