2.2.2CNN基本原理
图2-3展示了 CNN的结构。CNN可以视作DNN的简化。CNN对DNN的 简化主要体现在三个方面，分别是局部连接、权值共享和池化。
图2-3卷积神经网络结构
局部连接与全连接相对。在DNN中，层与层之间为全连接，每个层与上一 层所有的神经元相连，由于DNN经常处理的是一维数据，全连接结构易于实现 并且容易使用矩阵乘法表示。在处理多维数据如二维图像时，如果将多维数据展 开为一维，也可以使用DNN处理，但是需要训练的网络参数会非常多。CNN使 用局部连接可以有效减少所需训练的参数，在CNN中，每层神经元仅与上一层 相近的神经元相连，因为通常来说特征图中相邻区域的相关性强，距离远的区域 相关性较弱，釆用局部连接亦可以保证具有足够的信息提取到需要的特征。此外， 随着网络层数的增加，感受野会随之扩大，采用局部连接也可以使用到更多的输 入信息。
权值共享也是CNN减少所需训练参数的一个重要手段。局部连接时，每层 神经元的输入为上一层相连接的神经元输出加权平均的函数，其中的可训练权值 被称为卷积核。每个卷积核都会滑动遍历上一层所有神经元，与之进行对应相乘 再求和的操作，该操作类似卷积，由此可以得到一个特征图。如果每个神经元与 上一层相连接的神经元之间的权值与偏置都不相同，则仍会产生大量待训练参数, 为简化网络加快训练，故而使用卷积核进行处理。神经网络训练时，类似于DNN, 但是调整的参数为卷积核的值。通过增加卷积核数可以增加生成的特征图，增加 网络特征提取的能力。
池化可以降低网络规模。局部连接和权值共享是在卷积层采用的，池化层在 卷积层之后，用于减少卷积层生成的特征图的尺寸。池化简单来说是对特征图进 行下采样。直接使用卷积层生成的特征图可能需要消耗更多的存储空间和更大的 计算量。对特征图进行池化时，会将特征图按一定规则划分为若干个互不重叠的 区域，池化层神经元与对应区域每个神经元相连生成一个数值，池化后的数据组 成新的特征图。在CNN中，对特征图进行池化可以获取抽象的表达，可以用于 应对输入数据可能出现的平移旋转等变化【均。池化常用的方法有最大池化和平
18 均池化。最大池化进行的操作是选取区域中的最大值，平均池化选取的是区域中 的平均值。最大池化操作简单快捷，并且对于数据特征图发生的变化有一定的鲁 棒性，虽然未使用区域的所有信息，但是通常能够取得更优的效果厲〕。
CNN通常使用多个卷积层和池化层对通过输入层后的数据进行处理，在输 出层输岀所需要的结果。卷积层的局部连接特点便于CNN利用输入数据的空间 相关性，权值共享使每张特征图对应一个卷积核，网络训练时只需调整卷积核中 的参数，因此减少了需训练参数数量，更容易优化网络；池化层中，池化操作具 有降低网络过拟合的风险和提高所训练模型的泛化能力的优点。CNN利用这两 种特殊结构能够减小网络规模提高网络训练效率［殉。
批标准化(Batch Normalization, BN)最先由文献［37］引入机器学习以加快网络 训练。当设计的神经网络具有很多隐藏层时，在网络训练时，各层参数的变化会 导致各层输出数据的分布发生变化。当输出数据分布渐渐向激活函数饱和区域 (如tanh函数靠近-1、+1的区域)偏移，训练会变得艰难，文献［37］称该现象为内 部协变量偏移，同时提出了有效的解决方法，即批标准化，用以归一化神经网络 层的输入。BN具体实现为先求每层待输入的数据的均值和方差，利用这两个参 数对输入数据进行缩放和平移，将输入数据的分布转化为零均值方差为1的分 布。通常经此处理后，神经网络对输入数据敏感度降低并且可以使用更高的学习 速率训练，以较少的迭代获得更优的模型［37］［38］。
CNN在使用卷积层处理之前，会对输入特征图的外围边缘进行零填充操作 卩9】。主要有两种填充方式，分别为“same”和“valid”。卷积核大小为1x1时，两 种填充方式相同。使用“same”模式进行填充时，卷积前后输入数据和特征图的 大小相同，因此需要在输入数据周围填充适当的“0”数据。“valid”不采取填充， 经过卷积操作后，特征图大小通常小于原输入数据。
在CNN中，卷积层可以提取输入数据的不同特征，浅层的神经网络可以提 取低级特征，随着网络层数的增加，神经网络提取复杂特征的能力增强。卷积核 本质就是一个很小的矩阵，矩阵规模通常取3x3、5x5等。卷积核通过设定的步 长在前一层数据上进行平移，每移动一次，卷积核对应位置与之相乘求和，再加 偏置经过激活函数，就可生成一个特征值。一个卷积核可以生成一张特征图，故 通常使用多个卷积核处理上一层数据，以提取多个特征增加网络表达能力。网络 优化时，只需要按特定规则调整卷积核参数，优化算法与DNN类似。卷积层后 若需要对特征图进行降维则通常接池化层，对特征进行选择，进一步减少参数数 量。在池化操作时，窗口内的数据共同生成一个值，由此特征图成倍缩小，极大 减少了需要处理的数据量，从而精简了网络，减少需训练参数［均。
