2.3.1卷积结构
卷积神经网络的基本模块包含三个串行的连接层：卷积层、激活函数层以及 池化层，如图2-3所示。
图2-3卷积神经网络
卷积层通过一个卷积核提取图像上每个局部的特征，得到一个具有线性映射 关系的卷积图像特征图，每处的局部特征都接受了一个卷积核大小的输入。激活 函数层则是通过激活函数使线性图像特征产生了非线性变换，使模型能够学到非 线性的特征表示。最后的池化层进一步调整特征图的大小，减少模型的训练参数。
(1)卷积层
卷积是信息处理中一种非常常见的操作，其中一维卷积可以用如式2-3的微 积分公式表示：
s(f) = | x(t — a)w(a)da
其离散公式如2-4表示为：
其矩阵形式为式2-5：
其中*表示一维向量的卷积操作，s(t)为一维卷积的结果输出，x和w分别代 表原始数据输入和卷积核的学习参数。以上是关于一维向量卷积的数据操作，而 卷积神经网络则是进一步将一维卷积扩展到了二维卷积，用于对图像等二维数据 进行特征提取，卷积神经网络中的二维卷积操作如式2-6所示：
其中s代表二维卷积输出，*表示了原始图像和卷积核的卷积操作。该公式并 非严格数学意义上的卷积，但其与式2-3至2-5中的一维卷积过程十分相似，因 此业界也将其称之为卷积神经网络结构。
在卷积神经网络的卷积结构中，一个神经元只与其部分邻接的神经元相连。 在CNN的一个卷积层输出中，通常包含了数个特征平面（fbatureMap）,每个特 征平面由矩形结构排列的神经元组成，同一个特征平面的每个神经元即局部特征 具有共享权值的操作，即共享同一个卷积核的学习参数，特征平面的每处局部都 使用同一个卷积核参数进行计算，这种稀疏的连接方式就是卷积神经网络和普通 神经网络的最大差别。通过这样的权值共享机制，卷积神经网络可以大大减少所 需要的机器学习参数量。卷积核参数一般以一定的分布例如0均值高斯分布的随 机小数来对模型参数进行初始化，在网络的训练过程中通过损失误差的反向传播 迭代更新卷积核参数，找到最优的机器学习模型。
典型的卷积层操作可以用图2-4的步骤来表示，其中，输入是一个5*5*3大 小的原始输入，最后的维度3代表了输入具有的3个通道，在二维图像中可以理 解为RGB三种颜色或者其他颜色格式表示的不同图像风格。为了便于表示卷积 过程，本文使用了其中一个通道作为示例对图像进行卷积操作的表示。其中，常 规卷积神经网络中的卷积核除了拥有3*3表示卷积核的大小之外,还具有和输入 数据相同通道数的多个单层卷积核用于对输入的每个通道分别卷积并求和。
图2-4卷积层操作
卷积运算的特点为神经网络提供了重要思路，其共享权值的模式不仅减少了 机器学习所需要的参数量，还进一步提高了神经网络模型的泛化能力。卷积神经 网络的共享权值机制与普通全连接神经网络之间的主要区别在于是否进行局部 运算，如图2-5表示，卷积神经网络的每个神经元只与与其相连的卷积核大小的 局域输入相乘，而全连接的神经网络则对每个输入值都进行相乘的操作。
图2-5全连接神经网络（左）与卷积神经网络的局部连接（右）
（2）激活函数层
由卷积函数的公式可以看出，卷积操作是一层层线性的特征变换。但样本的 分布却常常是线性不可分的，例如简单的异或关系（XOR）,就需要输入数据之 间进行相乘的与非操作，因此需要通过激活函数来引入模型的非线性学习功能， 拟合非线性的数据输入，解决线性变换所不能解决的机器学习问题。另外，激活 函数还具有控制数据变化范围的功能，多层神经元的输出层逐层乘积可能导致输 出结果过大或过小，中间数据过大及过小均有可能导致模型的反向传播失效，模 型的学习参数无法继续回归到合理的数值范围，因此需要使用激活函数控制神经 网络每个层次的数据输出值。在深度学习网络中经常使用的激活函数包括S型 （Sigmoid）函数、正值滤波（ReLU）函数以及双曲正切（Tanh）函数几种。
S型函数：S型函数是机器学习中最早使用的一种激活函数，在深度学习技 术开始流行之前，S型函数就已经作为逻辑回归的计算方法广泛应用于传统的机 器学习中。它的公式定义如式2-7所示，其形状如图2-6所示。可见，S型函数 可以将输出控制在0到1之间，这使得它天然适合于二分类的机器学习任务，并 且由于S型函数与对数损失函数具有非常一致的非线性函数形式，因此在机器 学习早期的研究中，S型函数就经常被用于各种场景下的机器学习研究。但同时， 由于其导函数值被限制在0到0.25之间，在多层的神经网络反向传播时容易导 致梯度相乘消失的现象。
(2-7)
由式可知正值滤波函数是将输出小于0的部分映射为0,而大于0的部分则 维持正值不变，通过该函数可以很轻易地实现一些与或非的逻辑功能；正值滤波
13 函数将小于0的输出置0,不仅起到了非线性函数的功能，还具有稀疏化机器学 习参数的作用，即训练过程中将一部分学习参数永久置零，可以在一定程度上提 高机器学习模型的泛化能力。其缺点在于训练初期机器学习的中间层输出可能并 不稳定，正值滤波函数有可能过早导致机器学习参数无法更新，以至于过度稀疏， 即神经元“坏死”的现象。因此，后续的学者们提出了很多关于正值滤波函数的 变种，例如倾斜正值滤波(leaklyReLU)函数、高斯误差线性单元等。
双曲正切函数：Tanl)函数与S型函数具有非常相似的非线性函数功能，其 公式如式2-9所示，形状如图2-7。双曲正切函数将模型的输出限制在-1到1之 间，具有更丰富的非线性变换功能。
池化函数通常包括最大池化(Max pooling)和平均池化(Average pooling) 两种。由图2-8可以很形象地表示出来，其与卷积层操作类似，对每个局部进行 运算，输出其方块内的最大值或平均值来对特征图进行池化。
图2-8最大池化
池化层有两个主要的作用：减小特征图、减弱模型的过拟合影响。池化层配 合卷积神经网络使用具有平移不变性的作用。其池化操作就是图像的去冗余，例 如一张狗的图像缩小了一半依然能够被机器或人类辨别出这是一张狗的照片，这 说明这张缩小后的图像中仍保留着狗的重要特征，因此池化层只是去除了一部分 的冗余信息，而留下的信息则是更关键的尺度不变性特征，能够代表原图像中具
有辨别性的特征。同时，去冗余操作也在一定程度上有利于机器学习的泛化，改 善模型的判别效果。值得说明的是，在神经网络池化层的反向传播中，平均池化 会将某个神经元得到的梯度平均分成n等份向上传递给上一层神经元，保证池化 操作前后的梯度之和不变，并同步更新池化函数连接的所有局部学习参数；最大 池化则是在前向传播时记录下池化操作中最大值的位置，而在反向传播时仅将梯 度传递给该位置上的神经元，其他神经元的更新梯度则置为0。
三维卷积神经网络：以上关于卷积神经网络的介绍中，都是以二维图像中的 卷积作为例子进行介绍的，卷积神经网络在二维图像上的性能表现十分优越，具 体表现在图像分类、目标检测、图像分割等诸多计算机视觉任务中，但在视频等 三维图像序列中表现欠佳，原因在于视频是由多帧二维图像按移动顺序堆叠而成 的，其不仅包含了单帧图像的空间表现信息，还包含了帧与帧之间的时序关系。 为了学习到三维图像序列的时间信息，三维卷积神经网络（thi-ee-dimensional CNN, 3DCNN）被提出，在文献［13］中作者将其应用在人体的行为识别中。该网络结构 的设计是将二维卷积神经网络增加一个时间维度，扩展到空间与时间组成的三维 结构中。在卷积操作时，不仅用到视频中的单帧图像，同时也考虑了时序上连续 帧的联系，可以看作是二维卷积神经网络在时间上的扩展。3DCNN是一个同时 考虑空间和视频序列时序关系的卷积神经网络，其不仅适用于人体的行为识别， 也适用于所有视频分类、手势识别等其他具有三维结构数据输入的机器学习任务。 2DCNN和3DCNN的卷积操作区别如图2-9所示，其中左中右三个子图分别描 述了传统二维卷积神经网络、将二维卷积神经网络结构应用于时空三维数据以及 常规三维卷积神经网络结构的卷积操作。另外需要说明的是，2DCNN和3DCNN 在通道上的概念是一致的，其代表了不同的特征风格。在三维卷积神经网络中， 池化层也针对三维数据进行了适配，形成三维池化函数。
图2-9二维卷积（左）、三维数据中二维卷积（中）、三维卷积（右）
