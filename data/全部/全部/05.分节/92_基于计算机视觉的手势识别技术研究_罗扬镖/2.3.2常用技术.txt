2.3.2常用技术
卷积神经网络在图像等视觉任务上表现优秀，各种卷积神经网络模型也层出 不穷，在卷积神经网络中，还有一些技术能够让网络的训练变得更加准确和高效。 例如神经元丢弃（Dropout）技术〔4°］可以在每批次样本的训练中设置不同的神经
15 元连接，等同于不同模型组合提高了神经网络的泛化性能。批规范化(Batch Normalization,BN)⑷］可以加快网络的训练速度，并具有提高网络泛化性能的能 力，L2正则化可以帮助减小模型的参数值，深度可分离卷积［42］则大大减少了卷 积神经网络的学习参数量，加快模型的推理过程。
(1) Dropout 技术
过拟合(OverFitting)问题是机器学习领域中经常面临的问题，其指的是模 型对训练数据的过度学习，以至于在训练集上的一些非通用性特征以及噪音等也 被机器学习模型给“学会”了，而在更广泛的数据集上并不存在这样的特征，从 而导致模型在训练数据中的识别准确率偏高，在测试集上的拟合效果却不尽如人 意的结果。引起模型过拟合的原因有很多，其中最常见的一种就是模型过于复杂 所致。Dropout神经元丢弃技术由图2-10所示，可见Dropout技术是在网络的训 练过程中，随机挑选一部分的神经元使其停止工作，在前向传播时停止使用该部 分神经元进行传播，并在反向传播迭代时保持模型该部分参数暂不更新。该技术 相当于在每次训练神经网络模型时都属于不同的神经网络结构模型，因此最后得 到的训练模型天然具有机器学习集成的效果，提高了神经网络的泛化能力。
(a)常规神经元连接	(b)应用神经元丢弃
图2-10全连接型神经网络(左)、Dropout技术训练的网络(右)
(2)批规范化
机器学习领域有个很重要的假设就是数据的独立同分布假设，即假设训练数 据和测试数据是具有相同分布的，这是通过训练数据训练模型能够在测试集上获 得良好效果的一个基本保证。而在深层次神经网络的训练中，每批次数据会进行 串行计算和传递，包括隐藏层在内的所有神经元都参与了运算，导致隐藏层数据 的输出分布很不稳定，而随着网络层数的加深，更深层次的隐藏层数据输入受到 了多层浅层神经网络结构的影响会更不稳定，这就是所谓的“内部协变量转变” (Internal Covariate Shift, ICS)现象。这种现象的影响之一是机器模型在训练时 不能设置过大的学习率，导致训练变得困难，收敛速度也越来越慢，并且很容易 产生梯度消失或梯度爆炸的现象。批规范化是对数据归一化处理的办法，是一种 在神经网络内部对中间层输入数据的一种规范化，使隐藏层神经元的数据输入稳 定到均值为0,方差为1的正态分布。保证网络内部数据分布的稳定性，缓解了神经网络梯度消失及爆炸的现象，同时也加快了神经网络模型的训练速度，并天 然具有缓解模型过拟合的作用，批规范化算法的流程如表2-1所示。
表2-1批规范化算法
算法1 批规范化算法
输入：输入小批次特征数据X =
输出：y = {yi>y2,…,y”…,y«i}= B叫,B（X）
1 初始化批规范化参数：Y#
2 计算该批次均值方差:
3 更改该批次特征数据：
将数据分批送到神经网络模型中训练，BN算法首先对输入的每批次特征数 据计算其样本均值及方差，将其规范化为均值为0,方差为1的正态分布输入。 同时，BN算法在训练过程中不断通过反向传播更新两个批规范化可学习参数Y 和”，利用该参数将特征数据恢复成原始比例大小，保证学习模型的可靠性。这 两个参数代表了模型在所有训练数据上的泛化，即所有训练数据样本具有的可信 均值及样本方差。在每批次特征数据上进行的规范化保证了神经网络内部特征数 据的分布，使模型的训练更加稳定、收敛速度更快。批规范化一般添加在神经网 络的激活函数之前，以在激活函数之前规范化网络的特征数据。
（3） L2正则化
机器学习中过拟合是一种比较常见的现象，防止机器模型过拟合有很多种方 法，例如数据扩增、模型简化、模型集成等。还有一种比较简单且有效的方法是 加入模型参数的正则化，L2正则化则是在模型计算损失时将模型中的训练权重 以二次方的形式加入惩罚，其公式如式2-10及2-11所示。
其中，c是模型训练时的整体惩罚值,Co为模型预测误差损失，3是模型的 训练参数，久是正则化惩罚系数。将所有模型学习参数以二次方的形式加入到惩 罚项中，其导函数如式2-12及2-13所示，其中b是学习模型的偏置项。可见, 加入L2正则化项可以使网络在更新过程中不断减小模型的参数值大小，获得较
小的机器学习权重，且不对其他线性化项造成影响。
（4）深度可分离卷积神经网络
卷积神经网络通过共享卷积核的形式极大地减少了所需要的机器学习参数 量，但是在大规模的机器学习项目中，卷积神经网络会被多次使用，随着网络层 数的加深以及卷积维度的增加，传统卷积神经网络依然需要训练大量的参数。图 2-11是标准卷积的卷积核结构，其在空间维度上进行平移卷积运算，而在通道层 面上则使用不同的卷积核进行卷积计算，因此在标准的二维卷积中，每一个卷积 操作所需要学习的参数个数为：输入通道数X卷积核大小X输出通道数。
特征图权重单特征图
图2-12深度可分离的卷积神经网络结构
由图可以看到，在标准的卷积神经网络中，输出通道上的每个通道特征图是 由独立的卷积核卷积得到的，即输出通道上的每个特征图对输入数据进行完整的 计算，使得实际上的卷积网络需要消耗较多的计算资源。为了减少卷积核的数量 可以在通道层面上进一步简化，Andrew等人提出了深度可分离的卷积神经网络结构，即可以把输入数据逐通道进行卷积，再对每一个通道独立进行加权求和作 为每个输出通道上的特征图，如图2-12所示。深度可分离卷积神经网络所需要 的学习参数个数为：输入通道数X卷积核+输入通道数x输出通道数。在输入通道 数较多的情况下深度可分离卷积神经网络可以明显降低机器学习参数量及计算 成本。
