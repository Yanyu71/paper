2.4图结构参数估计方法
最大似然估计是数据挖掘、信号处理中一个非常有用的参数学习工具。它是 一个方法体系，试图解决两件事：首先，当你想要从数据中学习一些模型，它是 一种相当好的原则性方法，帮助你计算你应该做什么。其次，它往往是计算高效 简便的。最重要的是，许多复杂的模型诸如多项式回归，神经网络，混合模型， 隐马尔科夫模型，最大似然估计（MLE）的思想可以帮助你理解I］和分析问题。 对于许多估计问题，最小方差无偏（MVU）是不存在的，即使存在也没有相应的
10
系统程序找到他的最优解，因此，通常使用最大似然估计来代替MVU。MLE不 需要满足任何最优判别准则，其独特的特点在于“转动摇摆的方法”，使得它可 以很快地实现对复杂估计问题的有效求解。粗略地讲，一组数据的似然函数是在 给定概率分布模型下获取特定数据集的概率。这个表达式包含了未知模型参数， 而参数的取值是样本的似然最大。也就是给定n个独立同分布的样本值 打,久2，…,xn,这些样本来自于同一个分布但是不知道概率密度函数0（•），推测函 数%属于某个家族分布｛f（? |。）,0 6 9｝, 6是这个家族的参数矢量，称为参数模型， 因此，f0 =	1%），0。未知且认为是参数矢量真正的值，目标是找到理想的估计
值品尽可能逼近真实的6。值。为了使用最大似然估计，首先需要写Hl所有观察样 本的联合密度函数…。⑼=f（Xi|0） x f（x2|0） X ...X f（xn|e）,可以看出观 …凡依赖于参数仇反过来,8可以看作是关广观察值对,孙，…
数，那么似然函数可以写成：£（0；%1…,0）=©1，%2,…如⑻=IK"（阳|6），； 表示两个输入参数的分割。在实际中通常转化为对数似然函数，即 ln£（6;xi，..,xn） =2kJn/5|0）,最大似然估计是分析最大化的过程，通过找到 品值使得也£（&%1，..,0）最大化的过程，其数学表达式为：｛0m/e） £ ｛argmax In £（0;, %„）｝o如果最大值存在，无论是最大似然函数还是对数似 Ges
然函数其MLE解释是相同的，因为log函数是单调递增函数。
对于许多模型，最大似然估计可以是观测样本巧,一今的显式函数，然而也 有许多模型，没有封闭形式的解，不能求出解析求解；还有对于一些问题，最大 似然存在多个估计，而对于另外一些问题，最大似然估计不存在（即对数似然函 数值一直增加没有极值）。因此，数值优化技术比如梯度下降，牛顿下降口A网 格搜索法陷、评分方法网等被广泛使用。牛顿法、得分法等迭代方法，仅在网格 搜索法失去作用时才使用，但是不能保证MLE收敛性。对于绝大多数使用的最 大似然估计，MLE具有渐进无偏特性，可达到Cramer-Rao下限（CRLB）,并 且具有高斯概率密度函数（PDF）,是一种渐进最优的估计。
优化问题是应用数学和数值分析的一个分支，由最大化或最小化一个实函数 构成，通过在可行解中系统地选择输入值并计算该输入值的函数值。其数学描述 可以表示为，给定函数f:A- R, f是集合A到实数集的映射，对于A中的任意一 个x,寻求A中的元素X。使得f（x（）） < f（x）（最小化问题）或者f（x（）） > f（x）（最大 化问题）。A是欧式空间R"的子集，具体为一些约束条件，即A中的元素满足等 式或不等式约束。f的域A称为搜索空间或选择集，而A的元素称为候选解或可 行解。函数f称为目标函数或损失函数或成本函数。可行解是使目标函数最小（最 大）的解。在数学上，经典的优化问题通常表示为最小化，包括不同类型的目标 函数和不同类型的可行域。许多现实世界的理论问题都可以建模成这个框架，比
11
如，最大似然函数的参数求解。
费马和拉格朗日提出微积分的公式来确定最优解，而牛顿和高斯提出了迭代 方法使得所求解逐步逼近最优解。拉格朗日乘法从理论上，比如KKT条件，来 确定凸问题的解，在实际中用的比较多的还是基于牛顿的迭代梯度下降法。最速 梯度下降法是逐步最小化损失函数的过程，沿梯度下降的方向求解极小值（或沿 梯度上升的方向求解极大值），其迭代公式为：ak+1=ak + pfesW,其中6（的代表 梯度负方向，Pk表示梯度方向上的搜索步长。一般步长的确定是通过搜索方法来 确定，也就是将下一个点的坐标aa］看作是。人的函数，然后求满足f（ak+i）的最小 值，这种方法简单明了，易于实现，但是收敛速度不尽如意，在最小值附近以一 种取着的形式慢慢逼近最小点。牛顿法是在当前点用二次泰勒展开，令其导数为 0,近似最小化目标找出极小点，然后进行线性搜索。这种方法有很好的收敛速 度，但是需要付出计算代价，每次需要计算二阶导而且需要求逆。拟牛顿法（DFP） 是在k次循环时，在上一个循环中的海森矩阵（Hessian）基础上得到在当前点的 Hessiano这种方法省去了重新计算Hessian矩阵，仍然需要存储并更新Hessian, 还有求逆的过程。BFGS方法是通过直接近似Hessian矩阵的逆，省去了求牛顿 方向中需要的矩阵逆运算，从而达到既不需要每步重新计算Hessian,又不需要 求逆的效果。这种方法在每次迭代时仍然需要存储更新矩阵，当矩阵维度很大时 非常占用内存空间。L-BFGS在每一个循环都保存之前步骤的相关信息直接近似 计算出牛顿方向，从而不仅像BFGS一样省去了单个循环计算Hession矩阵并求 逆的运算，而且也省去了存储Hession的必要。这种方法是一种并行化计算，在 有限的内存中存储计算，省去了很大的内存，现实中许多大规模的可微的约束问 题都可以通过此方法来解。由此可见，基于迭代的一系列牛顿方法已经很成熟， 也有很多统计软件包［2。］可以拿来直接使用。
一般情况下，除非目标函数和约束条件都为凸的情况下，其他情况都有可能 存在局部最优解。在最小化问题中，局部最小解无*定义为，存在3 > 0, ||%-%*||, 所有的x满足f（x*） <f（x）,也就是说，在％*的周围区域，所有的函数值都大于或 等于改点的函数值。虽然局部最小至少和附近的一样好，但是全局最小值至少和 每一个可行点一样好。在凸优化问题中，如果存在局部最小点，也是在可行域的 内部，而不是可行的点集边缘，那么这也是全局最小点，但是在非凸问题中可能 存在多个局部最小点，都不是所需的全局最小。在许多问题中，也会经常碰到非 凸的问题，目前关于非凸问题的求解主要还是采用迫近法mi或将问题转化松弛 凸问题［22］、局部凸化［23］的手段。实际中也提出了大量的算法求解非凸问题，包括 商业可用的求解器，大多数都不能区别是局部最优解还是全局最优解，通常是将 前者作为原问题的解。
12
这里主要介绍了求解参数估计以及优化当中的一些基本的方法，这些方法也 是最重要的方法，很多复杂的问题都可以通过这种思路得到求解。在高维信号的 统计参数估计中比如本文的大规模数据的图建模，仍然可以延伸最大似然估计、 梯度求解的思想，但是要复杂的多，未知参数的个数要远远大于样本的个数，稀 疏假设的合理性等，下面将介绍本文需要采用的图结构参数估计方法。
