2.4.3稀疏惩罚方法
高维数据广泛存在于人们的日常,，远远不是某种特殊现象，比如信息科技， 生物，航天科技产生的数据维度都很高。这里“高维”所指的是所需要估计的未 知参数个数远远大于数据的样本个数，甚至是一个或者几个数量级。经典的统计 推断并不能直接用于高维问题。比如，线性最小二乘拟合中，未知参数的个数远 大于观测个数，对应的标准误差和测量的意义都有可能是病态的。不难看出，没 有额外的假设或者限制应用到某一特定的模型，高维统计是不可能的。平滑函数 的估计，使得基于结构平滑的假设可以用来拟合许多参数的情况。随着方法论、 计算数学的进步，另一种基于稀疏假设的高维数据统计推断得到了肯定。从平滑 到稀疏约束，或者两者结合，打开了复杂数据应用领域的新路径。比如，稀疏假 设允许一个病人的健康状态取决于几千个生物标签中少数几个，要比在平滑假设 中所有的生物标签都对病人的健康状态有贡献更有实际和显示意义。此外，稀疏 假设［27］不仅可以简化模型还可以保留数据中比较重要的信息，使得数据有更好 的解释效果。在上面的高斯图结构估计和因果图结构学习中都对此进行了相应的 稀疏假设，因为实际中图的节点不可能跟其他所有的节点都有关联。稀疏假设的 方法有很多，这里将做一些简单的分类介绍，以便在后面的模型中更好地嵌入。
在正则化稀疏模型中具有开创意义的方法是Lass。的提出，Tibshirani使用，1 范数替换岭回归中的G范数，得到了 Lasso估计阳。%惩罚在估计和预测准确率 方面有明显的优势，并在平滑的标准方法中，比如，Newton-Raphson有广泛的应 用。21惩罚在零点处不可微，这个特性在高维模型选择中也非常有用，因为通过 LASSO估计得到的零解可以丢弃一些不重要的特征。在不相干设计中，不可表 示的条件、压缩感知RIP【29】规则等有较好的性能。Lasso对应的求解算法LAR (Least Angle Regression)提出后，稀疏正则模型才引起了图像处理和机器学习 等相关领域研究人员广泛关注与深入研究。
假设线性回归模型：y = Ha + s,其中，y CRN是因变量，H e R^p是观测 矩阵，a CRP是回归系数，ee RN是高斯独立分布的误差向量。
(1)基于G岭回归的正则模型：
arg嗽-	+ 期(2-20)
G范数有平滑回归系数a的作用，对回归系数有一定的压缩，在一定程度上
16
避免了过拟合，但是不能产生稀疏解，使得预测变量的待估系数接近于0,从而 给模型的解释性做成了困难。
(2)基于匕的Lasso正则模型：
argmin|||y - Ha\^+A\\a\\i	(2-21)
入2 0是调整参数，|[a||] =?Mi|ap|,也就是说Lasso惩罚是对回归系数的 匕范数进行压缩惩罚，使绝对值较小的回归系数变为0,从而产生稀疏的解。与 岭回归不同的是，Lasso无法得到理论解析解,它是通过数值优化里迭代下降法， 比如坐标卜降法，不断迭代收敛得到。其回归系数即在每次迭代中的更新过程为：
&P = sign(%)(M| -入)+
( afp - A f afp > A
二1	0,	< afp < A	(2-22)
(	&fp + 尢	p V -A
今p为最小一二乘估计的解，可以看出，落在[-入，入]区间内的系数都被降为0, 而这部分系数所对应的变量不参与模型的拟合，从而实现了对量的选择。
(3)基于Lasso的SCAD稀疏惩罚模型：
argmin|||y - Ha\^ + Ep=i<PA,y(«P)	(2-23)
aERp 乙	'
p e {L …,P}, W儿y(?)是 SCAD 惩罚
'A|e|,0< |0| <A
(|6|2-2yA|e|+A2)
W尢y(6)= <	2(y-i)	, A < |6| < yA	(2-24)
空严，|6| >yA
I 2
y和4一般默认取值为丫 >24 2 0, SCAD模型对回归系的绝对值数落在 [0,为区间部分倾向于压缩为0,这和Lass。的作用相同。对于回归系数的绝对值 落在[人,丫刃部分，随着回归系数绝对值的增大惩罚程度减小，而对于回归系数的 绝对值落在区间小尢+8],不再进行惩罚。
Wang和Li的文章的中建议参数y = 3.7比较合适，另夕卜，当观测阵列正交 时，SCAD在每次迭代中的更新过程为：
(	sign("p)(|"p _/l|)+ ,	\a'p\ < A
即=，	((y - l)ap - sign(a,p)yA)/(y - 2),	2 < \a'p\ < yA
(	即,照|<必
(2-25)
SCAD方法的优势在于回归系数对于数据是是有连续性的，因而比较稳定。
17
(4)基于Lasso的MCP稀疏惩罚模型：
argmin|||y-Hcr|P 4-Sj=i<PA,y(ap) aeRp /
%,yC)是MCP惩罚：
(2-26)
W儿y（e）=
4|6|一喏,|0|<yA |6| > yA
(2-27)
y和2一般默认取值为y > 1,2 > 0,可以看出，MCP模型对回归系的绝对值 数落在［0/刈区间部分倾向于压缩，当y— co时，MCP惩罚逐步趋向于基于范 数的Lasso惩罚，回归系数的稀疏性也变得越来越小，而对于回归系数的绝对值 落在区间卜尢+②］,不再进行惩罚。当了一1时，MCP惩罚逐渐趋于（°范数，回 归系数的稀疏性也变得越来越大。
SCAD和MCP稀疏惩罚模型都是一类近似无偏稀疏模型。在模型选择中， 我们可以把自变量分为两种，一种是目标变量，其与因变量密切相关，在求解过 程中是期望得到的解，另一种是噪声变量，其与因变量无关，是目标变量的干扰， 在求解过程中是希望抑制的解。Lasso模型在进行稀疏惩罚时对所有的回归系数 都进行了相同程度的惩罚，因此对期望变量对应的回归系数也会进行相应的压缩, 导致了对目标变量回归系数的有偏估计。而SCAD和MCP模型有选择性地对回 归系数的变量进行压缩，在一定程度上克服了 Lasso有偏估计的缺点，是一种近 似无偏稀疏模型。其他的近似无偏稀疏模型还有自适应Lasso》1,松弛Lassos1,， 桥回归网模型等。自适应Lass。，松弛Lass。,，桥回归惩罚函数是凸的，SCAD和 MCP惩罚函数为非凸的。他们都各自有各自的优缺点的，非凸惩罚在变量选择 的一致性有较好的效果，但是也可能会导致全局最优解不存在。
（5）基于Lass。的分组稀疏模型：
分组稀疏惩罚是指将某些变量作为一个整体同时选中或同时不选中进行模 型的构造，也就是说，将回归系数绝对值几乎相等的那部分，所对应的变量彼此 之间相关度非常高，将他们作为一个组同时选中或同时移除。分组稀疏模型可以 分为三类：（1）通过对岭回归施加惩罚，（2）通过惩罚系数之差（和），（3）通 过两两无穷范数惩罚。通过岭回归施加的自动分组惩罚包括迹Lasso3］、弹性网 3】、弹性SCAD1361等，弹性网这种方法对全部变量都施加同等程度的岭惩罚，自 动分组比较不适应，而迹Lasso和弹性SCAD会根据变量间的相关系数的自适应 的进行分组；通过惩罚系数之差（和）的自动分组惩罚包括融合Lasso,加权融合 Lasso、弹性相关网等。融合Lasso是所有其他回归系数惩罚分组的基础，其他的 融合技术均受融合Lasso启发。加权融合Lasso和弹性相关网可以通过对变量之 间的正负相关系数自动分组。通过两两无穷范数的分组惩罚，使用无穷范数来约
18
束变量间回归系数的最大值，可实现正相关与负相关的自动分组。
(6)基于2。的非凸稀疏惩罚
两个最基本的惩罚% (岭惩罚)和乙惩罚(LASSO),都是凸惩罚并且都是计 算方便的，但是％实际上并不产生稀疏解，L受限于变量选择的不一致性，使得 在处理回归中的共线性问题中准确率大大降低。比如说，在高分辨率光谱估计中 需要足够多的过完备字典来表示信号特征，而许多正弦原子的分辨率是高度相关 的，类似这种共线性问题的回归使得匕惩罚甚至比％惩罚的性能要差。而11范数是 %范数的严格凸松弛，为了保持准确率和稀疏性，必须考虑类如“2。+ %”的非 凸稀疏惩罚。然而没有哪个技术直接应用离散惩罚，比如％和/%，或分组惩 罚，大多数都是将非凸惩罚转换成凸松弛的形式:
argmin^ ||y -Ha\f.+y la^0	(2-28)
aeRp /
为了解决共线性和噪声污染的问题,相对应的“2。+ %”的惩罚形式为: arg吧 n：||y — ”团1： +薪(2-29)
仇是追求稀疏的理想表示形式，然而1的参数调整却不能忽略，大多数参数 调整策略，比如K折交叉验证，是非常耗时的。YiyuanSheR]提出了基于2。的稀 疏约束的阈值筛选技巧|[a||o < m,与惩罚参数/I相比，m更具有意义和可定制性， 可以通过先验的稀疏假设很方便地设置，而，2范数惩罚参数是不敏感参数的，不 需要进行大量的网格搜索，许多研究者网已经给出了最佳的默认值(比如，le-3),可以有效地减少预测误差。同时，他还证明了其迭代的收敛性与一致性。
