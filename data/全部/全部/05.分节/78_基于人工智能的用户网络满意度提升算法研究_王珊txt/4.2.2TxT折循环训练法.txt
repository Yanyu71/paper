4.2.2TxT折循环训练法
上一节中提到Stacking算法中存在交叉学习问题，本节提出7x7折循环训练法，避 免交叉学习问题出现。假设选取T个基分类器，则将初始样本划分为互不交叉的T份， 得至(…，Dt},对第一个基分类器，每次选取作为训练集，用来预测D, 循环直至Dx到Dr均被预测，以此作为训练集在第一个基分类器上的第一组预测结果。 为确保每个基分类器对原始数据的充分学习，重新将原始数据划分为互不交叉的T份， 再次进行T折循环训练，重复T次，完成训练集在第一个基分类器上的“T折训练，得 到 T组预测结果，共 2T列。用 probkt(i)={ probki(io),probk^ii)}(^, t=1, 2,…，7)表示预测 结果，其中pro此4io)表示第k个基分类器在第t次T折循环训练中对第z个样本预测为 0的概率，p/•湖以时表示第七个基分类器在第t次7折循环训练中对第i个样本预测为1 的概率。当选定的T个基分类器都完成上述操作后，得到7个基分类器生成的2/列训 练结果。每个基分类器完成一次T折循环训练后，在测试集的一个样本X,上，共生成T 个预测结果，取平均后作为基分类器在测试样本整上的一个预测结果，每个基分类器在 训练集完成八7折训练后，则在测试集共产生T个预测结果，共2T列。
inodei_T	probT
初级学习器
次级学习器
图4-3 7x7折循环训练法训练阶段
为保留原始特征和预测类概率之间的隐含关系，将原始特征加入到次级学习器的输 入中，次级学习器的输入可表示为：
38
D'={(xi, Ci,Mi,probk^f) j沛=1,2,…k, /=1,2,—,77	(4-2)
次级学习器的输入特征维度为冈+2/+N+2。
考虑算法的复杂度，加为样本个数，T为基分类器个数，基分类器的算法复杂度估 计为。0)，则Stacking算法的复杂度估计为TxO(m)+O@), NStacking算法的复杂度估 计为Tx<9(M+O(d2), TStacking算法的复杂度估计为涔0(加〃)+0(出)，其中，0(0为次 级学习器的算法复杂度。NStacking算法与Stacking算法相比，次级学习器训练样本维 度增加网维，TStacking算法初级学习器训练样本个数减少为总样本的1/T,但训练次数 增加为Stacking算法的7倍，次级学习器训练样本维度增加因+NH维。TStacking算法 的基分类器个数增加时，会大大增加训练次数，因此基分类器个数T的选择不宜过大。 次级学习器维度的增加会增大训练时长，因此TStacking算法在高维数据集上耗时较长。
