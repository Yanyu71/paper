2.4.3分类算法性能评估
分类模型建立后需要对模型的准确率进行评估，如果建立了多个分类器想选择其中 “最好的”，需要进行模型选择。由于算法使用训练数据进行学习得到分类器，若仍然 使用训练数据对模型结果进行评估可能会错误地导致过于乐观的估计，因此，一般会将 原始数据拆分为训练集和检验集，检验集由未参与模型训练且含有类标记的样本组成, 通常在训练集上学习得到分类器模型，在检验集对模型进行评估。拆分的方法有保持、 随机抽样、交叉验证和自助法等。
表2-3所示的混淆矩阵（confusion matrix）汇总了用于表示分类结果的样本个数。 给定加个类（其中加三2）,混淆矩阵是一个至少为mxm的表，它显示了正样本和负样 本的合计，可对分类器识别不同类别样本的能力进行分析。
表2-3 2x2混淆矩阵
实际情况	预测结果
正例	反例	合计
正例	TP	FN	P
反例	FP	TN	N
合计	P'	N'	P+N
从混淆矩阵中容易看出TP （True Positive,真正例）、TN （True Negative,真负例）、 FP （FalsePositive,假正例）、FN （FalseNegative,假负例）各指标的含义，从而可以 得到被正确或错误预测的样本个数，但是混淆矩阵表示的预测结果还不够直观。接下来 介绍一些常用于分类算法性能评估的指标。
准确率（accuracy）是评估分类模型好坏的一种常用度量，它表示被正确分类的样 本占所有样本的比例。准确率的定义为：
TP + TN accuracy =------
P+N
(2-9)
当类分布相对平衡时，准确率可很好的评估分类模型性能，当类分布不平衡时，如
15
在欺诈检测或医学检验中，正样本都是稀少类，如欺诈检测中类标号属性为“力md”， 可能取值为“yes”或“力。”，98%的准确率看上去效果良好，但若只有2%的训练样本 是欺诈样本，该分类器可能只是正确地标记了非欺诈样本，而错误地标记了所有欺诈样 本。因此，不平衡数据进行分类时，还需要其他的评估度量。灵敏性(sensitivity)和特 效性(specificity)常用来评估不平衡数据下的分类模型。灵敏性又可称为真正率(TPR), 表示实际所有正样本被正确预测的比例；特效性又称真负率(TNR),表示实际所有负 样本被正确预测的比例，而假正率(FPR)表示实际所有负样本被错误地预测为正样本 的比例。以上度量定义为：
TP TPR=sensitivity=下-	(2-10)
TN TNR ^specificity=	(2-11)
FP FPR=L specificity= ——	(2-12)
N
召回率(recall)和精度(precision)也常被用作分类算法的评估度量，召回率越 高，表示越多的正样本被预测正确，实际上，召回率与真正率的值相等；精度表示被
分类器预测为正样本的记录中实际为正类的比例。以上度量定义为:
recall=
TP
TP + FN
precisions
TP
TP + FP
(2-13)
(2-14)
分类算法还有很多其他评估度量。ROC曲线是其中一种可视化方法，ROC曲线是 一条连续的曲线，如图2-6所示，其横纵坐标分别为假正率和真正率，曲线上每一点代 表了不同门限下的分类模型，改变门限值便可看到真正率和假正率之间的权衡。ROC曲 线可用于模型选择，对比不同分类模型的ROC曲线，越接近(0,1)点则表示分类模型
的性能越好。ROC曲线只能定性地对不同分类模型进行比较，若想定量表示分类模型的 性能，可采用ROC曲线下的面积(AUC),理想情况下AUC的值为1,随机猜测时,
AUC的值为0.5 o
图2-6ROC曲线示意图
16
