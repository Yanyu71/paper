4.2.1独立自注意机制
在介绍独立自注意机制之前，本文先对一般卷积运算的特点进行简要介绍， 并由此引出独立自注意机制。
局部邻域
图4-1卷积运茸图
图4-1表示卷积运算的示意图。最左侧方形网格代表输入特征图，中间方形 网格代表一个卷积核(又名滤波器)，与输入特征图的局部邻域大小保持一致。 输入特征图的局部领域与卷积核通过逐像素点乘再求和得到输出特征图上的一 点输出，如图中最右侧方格所示。推广到一般情况，给定输入XGRAXWXC；,其中力 代表输入特征图高度，w代表宽度，ci代表输入通道数。特征图的任意一个局部 邻域用Ng表示，其中V代表局部邻域的中心点列的坐标，k代表局部邻域 的大小。一个卷积核所代表的可学习权重矩阵为FFeR™,若有多个卷积核同 时对局部邻域做卷积则有,其中co代表卷积核数量，也即输出通 道数。则此局部邻域的输出为：
(4-1) 其中，加，"代表局部邻域中像素点乙”的坐标，聲”为该像素点对应的卷 积核中的权重值，将所有点积求和后得到一个输出像素点之后再根据
36
卷积运算的权重共享和平移等效的性质，逐邻域地将输入特征图通过卷积运算映 射为输出特征图。
受谷歌大脑团队的研究启发，本文将一种独立自注意机制应用于语音增强， 可以用来代替卷积算子。与图4-1中的卷积操作类似，这种独立自注意机制的本 质是对输入特征图的任意给定的局部邻域做自注意力运算，得到该局部邻域的一 点输出，然后对输入特征图的所有像素点依次进行上述运算，得到最终的输出特 征图。为了让独立自注意机制具备位置感知能力，解决排列不变性的问题，在此 引入了第三章中介绍的相对位置编码方案。此外，为了弥补与卷积运算之间的差 距，让善于捕捉高层次全局信息的注意力机制能最大限度兼顾低层次边缘局部细 节，本文在值矩阵中注入了基于绝对位置信息的距离感知特征。这里需要说明的 是，上文中提到的位置感知和距离感知能力是基于不同的方法实现的，位置感知 能力是通过将查询向量和二维相对位置编码做矩阵乘法实现，解决的是注意力机 制排列不变性的问题；距离感知能力是通过结合绝对位置信息的多值矩阵来实现, 目的是利于全注意力机制更好地捕捉特征图的低层次特征和边缘细节。后续的仿 真实验表明，基于独立自注意机制的语音增强模型具备更少的网络参数和更优的 去噪性能，下面将进行详细介绍。
图4-2单头局部独立自注意机制
图4-2给出了单头局部独立自注意机制的实现示意图。最左边代表输入特征 图，与卷积类似，这里提取一个以为中心的局部邻域，后续的计算流程 都是在这个局部邻域上展开的。通过注意力核，一种可学习的线性变换(learned transform),得到亏的查询向量和局部邻域上各像素点的键矩阵和值矩阵，如图 中虚线所示，再通过将权重矩阵(查询向量和键矩阵做矩阵乘法并通过softmax 归一化)和值矩阵进行逐像素做点积后求和，得到列的输岀儿，即输出特征图的 一个像素，如图中实线所示。计算单头局部独立自注意机制的公式如下：
q产 “Q%	(4-2)
37
kmn ^kXmn	(4-3)
% = Wvxmn	(4-4)
儿=(4-5) 其中％ xmn丘口表式局部邻域m,neNK (，J)的中心像素和邻域像素，k代表方形 的局部邻域的大小，局部独立自注意机制要做的就是计算出此中心像素与局部邻 域内所有邻域像素之间的依赖关系。WQ,WK,WVGW°^是可学习的线性变换，分 别叫做查询注意力核，键注意力核以及值注意力核，用于将局部邻域中的像素映 射为查询向量9严踏，键向量km„eW°以及值向量％ 衆化$劝max””(・)表示 对一个局部邻域刃，"w N许，力内所有的像素点进行理A max归一化后，像素点 入”相对于中心像素列的权重大小。最后通过与值向量vmneRC0做矩阵乘法后求 和，得到局部邻域的输出对输入特征图的其他局部邻域依次做相同运 算，得到最后的输出特征图。
将公式4-5与公式4-1比较可以发现独立自注意机制和卷积运算的相似性。 卷积运算通过将感受域内的所有像素点与卷积核对应权重做点积再求和后得到 输出，而注意力机制通过计算中心像素与邻域像素之间的依赖性(权重)大小， 并与局部邻域内所有像素映射得到的值向量做点积再求和后得到最终输出。在第 三章中曾提到还可以将注意力机制扩展到多头，每个头部对应一个特征子空间， 学习输入数据的不同特征表示。基于N个头的独立自注意机制只需将中心像素 列分为N组，得到x； eRc//,v ,将三个可学习线性变换分为N组，得到
,然后分别做上述单头注意力运算，最后将每个注意力头的输 Q K V
出j, e欣皿拼接起来得到多头注意力的输出儿e踏o
下面本文将在独立自注意机制中嵌入相对位置信息。在第三章中的二维相对 自注意机制小节中，本文使用了相对位置编码方案，通过在权重矩阵中添加相对 高度和相对宽度信息来实现二维的相对位置编码，解决了自注意力机制中排列等 变性的问题。尽管在随后的仿真实验中发现具有位置信息的注意力机制并未取得 最优的语音增强性能，本文分析这可能与输入对数功率谱图过大，且相对于语音 时域波形而言己经不再是序列化信号，注意力对其进行全局位置的建模反而会产 生很多无用的冗余信息进而影响模型性能，但在局部独立自注意机制中或许是有 效的，因此本文在本节中仍将其作为一个可选项，通过后续的消融实验来验证本 文的想法。相对位置编码方案在第三章中已经给出了详细的阐述，这里只给出局 部相对位置编码的公式，不再具体展开讨论。局部相对位置编码的具体公式如下：
y.j =工”,,”出(,；/)$" max”,” (力―+力汕 +尬—)％	(4-6)
38 公式中字符的具体含义与实现方式在第三章已经给出介绍。相比于公式4-5,公 式4-6在计算陷相对于中心像素点勺的权重时，同时嵌入了相对高度和相对宽 度信息，所以两个像素点之间的权重是由像素内容和像素之间的相对位置共同决 定的，通过注入相对位置信息，注意力机制也具备了类似于卷积的平移等效性， 进一步模拟了卷积的能力。
为了弥补与卷积运算之间的差距，在尽可能少地增加参数的前提条件下，让 善于捕捉高层次全局信息的注意力机制能最大限度兼顾低层次边缘细节，本文在 独立自注意机制中加入了基于绝对位置信息的距离感知的能力。对于卷积运算， 卷积核中的可学习权重参数是基于像素距离的，这种特性对卷积在学习局部边缘 特征方面起着关键作用。在网络的初始几层，输入的信号对网络而言具有更多低 层次的特征，进入网络的编码端后被逐层提取抽象的高层次特征，在这个过程中， 初始几层的卷积运算将具备低层次边缘学习的能力，而这对于基于内容交互的自 注意机制而言是较为困难的。
为了解决上述问题，本文修改了值向量的获得方式，在值向量中注入了像素 的绝对位置信息，用多个值注意力核的加性组合来替代传统的值注意力核，每个 值注意力核都有各自的权重，此权重是局部邻域内的像素的绝对位置的函数，且 所有权重的加和为1。值向量由如下公式得到：
%=(工屛(九"，2席片”	(4-7)
与公式4-4相比，值注意力核豹变为了多值注意力核晞的加性组合，其中每个 值注意力核呼都是一种可学习的线性变换，各自的权重系数P(m,n,A)是像素的 绝对位置(加异)和多值数;I的函数，多值数是一个超参数，和注意力头数类似， 需要人为设定。权重系数P(弘"，刃由如下公式得到：
p(m, n, A) = soft maxx ((rm + rn )r rj	(4-8)
其中，“乙€时分别表示像素％的可学习的行位置嵌入向量和列位置嵌入向量, 也是一种可学习的嵌入向量，用于辅助和融合行列位置嵌入。50/?max/.) 表示将归一化函数$Mmax(・)应用于像素忌”的所有位置嵌入表示后，第A个位 置嵌入的权重。
此外，卷积运算的参数量随着感受域的增大而迅速增加，而独立自注意机制 的参数量主要与输入特征图和输出特征图的通道数cj和CO有关。Ramachandran 等人［39〕指出，当ci = co = 128时，k = 3的卷积层和k = \9的独立全注意层拥有 相同的参数量和计算代价。然而本文在模型训练中发现对于相同的输入，尽管独 立自注意机制有着更少的训练参数，但计算代价却并不小，而产生这种现象的原 因是在计算机的硬件加速器上缺少了用于优化计算的内核。
39
