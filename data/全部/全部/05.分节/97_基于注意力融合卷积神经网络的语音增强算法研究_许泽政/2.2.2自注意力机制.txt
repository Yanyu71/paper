2.2.2自注意力机制
自注意力机制由Google机器翻译团队在2017年提出【37],作者指出基于RNN 的编解码模型不能并行计算，训练时间太长，因此提出了一种新型网络模型一一 Transformer,该网络模型同样是编解码架构，只不过用一种自注意力机制替代了 其中的RNNo自注意力机制与上述传统的注意力机制不同，传统的注意力机制 是在编码端和解码端之间做注意力，查询向量从解码端获得，而自注意力机则发 生在编码端输入序列内部或者解码端的输出序列中，具体流程如下图所示：
11
图2-4自注意力机制图
图2-4给出了输入序列尤 对其他输入序列做内部自注意力的示意图。从图 中可以看到，与传统注意力机制不同，给定输入信息^珂兀耳…兀］,通过 可学习映射得到各个输入序列的查询向量Q ,键向量K和值向量7 ,先是输 入序列尤对其他输入序列做内部自注意力，将Q与各个输入序列的键向量做 点积，得到禺对其他序列的“关注程度”，后续的过程与传统注意力一致，最后 的输出是尤的上下文序列X ,接着其余的输入序列依次执行上述过程。
