3.2.1全卷积神经网络
全卷积神经网络是从卷积神经网络发展而来，最早由Park等人应用于语音 增强领域。全卷积神经网络在卷积网络的基础上去除了最大池化层、平均池化层 等池化操作以及相应的上采样层，上述池化运算会不经学习地直接削减（取局部 最大值或者平均值）卷积特征图的大小，是一种信息有损的操作。因此，全卷积 神经网络能保留更多的特征信息，对语音信号而言可以更好地恢复未被噪声污染 的干净语音。此外。全卷积神经网络同时摒弃了卷积神经网络在最后一层使用的 全连接层，改用单通道的卷积层代替，一方面可以不破坏特征图中像素点之间的 相对位置，有利于在回归问题中发挥更好的预测能力。体现在语音增强问题上， 全连接层会使得原时频结构中有二维关系的时频单元分散在一维长向量的不同 位置，相对位置的改变会增大模型对语音信号进行学习建模的难度。另一方面, 通过卷积操作可以方便地还原最开始输入模型中的特征图的尺寸大小，保留了原 始输入数据中的结构信息。
图3-1基于UNet架构的全卷积神经网络模型
图3-1是一种基于UNet架构的全卷积神经网络模型。UNetS］是一种特殊的 全卷积神经网络，最早应用于医学图像分割领域，因其整体的编解码网络设计类 似于英文字母“U”，因此得名UNeto Grzywalski等人将UNet网络与长短时记忆 网络LSTM结合，提出一种新的卷积循环网络用于语音增强a】。在本文提出的 网络模型中，本文将这种基于UNet架构的全卷积神经网络作为骨干网络，在此 之上研究如何提高卷积层捕获全局上下文语境的能力。在图3-1中采用的UNet 架构可以分为编码端和解码端两个部分，一共包含14个卷积层，在图3-1中用 “Conv”表示。其中编码端包含7个卷积层，解码端有与之对称的7个反卷积 层。在编码端，卷积操作的通道数以2的幕次方数逐层增加，每一个通道代表一 种特征表示，这样可以从多个维度学习输入语音对数功率谱的时频特征，同时， 卷积操作会逐层压缩特征图尺寸，提取输入语音的高级特征表示，并抑制带噪语
14
音中的噪声干扰；在解码端，对学习到的语音高级特征表示进行相反的反卷积操 作，逐层减少通道数，重构特征图尺寸，最终恢复增强后的语音对数功率谱。
除了卷积运算外，本文在除输入和输出层之外的所有层进行批量标准化
(BatchNonnalization)运算【倒，在图3-1中用"BN"表示，在网络的最后一层 使用线性激活函数(即不增加任何非线性激活函数)，在其它层使用常规的ReLU 激活函数，在图3-1中用“ReLU”表示。其中，批量标准化算法由Google团队 于2015年提出，它通常在卷积层和激活函数层之间使用，基本原理是将卷积层 的输出归一化为标准的正态分布，使得网络的输入输出值更加稳健，避免了训练 过程中梯度消失或者爆炸的问题，有利于网络的迅速收敛，加速整个网络的训练 过程。ReLU激活函数是为了给网络模型引入非线性因素，理论上可以使神经网 络表示为任意的非线性函数，这也是神经网络可以学习更为复杂的函数映射的基 础，ReLU激活函数的公式见式(3-1),图像表示见图3-2。图3-1中UNet的通 道数在编码端逐渐增加或者保持不变，在解码端进行对称的相反操作，为了在输 入特征图的频率方向上更多地学习上下文信息，在频率方向进行步长为2的卷积 操作，在时间方向上取步长为1同时进行置零填充，在编码端逐层将特征图的频 率维度减半，在编码端的最后一层减至一维，获得输入带噪语音的高频特征表示， 在解码端中再逐层地恢复原始的输入尺寸，整个过程不改变特征图的时间长度。
(3-1)
图3-2 RELU激活函数图像
此外，UNet编码端在逐层提取输入语音的高级特征表示的同时会不可避免 地损失掉一些细节信息，而这些细节信息很难在解码端通过反向卷积恢复，会对 最后的输出增强语音造成损害，而且随着网络层数的加深，模型会变得难以训练, 出现梯度弥散的问题，机器学习领域通常采用跳跃连接(skipconnection)机制汎1 来解决上述问题。跳跃连接一般分为两种，一种是特征图融合相加的方式，将编 码端的提取的语音信号对数功率谱特征和对应解码端的谱特征进行逐时频点相
15 加，此种方式要求编解码端的对数功率谱特征的尺寸和通道数完全一致，另一种 是特征图拼接方式，将编码端的对数功率谱特征和对应解码端的谱特征沿着通道 方向进行拼接，此种方式允许编解码端的对数功率谱特征的通道数可以不同。在 图3-1中，本文采用第二种跳跃连接机制，将编码端的特征图传递到对应解码端， 在通道方向上进行拼接。跳跃连接既有利于模型训练，又有助于恢复目标语音的 细节信息。在下一节的实验仿真中，本文也对两种跳跃连接方式进行了比较，验 证了特征图拼接方式的跳跃连接机制在语音增强领域更具有效性。
