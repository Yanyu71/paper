4.1引言
卷积神经网络吸引了许多研究人员的关注，已经在语音信号处理领域中取得 了许多不错的成绩，在诸如语音识别，语音分离与增强等技术上都涌现出了许多 基于深度学习卷积神经网络的信号处理算法。然而，由于卷积运算具有权重共享 和局部感知的固有特性，在一个较大的感受域内放缩特性较差，导致其难以学习 到特征图内长距离的上下文依赖关系。为了解决卷积运算存在的问题，研究人员 最开始侧重于重新设计卷积算子，寄希望于从卷积网络本身寻求突破，例如膨胀 卷积(又名扩张卷积，空洞卷积)。在计算机视觉领域，已经有研究人员开始将 具备全局相关性建模能力的注意力机制应用于传统的卷积神经网络中，比如通道 挤压激发(Squeeze-and-Excite)机制，空间感知非局部模块(Non-local Block) 等，这些方法都是将注意力机制用作一种校正手段与卷积网络结合，通过重新校 准原始卷积特征图来帮助网络提升捕获长距离交互作用的能力。在语音增强技术 领域，将注意力机制与卷积网络相结合的做法尚数少数。Hao等学者在2019年 提出了一种结合非局部模块的卷积神经网络，将计算机视觉任务中兴起的这种注 意力机制应用到语音增强领域中来。Lan等学者在2020年提出了一种结合通道 挤压激发机制的卷积神经网络用于语音增强，可以帮助模型学习重要特征，抑制 无用特征。上述方法在其各自的论文中表示，实验结果显示将注意力机制与卷积 网络相结合的做法不仅提高了神经网络的计算效率，而且在客观语音清晰度和语 音质量指标方面都优于其他对比方法，这给本文的研究指明了方向。
不同于上述论文中的做法，本文在第三章提出了一种用于单通道语音增强的 注意力强化全卷积神经网络AAUNet,将一种新的二维相对自注意机制嵌入到基 于UNet架构的全卷积神经网络中，将卷积运算与自注意力运算两者产生的输出 在通道维度上拼接起来，这一特性可以灵活地调整注意力通道在整个特征图通道 中所占的比例，理论上可以在卷积关注局部细节和自注意力获取全局语境之间找 到最优的组合，强化卷积运算获取全局语境的能力。然而，本文在实验中发现， 将这种注意力机制的通道数占比设为100%时，也就是全注意力模式，语音增强 的效果呈现下降趋势，因此将这种注意力机制与卷积结合使用是一种理想的选择。 在本章中，本文将尝试在语音增强领域用完全注意力机制来替代卷积运算，力争 用最少的参数实现最优的性能。
