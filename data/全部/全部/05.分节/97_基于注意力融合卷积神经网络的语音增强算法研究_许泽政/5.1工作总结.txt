5.1工作总结
语音增强是一种语音信号处理技术，是语音分离领域的一个主要分支，是说 话人分离的一种特殊情况。语音增强技术的目的是从一段被噪声源或者混响环境 破坏了的带噪语音中恢复出干净的说话人语音，即增强语音。在这个过程中做到 背景噪声完全去除同时增强语音不会失真、保证增强语音的恢复质量和可理解性 是本领域学者一直不懈追求的目标。基于数字信号处理技术的传统语音增强算法 经过许多年的改进和发展，在既定的假设域内取得了良好的效果，已经在手机终 端、降噪蓝牙耳机等商品上实现了落地实用。随着计算机算力的增强，基于大数 据和深度学习的语音增强算法近年来异军突起，取得了十分亮眼的成绩，无需任 何前提假设就能在极低信噪比和复杂背景环境下恢复出干净语音。其中，基于卷 积神经网络的语音增强模型，相比于传统算法，能更好地保证增强语音的质量和 可理解度，相比于其他神经网络，其独特的权值共享特性能在保证去噪效果的前 提下大幅度减少网络的参数量，符合神经网络轻量化发展的趋势，得到研究人员 的广泛关注。
然而，卷积运算受限于感受域的局部性，一次只处理输入特征图的一个局部 邻域，这种特性决定了卷积算子很难捕捉到特征图上长距离的上下文信息，难以 具备感知全局语境的能力，这将不利于更好地恢复目标语音。此外，在卷积网络 中每一个通道代表一种特征类型，随着通道数的增加，网络中会产生许多冗余的 信息，逐层传递之后无益于模型的训练和学习。基于以上分析，本文研究的重点 是通过自注意力机制来强化全卷积网络获取长距离上下文信息的能力，帮助卷积 网络强调有用的特征，抑制冗余的特征。本文所作的主要工作总结如下：
(1) 本文提出了一种用于单通道语音增强的注意力强化全卷积神经网络 AAUNeto本文设计的基于UNet架构的全卷积神经网络一共包含14个卷积层， 编码端和解码端各包含7个卷积层，特征图的频率维度在编码端逐层减半，通道 数以2的幕次增加，在解码端进行相反的运算，并通过跳跃连接机制将编码端的 特征图传递到对应解码端，在通道方向上进行拼接，有助于恢复目标语音的细节 信息。本文将一种新的二维相对自注意机制应用到全卷积神经网络中，具体的做 法是在编码端的最后三层和解码端的前两层进行注意力强化操作，将卷积运算与 注意力产生的输出沿通道方向拼接起来生成新的特征图，通过灵活调整注意通道 的比例，可以在卷积关注局部细节和自注意力获取全局语境之间找到最优的组合。
51
(2) 本文提出了一种基于独立自注意机制的单通道语音增强算法SAUNeto 上述AAUNet的核心思想是用自注意力机制去辅助和强化卷积运算，在第三章 的实验中发现，当自注意力机制的通道数占比为100%时，即全注意力机制，语 音增强的性能出现了下降。针对这一问题，本文对另一种独立自注意机制展开了 研究，希望其在语音增强领域能完全替代卷积而不是作为一种辅助手段。相比于 二维相对自注意机制，独立自注意机制可以自由设置运算区域的大小，通过多值 矩阵提升基于距离的感知能力，通过后续的一系列实验表明，基于独立自注意机 制的语音增强网络只需要增加十分少量的网络参数，就可以获得显著的性能提升。 本文目前的工作仅将独立自注意机制用作模型中的独立一层，在这一层中来完全 替代卷积层，而不是完全替代整个卷积网络，在未来的工作中，可以继续探究完 全抛弃卷积的基于纯粹注意力架构的语音增强模型。
(3) 本文在上述提出的网络模型中，采用Huber Loss作为损失函数。Huber Loss结合了 L1和L2损失函数的优点，通过合理设置超参数，可以调节Huber Loss对L1或者L2的侧重性，对噪声具有较好的鲁棒性，可以使网络模型更快 地收敛。
(4) 本文对二维相对自注意机制在语音增强卷积网络上的通用性展开了研 究。将二维相对自注意机制应用到其他基于卷积网络的增强模型中，并提高了模 型的去噪能力，证明了二维相对自注意机制可以作为一种插件应用于卷积网络， 以提升语音增强网络的整体性能。
