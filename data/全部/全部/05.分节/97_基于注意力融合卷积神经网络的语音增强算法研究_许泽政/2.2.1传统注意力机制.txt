2.2.1传统注意力机制
硬注意力机制和软注意力机制由Kelvin在2015年提出，用于图像描述 (ImageCaption)任务跑。下面给出软注意力机制的实现方式，具体如图2-3所 Zj\ O
给定编码端的输入序列X =[尤,^2,…，无”],给定从解码端学习得到的查询 向量0 ,图中矩形块代表一维向量，将所有输入序列分别与查询向量进行点积 运算，得到每个输入序列的注意力得分，表示一种关注程度，然后通过归一化指 数函数对所有注意力得分进行归一化，得到注意力权值向量A = [at,a2，-,an], 图中的方形块代表概率值，然后与对应的输入序列做加权平均处理，得到最终的 上下文输出序列y。
10
从上述的描述中可以看到，软注意力机制可以对每个输入序列赋予不同的权 重，帮助模型抽取出关键且重要的信息，模仿了生物神经元的关注能力，有利于 模型的学习。硬注意力的思想和软注意力类似，只不过软注意力是对所有输入序 列加权平均，而硬注意力是直接选择注意力得分最高的那个序列作为输出，值得 注意的是，硬注意力是不可微分的，不能直接用于训练。
此外，还有两种传统的注意力机制，分别为全局注意力机制和局部注意力机 制，由Luong等人I"于2015年在Kelvin的研究成果上提出，用于神经元机器翻 译领域。全局注意力机制的含义是对编码端所有的输入序列做注意力，整体思想 和软注意里十分类似，只不过在注意力权值向量和查询向量的实现方式上有所差 别。为了减少计算开销，局部注意力机制不对整个输入序列做注意力，它会预测 一个位置向量，只对属于位置向量内的序列做注意力，相当于是硬注意力和软注 意力的结合体，但它是可微的，同时还能减少计算开销，不足的地方是可能无法 准确地预测位置向量，导致模型的性能下降。
