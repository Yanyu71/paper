3.2.2二维相对自注意机制
上小节对基于UNet架构的全卷积神经网络模型做了详细的介绍和阐述。卷 积神经网络在语音增强领域也已经取得了不错的成绩。然而，卷积算子有一个明 显的特点，它通过有限的感受域来实现局部性运算，一次只处理特征图的一个局 部邻域，即所谓的局部感知能力，这种特性决定了卷积算子很难捕捉到特征图长 距离的上下文信息，从而丢失了感知全局语境的能力，这将不利于更好地恢复目 标语音。此外可以直观地感受到，在卷积网络中每一个通道表征着一种特征类型， 随着通道数的增加，网络中将会产生许多冗余的信息，逐层传递之后将无益于模 型的学习，如果可以通过某种手段帮助卷积网络强调有用的特征维度，抑制冗余 的特征维度，这在理论上可以使得基于卷积网络的语音增强系统性能仍有进一步 提升的空间。
在第二章中，本文已经对一种自注意力机制做了简单介绍，自注意力机制作 为捕获长距离上下文信息的重要进展而出现，已经广泛应用于机器翻译，答案选 择，图像标注等序列建模任务。卷积运算和自注意力机制本质上都是提取和学习 输入特征图信息的一种方式，只不过二者提取和学习数据的模式和侧重点不同， 卷积运算的局部感知能力以及权值共享特性决定了其更擅长学习局部信息。而自 注意力机制背后的核心思想是在输入特征图内部，对不同的隐藏单元(hidden units)赋予不同的权重，比如重要的单元被赋予“1”权重，冗余的单元被赋予 “0”权重，最后计算所有隐藏单元的加权平均值，以此来建模全局不同单元之 间的上下文依赖关系，帮助模型聚焦重要信息。由此可见，相较于卷积运算，自 注意力关注的是特征图内部自身的全局相互关系，因此也得名自(我)注意力机 制。本文研究的重点即是利用自注意力机制去强化全卷积神经网络，提高卷积运 算学习长距离上下文信息、获取输入信号全局语境的能力，由此期望进一步提升 整个语音增强系统的性能。
本小节将一种新的二维相对自注意机制应用于语音增强，它通过将自注意力 机制扩展到二维来加强自注意机制的表征能力，同时保证了卷积运算过程中的平 移等效性。此外，这种新的注意力机制采用了相对位置编码方案，通过在权重矩
16
阵中独立地添加相对高度和相对宽度信息来实现二维的相对位置编码，解决了自 注意力机制中排列等变性的问题。具体的实现方式是在运算过程中将学习局部信 息的卷积特征图连接到能够建模长程相关性的自注意特征图上，生成额外的特征 映射，而不是通过融合相加或者门控制来重新校准原始的卷积特征。二维相对自 注意机制将卷积运算与自注意力运算两者产生的输出特征图在通道维度上拼接 起来，这一特性允许本文灵活地调整注意力通道在整个特征图通道中所占的比例, 理论上可以在卷积关注局部细节和自注意力获取全局语境之间找到最优的组合。 最后，多头的二位相对自注意力机制还可以使模型同时关注空间维度和特征维度 (每个头部对应一个特征子空间)。
图3-3单头二维相对自注意强化卷积模块
如图3-3所示，本文给出了单头的二维相对自注意强化卷积模块的实现流程 图。多头注意力在下文中会给出其公式实现，因为多头的实现流程图较为复杂， 但是通过公式可以比较简洁地表示出来。在图3-3中，给定前一层的特征图输出 维度(T,F,C),其中T代表时间方向，F代表频率方向，C代表通道方向。本文 将前一层的输出(也即当前层的输入)进行展平操作，得到初始的二维张量 XwlfFxc ,通过三种不同的特征映射得到一个单注意力头的查询向量Q, (queries),键向量(keys)以及值向量匕(values),具体实现公式如下： 姑吧	(3-2)
Kh=XWK
(3-4)
其中，三个公式中的下标％表示第方个注意力头，和略wIRg” 是参数可学习的线性变换，用于将输入的特征图重新映射到三种不同的特征空间, dqlk和必分别代表每个注意力头的查询向量/键向量以及值向量的深度。对每 一个单头注意力，二维相对自注意机制的输出为：
其中，S/max(zJ仝严⑵)是一种归一化指数函数，常用在多分类问题上, 工 gexp(zj
代表不同类别的输出在所有可能的输出中所占概率的大小，它首先将输入向量映 射到指数空间，保证概率值非负，然后对所有可能的取值Z,- (z = l,2,3...C)进行归 一化，映射到0-1概率空间。这里本文通过SMmax(・)函数得到注意力机制的权 重矩阵，表示对不同隐藏单元的关注程度，最终得到单头二维相对自注意机制的 输出Ah。相比于一般的自注意力机制，本文在权重矩阵中嵌入了相对高度厅'和 相对宽度/^信息，这是一种相对位置编码方案，可以解决注意力机制中存在的 排列等变性的问题，下面本文进行具体阐述。
在自注意力机制中，如果没有加入明确的位置信息，自注意力是排列等变的， 即对输入序列的顺序并不敏感，因为自注意力机制本质上是对输入序列之间分配 权重，模拟当前序列对其他序列的关注程度，因此排列等变性是算法本身所决定 的，而本文认为这种对顺序的不敏感性对语音信号的增强和恢复是没有帮助的。 由此本文在自注意力机制中引入了相对编码方案，在权重矩阵中独立地嵌入相对 .高度和相对宽度信息。因为嵌入的位置信息是相对的而不是绝对的，所以这种编 码方案在解决自注意力机制排列等变性问题的同时也确保了卷积运算中平移等 效的性质。
在公式3-5中，P^l,P^^xTF分别是时间维度和频率维度上的二维相对 位置矩阵，对任意两个时频单元，它们之间的相对时间位置 和相对频率位置P%j)满足下述公式：
P%j) = q 用Q	(3-6)
形亿力=4心角)'	(3-7)
这里幺e表示第个时频单元的查询向量，也就是0矩阵的第「行向量， S陂％是沿时间维度的相对位置/-丿；的可学习嵌入，同样，礁/欣叫“是 沿频率维度的相对位置if~ jf的可学习嵌入。因此，对时间维度而言一共有 2*7-1个相对位置嵌入需要学习，对频率维度一共有2*F-1个相对位置嵌入需 要学习，这些相对位置嵌入在当前层的所有注意力头之间是共享的。举例说明， 如果本文只看时间维度，并且假设时间维度为3,那么一共有2*3-1 = 5个相对 位置嵌入需要学习：朮z；z•-2), r2(i,i-V),	, rA(i,i + V), rs(i,i + 2), i 表示
第i个时间点，斤(门-2)表示第i个时间点和第12个时间点之间的可学习嵌入， 其他的以此类推。相对位置矩阵展开如下：
18
罗(1,1) 加1,2)罗(1,3) 族) 価) 泅
P；性P『(2,1)耳"(2,2)罗(2,3)=色区)弘区)弘区) (3-8)
护3,1)膚(3,2)罗(3,3) %(彳)血) M
然后，将每个注意力头的输出A e Rr^沿着最后一个维度拼接起来，得到二维 相对自注意机制的输出：
A(X) = Concatenate^ Ai,...,Ah,...,AN]W0	(3-9)
这里N代表注意力头的数量，%€卅恥陋是线性可学习矩阵，用于将拼接起来 的注意力头的输出做一个线性拟合，最后，将&(力的维度变为(T,F,Ndv)a值得 注意的是，不加位置信息的自注意力机制的最大内存消耗为0((7^)2“)，如果保 存所有时频点的可学习嵌入，那么加上位置信息后会额外产生。((加Yd/)的内 存消耗，一般而言dq/k > N o为此，可以将cheng等人提出的高效记忆相对掩蔽 注意算法刖应用于二维相对自注意机制，可以将位置信息的内存消耗降至 0(%)。
最后，本文将标准卷积的输出Conv(X)和二维相对自注意机制的输出班X) 沿通道方向拼接起来，得到注意力强化卷积的最终输出：
O(X) = Concatenate[Conv(X)9 A(X)]
这里标准卷积的输出维度为⑺F,C-Ndv)。
