5.4.1强化学习策略制定
强化学习，是指深度学习模型在不忘记如何执行先前训练过的任务的情况下 学习连续新任务的能力，主要目标是克服对所学任务的遗忘，并利用先前的知识 在新任务上获得更好的表现或更快的收敛以及训练速度。
一般来说，主要有两种强化学习的结构用来解决鲁棒性问题，它们的区别主 要在于网络结构在学习过程中是否发生变化。第一类是使用大容量的固定网络结 构。在训练连续任务的网络时，根据模型参数对旧任务的重要程度，采用一些正 则化项来防止模型参数与先前学习的参数偏离过多。FearNet【50】网络通过使用伪 排练Pi】将最近的记忆整合到长期存储中来缓解灾难性遗忘，伪排练使用生成式 自动编码器来生成先前学习的示例，这些示例在整合过程中与新信息一起回放。 Fernando等人阿提出了 PathNet,其中一个神经网络在每一层有十个或二十个模 块，通过进化的方法为每一层的一个任务挑选三个或四个模块。然而，这些方法
44
通常需要不必要的大容量网络，特别是当任务数量很大时，因为网络结构在训练 期间从不动态调整。
另一种是通过动态扩展网络的方法来适应新的任务，与此同时保持原有结构 的参数不变。渐进式网络［53】用固定大小的节点或层扩展架构，导致网络结构极其 庞大,特别是面对大量的顺序任务。由此产生的复杂体系结构可能存储成本很高， 甚至由于其高冗余而不必要。动态可扩展网络(dynamic Expandable Network, DEN) 3〕在向原始网络添加新参数时引入了群稀疏正则化，缓解了这一问题。对 比来看，第一种策略所需的大容量网络并不符合本文的边缘计算目标，因此这里 会选用动态扩展网络的方式。
此外，强化学习最关键的地方在于需要和未知的数据进行交互并对其学习， 在每一步中，控制器观察环境的当前状态吐，根据策略71 CAt \St)决定行为儿， 并观察奖励信号4+1。这就需要制定合理的策略。
在强化学习中，主要常用的策略有两种。从Value Based强化学习方法开始 说起，它的核心是对于下一步调整的动作通过一个价值函数来计算出动作的价值, 随后从可能的动作集中，遍历选取一个最大价值的动作。这就要求动作集必须是 有限个的状态集合，否则遍历法将无法收敛。DQN算法［洌，是对Value Based强 化学习方法的一种改进，由于Value based算法的核心价值函数计算可能会很难 求解，当需要解决的问题数据集变大时，很难遍历计算价值函数，所以用一个Q 价值网络来代替价值函数，这个Q价值网络是需要提前通过反向传播算法来更 新权值，进行训练优化收敛的，这样在实际模型的使用中，新增的动作能够很快 通过固定好参数的Q网络来计算出该动作的价值，便于对比获得最优的下一步 动作。
虽然DQN对Value Based强化学习方法做出了很大改进，但是由于它们遍 历计算价值的共同点，它们同样只能处理有限个状态集合的问题，而在本文的示 功图分类识别目标下，对于大量新增的训练数据，通过遍历无法确定调整网络结 构的方向，所以针对本文的示功图识别目标这个策略是行不通的。
再来看Policy Based强化学习方法，它采取的思路其实是将接收到上一次任 务的状态与即将进行的下一次动作整体视作一个策略，将其用一个函数进行表示, 因为这样的函数通常是不可导的，所以会用到梯度上升法来找到这个梯度函数的 极值，也就是最优策略。
由此看来，虽然Policy based方法能够实现随机化的策略寻找，但是模型很 容易收敛到局部最优解，而且由于策略的随机性，导致会出现很多无效策略的尝 试。
在考虑到两者各有的优劣势之后，本小节决定采用两者合作的Actor-Critic 算法。其中Actor使用Policy Based方法，负责生成示功图分类识别网络结构调 整的随机策略。而Critic使用DQN算法，负责评估的每个网络结构调整策略的 收益价值，并从中选取最大收益的策略作为行动。
其中策略函数的可以被表示为式5-3o
7C0 (s, a) = P(a|s, 8) = n{s, a)
其中价值函数的可以被表示为式5-4和式5-5 =
总结来说，Critic的目标是计算最优价值策略，Actor的目标是利用Critic的 结果来更新参数，这样循环往复，每一次在实际工作中，示功图的新待检测数据 集输入模型分类识别打上标签之后，再通过Actor-Critic方法来实际调整模型的 网络结构，达到持续学习新来知识的目的。
