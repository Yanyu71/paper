5.4.2强化学习模型构建
在这一节中，将详细叙述强化持续学习［殉的模型的构建。它主要由三个网络 组成：控制器网络、价值网络和任务网络。控制器被实现为一个LSTM网络，用 于生成策略和确定每个任务将添加多少过滤器或节点。这里将价值网络设计为一 个全连通的网络，它近似于状态的价值。任务网络即是用于解决特定任务的任何 感兴趣的网络，在本文中，上一章中设计实现的卷积网络就作为任务网络。
在任务t-1的学习过程完成并且任务t到达之后，使用控制器来决定每个层 应该添加多少过滤器或节点。为了防止对旧知识的遗忘，在新任务到来时，不会 修改之前任务对应的网络权重，只训练新添加的节点权重。在训练了任务t的模 型之后，根据每一层的形状给每个新添加的过滤器加上时间戳。在推理过程中， 每个任务只使用t阶段引入的参数，而不考虑在后面的任务中添加新的节点来防 止知识遗忘。
当任务网络有m层的时候，当面对新来的任务，对于每一层i,需要指定一 个处于0到％ - 1的数来作为添加的神经元的数量。最直接的做法是遍历所有的 组合动作就可以获取第m层网络的最优配置。但是对于m层网络，这样的遍历 法获取最优策略的时间复杂度是。（口£%），这对于深层的神经网络如VggNet和 ResNet是难以接受的。
为了解决这个问题，此处将一系列操作视为固定长度的字符串。可以使用一 个控制器来生成这样一个字符串，表示每层中应该添加多少个过滤器。由于连续 层之间存在循环关系，控制器自然而然就设计为了 LSTM网络。在第一步，控制 器网络接收一个空embedding层作为当前任务的输入（即状态s）,该状态将在训练期间固定。对于每个任务t,将网络连接一个softmax层输出s表示对每个动 作进行采样的概率，即要添加的滤波器的数量。本文以自回归的方式设计LSTM, 上一步中的概率Pt作为下一步的输入。这个过程是循环的，直到得到所有m层 的动作和概率。动作序列血：祝的策略概率遵循如式5-6的乘积法则。
7T(ai：m|s; 8』=H巻 1 Pt，i, a-i	(5-6)
其中直表示控制器网络的参数。
接下来是任务网络部分，在本文中的任务网络是第四章设计实现的CNN网 络，这里将按顺序方式处理到达的T个任务，其中第T个任务的训练集表示为 Dt =仇刃｝亀，验证集表示为％ =仇力｝答，测试集表示为仇=｛兀如曾，对于 第一个任务，把其当做一个标准的监督学习问题来解决，去训练好任务网络。此 处将第t个任务训练好的参数定义为叱°。当第t个任务到达时，第t-1个任务的 最佳参数"上1已经是已知的。随后使用控制器来决定需要添加的网络节点数量 并实现得到一个扩展的子网络，它待学习的参数表示为甌(包括"上1)。新任务 的训练过程如下所示，保持"爲固定，仅反向传播WtWti的新增的参数。这样， 新任务的优化公式可以表示为式5-7o
min
这里使用如式5-8的随机梯度下降法来学习新增加的网络节点，7?作为学习 率。扩展的子网络节点将被进行最多epochs次训练或者达到收敛就停止。然后 在验证数据集％上测试子网络，并返回相应的精度儿。获得最大回报的扩展子网 络节点的参数将是第t个任务的最优参数，将其存储以供以后的任务使用。
为了便于控制器随着时间的推移产生更好的节点调整，这里还需要设计一个 奖励函数来反映每个任务中网络节点调整的表现。考虑到扩展网络的验证精度和 复杂度对于模型都很重要，因此将这两项结合起来设计了第t个任务的奖励公式 如式5-9 o
Rt =血⑸，％祝)+ aCt(iStla1:m)	(5-9)
其中验证集％上的准确率表示为血，网络复杂度表示为G = 一1：巻1他,其中伤 是第i层添加的网络节点数，a是用来调整验证集准确率和网络复杂度的奖励比 例的参数，由于&是不可微的，因此使用策略梯度下降来更新控制器，接下来将 详细叙述。
控制器的预测可以被看成是代表在m层网络中需要添加的网络节点数目的 一系列动作相当于是设计子网络的新结构并且在新任务中进行训练。在子网络收敛或者达到epochs时，控制器用其在验证集上的准确率人和模型复杂度 G通过奖励公式计算出定义的回报俭，以此来对控制器进行强化学习，控制器找 到新任务的最佳增量结构也就等价于如式5-10的最大化预期回报。
丿(比)=Vs/St)	(5-10)
其中&是真值函数，为了加速比在策略梯度上的训练，式5-11利用了以％为 参数的价值网络来近似替代价值U(St；弘)。
ec^R(St, a1:m) - 7(St; 0V))	(5-H)
其蒙特卡洛近似为式5-12 =
汕角 V9clogn(：a^\St-,比)昨4监)-VQSt> %))	(5-12)
其中N是batchsize,对于价值网络，釆用梯度下降法来更新％,梯度可以表
示为式5-13和式5-14o
总结来说，下面是强化学习训练过程的算法流程：
算法1：强化学习算法
输入：t个任务的数据集，D = {Di，D2, D3......Dt} 输出：第t个任务训练完成时候的参数权重Wt。 for i from 1 to T,进行迭代。
if t=l then:
使用6训练神经网络并获取Wf
else：
调整神经网络并获取Wf
end if
end for
48
算法2：网络节点扩展算法
输入：当前任务的数据集前一任务最优网络参数Wfi，每一层网络的结构 rii, epochs Te
输出：
for i from 1 to 7^,进行迭代。
使用controller的策略生成动作0：^
根据血：机以及前一任务最优网络参数"上1，梯度训练网络，获取叫⑴ 评估controller的梯度和生成的价值网络
end for
返回最佳梯度的网络参数"t12 = argmaxwp Rt
