2.4.2示功图样本增强
在数字归一化预处理后，其次急需解决的一点问题在于，基于示功图的抽油 机故障诊断领域并没有权威统一的公开数据集，从某油田获取的实际示功图样本 是比较少的，示功图在抽油机的样本有限，那本文构建数据集前，首先需要解决 的是示功图的小样本问题。
近些年在深度学习领域最常见的用来解决样本增强的问题的方法有生成对 抗网络［331(Generative Adversarial Networks, GAN)和自编码器［34］(Autoencoder, AE)。
14 GAN包括生成器和判别器两部分，这两个模型不断的进行博弈，并且持续epoch 轮次的交替训练或者收敛，生成器用于仿造假样本，判别器用于区分实际的真实 样本和生成器仿造的假样本，直到训练结束或者收敛，两个模型达到平衡，此时 判别器判断样本真伪的概率接近五成，生成器的生成高度近似样本目标也就达成 To而AE是包括编码器和解码器，数据集的样本数据的输入进编码器后，通过 网络权值的训练，将输入数据的高维特征进行降维映射。解码器用于接收数据降 维后的低维特征，然后通过对编码器镜像网络结构权值的训练，从低维特征从重 构出符合输入样本分布的数据。从前面的分析，可以知道它们最大区别是，AE 的训练目标和过程都是显式的，其训练过程强制性地把数据拟合到某种分布上， 而GAN通过博弈策略来优化生成器和判别器网络，达到让生成器产生数据的分 布的目的，最终是直接拟合训练数据的分布而不追求符合公式上的强分布性。为 了获取强分布性的数据集，以适应本文的示功图分类识别的场景，因此本文将在 本节选用自编码器来解决样本增强的问题。
首先，讨论基础的自编码器模型。编码器接受输入向量xe[0, l]d,并通过 关系式2-2来映射。
y — fe(.x)= s(Wx + b)	(2-2)
从而计算得到隐藏层表示yw[0, l]d，o随后，解码器部分将得到的低维表 征y通过关系式2-3来映射。
z = %心)=s(W'x + b')	(2-3)
从而将表征向量y重构为输出向量z e [0, 1严。其中权值矩阵VT是"的转 置矩阵，即W' = WTo在这种情况下，每个输入训练的X都会被式2-2映射成一 个相应的y,再被式2-3重构成一个增强样本z。在训练过程中，参数的优化目 标是使得重构向量z和输入向量x的误差最小化，如式2-4 =
6,6' = argmin^'Zf^ L{x',z'}	(2-4)
其中，L是损失函数。这里还可以用到另一种损失函数是重建交叉嫡，因此 此处应该如式2-5将x和z解释为位向量或位概率向量。
LH(JC,Z) = - Xk=i xk logzk + (1 -%k) log(l - zk)	(2-5)
需要注意的是，如果这里X是一个二维向量，贝必H(X,Z)需要表示成示例X 的负对数形式，在给定参数z的情况下，式2-4可以写成式2-6。
6,0' = argminEqoM [L{X, gB< (x)))]	(2-6)
其中q°(Q表示属于相同的经验分布的n个训练输入样本。
但在本小节选用的是卷积自编码器来解决示功图的样本增强问题。其原理和 自编码器一样，但是把全连接层给去掉后，取而代之的是卷积层和池化层的组合。
15
从上面的公式推导可以看出，使用的是全连接层的基础自编码器，完美适用于一 维输入向量的样本增强，但是对于二维的输入向量，比如视频或者图形，全连接 层会损失一部分空间的特征信息，相比较之下，通过卷积层的特征提取和池化层 的降维抽取，二维输入向量的空间特征信息能够被更好的保留下来。图2-13和 图2-14分别是卷积层计算和池化层计算的示意图。
♦ Output
aw + bx + ey + fz
图2-13卷积层计算示意图
池化前	池化后
图2-14最大池化层结构图
卷积自编码器的整体原理还是属于自编码器，但是通过卷积层对于输入向量 的特征提取后，对二维输入向量的局部特征进行了保留，卷积神经网络的权重是 共享的。因此二维向量的重建过程可以理解成是将保留了特征的像素块按照特征 分布进行排列恢复。训练过程重参数优化用到的损失函数也与基础正则自编码器 一样，具体可表示为式2-7。
= argmin^^iKx'.z') + (2-7)
通过上述分析，选用了卷积自编码器来进行样本增强，这对于本文的工作其 实是具有两方面的优势的，一方面解决了示功图自身的小样本问题，另一方面卷 积自编码器的编码层在学习分布的过程中能够学习到一些示功图图形的特征，可 以把其网络参数保存下来，再利用这些参数来预训练本文后面的卷积神经网络分
16
类模型，这在一定程度上也能够解决小样本问题，并且避免分类模型陷入局部最 小以及过拟合，本章设计实现的卷积自编码器的结构如图2-15所示。
图2-15用于示功图样本增强的卷积自编码器结枸
模型输入的示功图是224x224像素，编码部分具有2个卷积层和池化层的 组合以及一个全连接层，卷积层是为了将示功图的特征进行局部感知，避免对每 个像素都进行获取，一是这样的权重数量巨大难以训练，二是这样的操作也很容 易造成模型过拟合，而池化层是对卷积得到的功图局部特征值进行按规则下釆样, 比如本文用到的是最大池化方式，在一定矩阵范围内取其最大值。如图，通过两 层这样的结构，就能够得到表示输入示功图的20维矢量。随后的解码部分，其 实也就是编码层的镜像结构，会将表示形式解码回224x224像素图像。
由于最大池化操作(Max-pooling)在TensorFlow没有内置相应的反池化方 法，因此本文解决的思路是用左上角的原始条目用NxM内核替换池映射中的四
个角落的条目，其中N和M为池核的形状，如图2-16所示。
在CAE预训练构建完成后，可以看到对示功图的重构结果，如图2-17所示。
口 O H 口 □口
口口 u [J □口
d zr CL 一 o zr
d zZ £1 — O b
zr c? C7 o zZ
' J™	'"i rl X ■-亠' 西	-
£~* ZZ C* C7 O ZT
_ Q _ 口 d O
—4 ™. O a o
D 口 _ 口 £1 O
口口 一口 d o
rx □ □ u 口 o
二口匸 q u o
图2-17自编码器重构示功图的部分结果
图2-17左是输入的示功图数据集，图2-17右是经过自编码器重构的示功图。 大部分示功图都能够被解码器部分完好的重构出，也代表着此时编码器部分己经 自学习到一部分示功图特征。
