4.2.1循环神经网络(RNN)模型
RNN节点就是与普通的神经元节点之间的区别就是它多了一层链接同层的节点 到自身节点的权重，每个RNN节点能够接收到上一时刻的传递过来的信息，这其实 就是人为地添加了一个先验信息：认为RNN单元前一时刻的状态对后一时刻的状态 有直接影响，在这点上与马尔可夫模型【的有相似性。
44
Output Layer
Hidden Layer
Input Layer
图4-4 RNN结构示意图[34]
图4-4所示就是就是一个标准的单隐层的RNN网络，从下面的前向传播公式就 可以看出RNN节点在t时刻点的值来源于两个部分，一个是input layer的输出，另 一个就是hidden layer自身t-1时刻的值。
% =6 % + U也-+ 4）	（式 4-33）
弘=6,忆九+九）
（式 4-34）
其中为是时刻t的输入向量，々是时刻t隐层状态，乂是时刻t的输出向量，匕和 bh是输入层与隐层间的权重矩阵和偏执，Wy和力是隐层与输出层间的权重矩阵和偏 执，“是隐层上一时刻向下一时刻映射的权重矩阵，叫是激活函数，一般使用Sigmoid 函数或ReLU函数。
图4-5是RNN 一个前向传播的示意图，1-7表示的是不同的时刻，时刻越远的 节点用越黑的圆来表示：
输出层
隐层
输入层
2	3	4	5	6	7
图4-5 RNN前向传播示意图
我们可以看到，在时刻7,隐层实际上接受了从1到7时刻所有传过来的信息, 并且这个过程中权重矩阵4是保持不变的。	° '
45
但是这也就出现了一个问题，由于4不变的，我们假设激活函数叫是ReLU, 就可以发现前面某一个时刻传递过来的信息被表示成&的连续乘积的形式。这意味 着权重矩阵中任意一个权重的幕次都会随着时间不断增加，该权重任意作用于一个输 入节点之上会使该节点传递的信息要么为零，要么很大，出现梯度消失和爆炸的情况。 如果我们把外换为Sigmoid函数，虽然能够控制梯度爆炸，但是还是无法解决梯度 消失的问题。
所以通常RNN在较长序列预测上会出现衰退或者震荡的情况。随着时间对自身 权重的不断累积，也就是为什么简单的RNN网络不能处理长序列的原因。
1）长短时序记忆（LSTM）同单元
如图4-6所示，长短时序记忆单元是一种对RNN单元的改进结构，他引入了三 个门：输入门/\输出门遗忘门每个门输出是一个近似二值化的形式， 起到了一种类似电路开关的效果，它根据当前的输入和上一时刻的输出决定自身的输 出，当门输出接近于零时意味着门被关闭，，接近于1的时候意味着门被开启。
图4-6 LSTM单元结构【】
输入门控制着信息是否向LSTM单元的输入，输出门控制着LSTM的信息是否 向外的输出。s,为LSTM的记忆单元，状态会随着输入不断累积。而为了避免LSTM 记忆单元的输出趋于饱和，遗忘门被引入进来，它在适当的时刻会使记忆单元清空， 控制着记忆单元是否遗忘信息。因此LSTM单元相比于RNN单元拥有更加完善的信 息记忆机制，不会因为序列过长而发生梯度爆炸或消失，在其门的结构控制下能够实
46
现对数据更加有选择性地学习。
LSTM的前向传播公式。
输入门：
1	H	C
a； =£+ £w" 丁	（式 4-35）
21	h=l	e=l
b：=_f（a；）	（式 4-36）
输出门：
1H	C
% = Z *刷 + E whe）b'~l + Z “广	（式 4-37）
7	A=1	c=l
%=/«）	（式 4-38）
遗忘门：
]	H	C
W = E Wj# + £ 叫*7 + £ W稣S；1	（式 4-39）
i=l	h=\	c=l
4=/0）	（式 4-40）
记忆单元：
I	H
4=2 w遥'+ X W加娟	（式 4-41）
/=1	/?=1
S：=哈丁 + b；g（4）	（式 4-42 ）
网络输出
其=叫〃区）	（式4-43）
其中，
/W = -^—T	（式 4-44）
1	+ e
47
於）=春-2
2
3）=4尸1
（式 4-45）
（式 4-46）
