2.2基于时空融合的多媒体事件检测框架
本文基于对当前主流的多媒体事件检测框架的优势与缺点的分析，并结合了当时 视频分类任务上较新的一些方法，提出了如图2-3所示的基本框架，其过程可以分为 三个部分：
（1）基于深度网络的特征提取。本文利用深度的卷积神经网络对视频帧进行特 征提取，包括密集局部特征和显著区域特征提取算法。同时，我们也比较了非深度学 习方法中的IDT算法。最终选取了密集局部特征作为基于深度网络的特征提取的基本 算法，具体细节将在本文的第三章中进行详细的介绍。
（2）多媒体事件的时空表示。利用视频帧的时间与空间关系进行特征融合，形 成对视频的时空表示。本文比较了 VLAD、LSTM、netVLAD、时域卷积等特征融合方法, 最终选取了 VLAD作为多媒体事件的时空表示的基本算法，具体细节将在本文的第四 章中进行详细的介绍。
(3)基于Linear SVM的多媒体时间分类。根据训练视频的特征训练线性SVM 分类器，对测试视频进行相关事件的检测，具体细节将在本文的第四章中进行详细的 介绍。
视频帧
\基于深度网络 臂的特征提取
多媒体事件的 时空表示
基于Unear
SVM的多媒体 事件分类
图2-3基于时空融合的多媒体事件检测框架基本结构
本文的方法结合上述主流两种框架的优点，并改进了相应不足的地方。
首先，在视频帧的表示上，本文采用了基于深度卷积神经网络的特征提取，相比 于提取语义信息，介于图像像素信息和抽象语义信息之间的视觉特征与大大加深了对 视频帧的描述的精确性。
其次，与利用语言模型对语义信息进行分析与整合相似，本文提出采用时空融合 的方式来处理帧级别的视觉特征，这相比于求平均的方式将会有较大程度上的提高。
