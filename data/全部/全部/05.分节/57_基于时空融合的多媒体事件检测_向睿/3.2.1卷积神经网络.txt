3.2.1卷积神经网络
卷积神经网络是在原始的神经网络基础上新提出的一种结构，图3-3中，(a) 为原始神经网络的连接结构，(b)为CNN的链接结构。相比于原始的神经网络， CNN最首要的一个变化在于局部感受野的提出，利用参数共享的滤波器实现了从底 层图像原始信息到高层语义信息的逐步抽象。
(a)	(b)
图3-3卷积神经网络网
局部感受野的思想被证实与人的视觉神经类似，CNN网络中卷积层的节点与前 一层的特征图呈局部连接结构，每个节点都对应了前一层中局部区域，它只接受这些 节点的输入，这种局部连接的结构使得网络参数大大减少。而另一方面，这些局部连 接的节点间的参数是共享的，而不同的视觉概念靠多个不同的卷积滤波器来映射，这 种参数共享的结构同样极大程度上减少了参数，使得在图像领域训练高层的神经网络 结构成为可能。
在CNN对原始图像进行逐层抽象后，高层的滤波器需要更大的感受野来抽象出 新的概念，而CNN中的下采样层通过直接减少特征图尺寸的方式变向扩大了感受野,
15
使得越高层的滤波器对应原图像上的感受野越大，从而实现了原始图像信息被抽象、 融合、再抽象、融合 的过程。下采样层目前一般采用Max Pooling或者Average Pooling,分别是对输入求最大和求平均的操作，这两种结构都保证了局部的旋转不变 性和尺度不变性，使得模型具有良好的泛化性。Max Pooling能够突出感受野的响应, 作为中间的下采样层十分有效，而Average Pooling是对响应的平均，适合用于网络 最后一个的卷积层融合特征图作为图像的输出特征。
在目前常用的CNN网络中，一般采用ReLUm这种非线性激活函数，相比于传 统的Sigmoid函数，ReLU具有更快的收敛速度，同时使神经元具有更强的稀疏性， 而且被证实更加接近于生物的脑神经信号。两种非线性激活函数如图3-4所示。（a） 为Sigmoid函数，而（b）为ReLU函数。
（a）	（b）
图3-4非线性激活函数
