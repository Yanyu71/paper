4.3.2干扰感知多智能体Q学习模型
在得到每一条V2V链路的最优QoS参数后，可以得到异质频谱分配子问题， 如下所示：
max V EC,	（4-21）
WP	0 広N	"
Zq泌
y：2y：,
S.t.\ M
、％，卩”貯｛0，1｝，
本研究将异质频谱共享问题建模为一个马尔可夫决策过程MDP = （S,A,P,r）, 其中S表示状态空间，力表示动作空间，尸表示转移概率矩阵，尸表示奖励。马 尔可夫决策过程具有马尔可夫性（即系统的下一个状态仅与当前状态有关），但 不同的是马尔可夫决策过程进一步考虑了动作，即系统的下一状态不仅与系统的 当前状态有关，还与系统采取的动作有关。
针对于MDP问题，可以将其进一步建模为干扰感知多智能体深度Q学习模 型。Q学习是智能体以“试错”的方式进行学习，通过与环境（State）包括邻居 智能体进行交互获得奖励（Reward）来指导其选择动作（Action）,并训练出最优 的动作选择策略（Policy）,使智能体的奖励最大化。干扰感知指的是，智能体具 有感知在每个频段受到干扰的能力。在所设计的分布式算法中，由于智能体在观 测周围环境时，邻居用户的数据具有不确定性。因此，不同于已有的基于强化学 习的频谱共享算法，只观测固定数目的邻居用户，保证状态空间维度的稳定。本 研究采用干扰感知的方式，通过智能体感知到的在每一种频谱资源所受干扰构建 状态空间，来反应邻居车辆的存在与否、距离远近等信息。
47
图4-3干扰感知多智能体Q学习模型
所设计的干扰感知多智能体深度Q学习模型如图4-3所示。每一个V2V通 信对作为一个智能体，在第马个时隙，每一个智能体与通信环境进行交互，即从 状态空间S中观测一个状态％。每一个智能体基于观测到的环境选择动作，根据 已训练得到的策略”从状态空间力中选择一个动作a”,，这里的动作代表蜂窝频 谱资源分配参数和通信模式选择参数。在第n,个时隙选择动作后，通信环境发生 变化，智能体观测到状态％+i，并获得奖励这个奖励由智能体和其邻居智 能体所能达到的有效容量以及CUE通信质量决定。
具体的干扰感知多智能体深度Q学习模型如下所述：
1）	状态空间S
由于智能体具有干扰感知功能，因此，所设计的状态空间包括以下几方面： 蜂窝通信模式V2V链路的瞬时信道状态信息g”，（Channel State Information, CSI）；毫米波通信模式V2V链路的瞬时CSI \ ；蜂窝通信模式V2V用户所复 用的蜂窝用户的瞬时CSI g；；上一个时隙，智能体在每一个频段感知到的干扰 ...,7^1'］；每一个V2V链路发射用户当前业务s可容忍延时门 限為ax和最大可接受数据溢出/丢失概率昭ax； V2V链路的瞬时流量到达速率兄”,。
邻居车辆的定义，使用文献［56］中的信噪比限制区域计算得到，当V2V链路 的发射机处于V2V链路的接收机SLA范围内，则为V2V链路的邻居车辆。
2）	动作空间力
动作空间表示为^{a„,|Va„z	其中a”, =0频谱代表使用毫
米波频谱资源，即％ = 1, Pg =	；其中a”,=加'表示使用蜂窝用户％
的频谱资源，即ct,n =0, plnCm, = lo
3）	转移概率矩阵尸
我们定义马尔可夫决策过程中，状态％在选择动作后，转移到下一状态 s”,+］的状态转移概率为P（%+i，G丨％卫片）。且在本模型中，智能体没有任何通 信转移概率矩阵P =［卩（％+］爲I S®，a% ）］的先验知识,因此可以使用model-free 的Q学习模型求解。	''''
48
4) 奖励口
瞬时奖励被设计为智能体和其邻居智能体所能达到的平均有效容量，目的是 为了在满足不同业务的QoS需求的前提下，提供尽可能高的系统传输速率。同 时，智能体还需要考虑满足蜂窝用户的最低通信质量要求和V2V链路的源数据 传输速率保障。当蜂窝用户的接收SINR严”小于门限/■:时，则返回一个负反馈 治＜0。当V2V链路的EC,”小于源数据到达率门限金时，则返回一个负反馈
万＜ 0。 因此，第坷时隙的瞬时奖励可以表示为
其中，刀为当前智能体和其观测到的邻居智能体数量的总和。
5) 策略”
传统的Q学习，定义了动作价值函数0(%,a”,)表征在状态％情况下选择 动作a”,后带来的影响，是关于有限状态的“表格值函数”。Q学习通过状态-动 作函数QgaJ来选择最优动作，即在状态％情况下选择能使Q值最大的函 数，通过这种方式，可以得到最优控制策略;即
分(% F 吨 max Q(s”, a”J	件占)
0(%)通过bellman公式更新，如下所示
0(%,	, a“, )+a[&+厂 0傀,aj]	＜4'24)
其中，R",+E+I + /maxVisS O(s”,+”a”,+J表示在状态sn：时选择动作a”,后所能获 得的长期折扣回报，心代表学习率，7代表奖励折扣因子。
但是，传统的Q学习由于使用Q-table来表征有限的状态-动作对，因此，在 面对高维的通信环境状态空间时，极大的Q-table可能面临大部分状态-动作对不 被访问，Q值更新慢，收敛速度慢。因此，本研究考虑使用神经网络来拟合状态 动作之间的复杂映射关系，使用深度强化学习来进行V2V异质频谱分配。
