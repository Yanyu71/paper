6.1.1实验数据集
迄今为止，许多可用的语音数据集都只包含一个说话人处于活动状态。如第 一节所示，包含重叠语音段的数据集缺少准确的注释，因为混合中语音开始和偏 移的注释对人类来说相对复杂，或者缺乏可控的听觉环境，例如电视/广播场景。 由于没有完全重叠的（overlap）说话人讲话场景的真实数据集，因此我们选择手 动生成合成带有overlap的混合语音数据，即模拟“鸡尾酒会场景”的数据。
我们认识到，在模拟的“鸡尾酒会”环境中，混合语音数据缺乏人际交流的对 话内容，但提供了受控的语音环境，有助于理解说话人数量估计模型（如baseline 中的CRNN结构和本文提出的基于GST结构的方法）如何解决计数估算问题与说 话人聚类模型对于overlap数据的处理能力。我们选择了一个语音语料库，该语 料库优先选择大量不同的说话者，而不是发声的次数，从而产生了大量独特的混 合语音数据。这部分数据的选择与处理借鉴论文［36］中的方法。为了进行训练， 我们选择了与论文［36］相同的数据集LibriSpeechclean-360［54］,其中包括921名说 话者（439位女性和482位男性说话者）的363小时英语语音，并以16 kHz采样。
为了生成单个训练样本{X, k},我们从语料库中提取了一组独特的L个说话 者。对于每个说话人，选择其一个随机语音，重新采样为16 kHz并应用WebRTC VAD,在第二章中介绍的基于GMM模型WebRTC规范的VAD方法。本文在生成 overlap语音时，在心好内随机选取人数，即为真实类别标签（ground truth） ko根 据该人数设定，从数据集中随机选取speaker,之后，从该说话人的语音数据中随 机选择一个，为合成语音中的一个说话人数据。接下来WebRTCVAD估计用于消
33
除发声记录开始和结束时的静音。随机选择混合位置，混合不同说话人的数据， 直到达到所设定的说话人人数。对所有说话人重复进行此过程，以便创建L个时 域信号。信号经过混合和峰值归一化以避免削波。
此外本文实验中，说话人聚类任务的训练数据集包含超过14万个音频剪辑， 总共约100个小时，所有视频片段均来自YouTube。此外，该数据集是中文数据 集。
本文使用LibriSpeech数据集手动合成overlap多说话人数据，并在训练说话人 数量估计模型时，加入上述的中文数据集。除了增加数据量的作用外，该数据集 包含加入了真实的生活背景音等，增加了数据集中的语音环境的多样性，此外 LibriSpeech为英文数据集，加入中文数据集，增加了数量估计任务中，模型对语 言的鲁棒性。由于本文需要使用说话人数量估计模型检测overlap数量，且时长片 段最小需要达240ms,因此额外合成了不同长度的overlap语言数据，用于增强 countnet模型对于不同时长的语言数据的检测能力，尤其是非常短小的语音片段。
最后另一份需要说明的测试集是LibriCount数据集。在countnet论文中，作者 合成并公开了LibriCount数据集，LibriCount数据集包含［0..10］个说话者的模拟鸡 尾酒会环境，共包含5720个持续时间为5秒的多说话人混合语音数据。因此在测 试countet模型时，我们也使用LibriCount作为另一个用于与countnet-baselien模型 对比测试数据集，进行说话人数量估计任务的测试数据。
