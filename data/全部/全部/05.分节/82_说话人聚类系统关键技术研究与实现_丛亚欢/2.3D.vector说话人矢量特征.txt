2.3D.vector说话人矢量特征
本节介绍说话人特征矢量方法。在当今说话人特征表征领域，一共存在三种 主流的特征矢量表征方式，分别是i-vector[16], x-vector[19], d-vector[30],其中 d-vector是一中基于深度学习的特征表示方法，由google提出，其效果在一直的测 评上达到甚至超越经典i-vector特征表示方法，成为当前说话人表征的有效方法 之一。
i-vector是代表帧级特征分布模式的特征。i-vector提取实质上是GMM超向量 的降维(尽管在计算i向量时未提取GMM超向量)。d-vector使用DNN方式训练而 11
得。为了提取d-vector,需要对DNN模型进行训练，从该DNN的最后一个隐藏层 中获得。因此，与i-vector框架不同，它没有关于要素分布的任何假设（i-vector框 架假定i-vector或潜变量具有高斯分布）。
不同的蓝色代表来自不同说话人的embedding/utterance［30i
本文所使用的特征矢量表征方法为d-vector的改进版本，基于GE2E （Generalized end-to-end）的d-vector表征矢量。在论文［30］中提出了通用的端到 端（GE2E）损失函数，以更有效地训练说话者验证模型。理论和实验结果均证 实了这种新型损失函数的优势，此外论文［30］中还引入了MultiReader技术来组合 不同的数据源，从而使d-vector的模型能够支持多个关键字和多种语言。结合这 两种技术，d-vector模型产生了更准确的说话人矢量。
GE2E训练数据采取批处理形式，每组数据包含N个说话人，每个说话人选取 M个语音数据，如图2-2所示。每次选择N*M个utterance （在本文中指240帧的mel 特征）组成一^ batch,这些utterance来源于N个不同说话人，每个人选择M个不 同的utterance,用咒巧表示从第i个说话人的第j个utterance。使用［31］提出的基于 LSTM的特征提取网络，竝丿•作为网络输入，经过LSIM结构后连接一个线性层作 为网络d-vector的输出层，而d-vector则是线性输出层的L2归一化的结果。
GE2E是一种相似性度量方法，用于衡量d-vector：	和各自类中心q的相似
度距离，相似度矩阵》讥被定义公式（2-10）,具体计算方式参考［30］o
Sji,k = 3 • cos（e；i，ck） + b	（2-10）
在训练过程中，希望每个utterance的embedding相似于该说话人所有 embedding的质心c,同时又远离其他说话者的质心。如图2-2中的相似度矩阵所 示，我们希望有色区域的相似度值较大，而灰色区域的相似度值较小。图2-3以 不同的方式说明了相同的概念：我们希望蓝色嵌入矢量接近其说话者的质心（蓝 色三角形），而远离其他质心（红色和紫色三角形），尤其是最接近的质心（红色 三角形））。给定嵌入向量句，所有质心q和对应的相似度矩阵®i肿 可以通过公 式（2-11）和公式（2-12）两种方法计算损失函数，为了计算简便本文使用公式 （2-11）作为loss的计算方法。
图2-3说话人类别间质心关系㈤]
此外，论文[30]中提出了MultiReader方法，一致类似的正则化技术，帮助解 决不同数据源下训练同一模型的规范问题，同样，本文在训练d-vector时，使用了 该技术。考虑以下情况：我们关心具有小数据集D1的域中的模型应用程序。同 时，我们在相似但不相同的域中拥有更大的数据集D2。我们希望在D2的帮助下 训练一个对数据集D1表现良好的模型：
厶(Di—； w) = ExeD1[L(x; w)] + aExeD2[LCx; w)]	(2-14)
在正常正则化流程中，通常使用a||w||2对模型进行正则化。但是MultiReader 中使用了ExeD2[LQx; w)]进行正则化。当数据集D1无法容纳足够的数据时，在D1 上训练网络可能会导致过度拟合。要求网络在D2上也要表现得相当好，这有助于 规范网络。可以将其概括为组合K个不同的，可能极不平衡的数据源：%，…,DK° 对每个数据源进行权重加权，表示该数据源的重要性。在训练过程中，在每个步 骤中，从每个数据源中提取一批/多组话语，并将合并的损失计算为，其中， 厶(x/w)是(2-13)中定义的损失函数。
厶(D1，...,DK) = y akEx eD [L(_xk; w)]	(2T5)
*fc=l
13
