3.4AdaBoost算法
28
AdaBoost算法即自适应增强学习算法，是基于Boosting思想的迭代型的机 器学习算法，是针对同一个训练集训练不同的弱分类器，然后再把这些弱分类器 进行结合从而构建成一个最终的强分类器。AdaBoost算法中前一个弱分类器分 错的样本的权值会得到加强，之后被用来进行下一个基本分类器的训练，并同时 在每一轮的训练中加入一个新的弱分类器，直到迭代终止条件。迭代终止条件可 以理解为达到预先设定的最大迭代次数或者是足够小的错误率。
AdaBoost算法流程如下：
输入：训练数据集T =	(x2,y2), -,(xN>yN))
过程：
(1)初始化训练数据的权值分布= (Wu，...,WU,	wu =9=
1,2,
(2) repeat
for m E [1,2, do
使用具有权值分布Dm的数据集进行学习
学习后得到基本分类器6„1(%),
计算GMx)的分类误差率见公式(3-14),
计算GmG)的系数见公式(3-15)
更新训统数据集权值分布Dm+l =(Wm+l,l，…,Wjn+Li，…,
(3)构建基本分类器的线性组合7(x).
(4)得到最终的分类器G(x).
输出：最终分类器G(x)
算法中"是实例问题，9是标记集合，其中％iENGR% % wy = {-1,+1}。基 本分类器Gm(x)在训练数据集上的分类误差率计算公式为：
em = E乜 1 wmiI{Gm(.xi)芋 %),	(3-14)
计算Gm(X)的系数见公式：
手，	(3-15)
在更新训练数据集的权值分布时，Dm+1 = (Wm+1,1，…,Wm+1,0…,Wm+i,N)中 Wm+l,Z计算公式为：
/eYm,Gm8) =%
Wm+i,i =意 a …-	(3-16)
I
其中，Zm是规范化因子，其计算公式为：
zm = £匕 Wnie-amyiGM^	(3-17)
使Dm+l成为一个概率分布。在更新权值时，被基本分类器GmCO分类错误的样 本的权值被放大，而分类正确的样本的权值变小，分类错误的样本在下一轮学 习中重要性变大。在构建最终的分类器时，线性组合f (%)实现了 M个基本分类
29
器的加权表决，系数a7n代表了基本分类器%，(%)的重要程度，需要注意的是的n 之和不为1。构建基本分类器的线性组合公式为：
fM =Sm=iam^mW-	(3-18)
得到最终的分类器G(%),其计算公式为：
G(%) = sign(/(X)) = sign(£^l=1amGm(x')').	(3-19)
AdaBoost算法属于集成方法，是一种再学习的方法。集成方法有Bagging方 法和Boosting方法两种，其中Bagging方法是基于数据随机重抽样分类器构造的 方法，而Boosting方法是基于所有分类器加权求和的方法，AdaBoost方法就属 于Boosting。Boosting方法有两个重要的关注点，一个关注点是指该方法需要进 行一系列弱分类器的训练，另一个关注点是指将弱分类器组合成为一个强分类器。 AdaBoost算法是针对弱分类器的训练，加大了分类误差率小的分类器权值，使 其在表决时起更大作用，而对于将弱分类器组合成强分类器的部分，增大了分类 误差率小的分类器权值，使其在表决时起更大作用，从而得到强分类器。
AdaBoost算法的优点有分类精度高、泛化错误率低，不易过拟合，易编码， 可用在绝大部分分类器上，无参数调整，实现及应用简单，无需做特征筛选，提 供的是框架，可以使用各种方法构建弱分类器等。在实际应用中，将AdaBoost的 多个弱分类器算法组合起来之后，可以达到降偏差的效果，进而得到一个偏差小、 方差小的泛化能力好的模型，因此不会过拟合。AdaBoost算法虽然有很多优点， 但却对离群点很敏感的缺点。AdaBoost算法适用于二分类或多分类的场景，适 合用在特征选择上，用于做分类任务的baseline等。
30
