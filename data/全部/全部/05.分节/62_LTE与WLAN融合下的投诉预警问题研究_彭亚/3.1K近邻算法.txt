3.1K近邻算法
K近邻算法，即KNN算法，是一种较为常用的有监督机器学习方法，通过 测量不同样本之间的距离来对样本进行分类。算法思路可以概括为在特征空间中, 若某一样本的K个最近似的样本绝大部分属于某个类别，则该样本也同样为该 类别。通常情况下对样本进行类别的判定主要是根据样本周围最相邻样本的类别 来进行的。
KNN算法流程可以表述如下：
输入：B[n]代表N个训练样本在特征空间中的坐标，k代表近邻样本个数 过程:
1：取B[l]〜B[k]共k个样本，作为x的初始近邻样本；
2：计算k个近邻样本与测试样本x之间的距离由，且i=l,2,..…,k；
3：取心的最大值作为最远样本距离D,即D = ?nax{di= 1,2,k)； 4：计算样本x与余下的n-k个样本的距离由；
5：比较样本距离山与D的大小，若dt<D,则用必取代D;
6:得出最终所需的K个样本；
7：计算样本x属于每个类别的概率，概率最大的那个即为样本x的所在类. 输出：x所属的类别
KNN算法在应用的时候需要注意的要素有K值的选择、距离的度量以及分 类决策规则。其中，K值的选择会对算法的结果产生重大影响，当k取不同值时， 分类结果也会有相应不同，K值较小容易发生过拟合，K值较大则学习的近似误 差会增大。K通常是不大于20的整数。距离的度量一般采用人距离，距离越近 意味着这两个点属于一个类别的可能性越大。在对距离进行度量之前，需要对将 样本的每个特征进行规范化处理，可以防止初始值域较大的特征权重过大以及初 始值域较小的特征的权重过小的情况。其中，
Lp = J££=i%一”|p	(3-1)
当p=l时为曼哈顿距离，当p=2时为欧式距离。除了用及距离对距离进行度量 外，也可采用夹角余弦来进行距离的衡量，如文本分类采用余弦计算相似度就比 欧氏距离更合适。分类决策规则有投票表决法和类别判定法。投票表决法，即为
23
待分类样本的近邻中哪个类别的样本最多就将该样本分为这个类别，是以概率为 标准，可以通俗的理解为少数服从多数原则。加权表决法则是根据样本间距离的 远近程度为依据，对近邻样本的类别进行加权，距离越近则对应权重越大，是以 量化为标准。
KNN算法的优点是精度高，对异常值不太敏感，且简单、易于理解和实现， 不需要估计参数以及训练，适合对较为稀有的事件或样本进行分类，尤其非常适 合处理多分类问题。算法缺点是在样本的类别不平衡时样本可能会被划分到样本 容量大的类别，此外算法的计算量很大。在样本类别不均衡的时候，若某一个类 别具有相对过大的样本容量，而其余类别的样本容量过少，就可能导致该样本被 分类到样本容量过大的那一类别，可以采用对类别加权的方法来解决这一缺点， 与样本距离较小的赋的权值较大,而与样本距离大的权值较小。在KNN算法中， 每一个样本进行分类都需要计算该样本到全体已知样本的距离，因此算法的计算 量大、相应的内存开销也大，针对于此问题，可以采用对已知的样本点进行剪辑 的方法，去除在样本分类当中影响较小的样本，从而降低算法的计算复杂度。
KNN算法可以简单的理解为在一堆已经确定类别的样本数据中，当有某个 新的样本进入时，计算其与每个确定类别的样本间的距离，挑选距离最近的k个 点，并确认k个点的类型，并按照样投票原则将新的样本归类到占比相对较大的 那个类别。
