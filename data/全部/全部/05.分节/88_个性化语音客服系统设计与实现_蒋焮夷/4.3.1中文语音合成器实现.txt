4.3.1中文语音合成器实现
在迸行模型的训练之前，考虑到语音信号数据量较大且需要配合音色编码器 的使用，如果直接采用训练过程中读取音频文件做处理的办法会极大地增加训练 时长，本文在进行训练前先进行了数据的预处理操作，主要包含了训练文本的生 成、对正确结果的梅尔频谱进行向量化存储预处理和对于编码向量存储的预处理。 启动训练的步骤如下图4-10所示。
图4-10语音合成模型训练流程图
57
这里的中文语音合成器还是以Tacotron模型为基本选型，原官方模型采用为 英文，可以通过空格分词后将词汇对应为音素，但是中文文字显然无法直接进行 音素对应，且中文发声需要考虑音调的变化特征，本文实现先对中文分词，再进 行了拼音的转换，主要使用了 python的phkit库实现拼音的转换和最后到音素的 转换，使得中文能够和英文一样，进行音素的训练，最后产出为梅尔频谱，前文 第三章介绍了模型的详细结构和整个系统的设计结构，这里不再赘述，关键参数 值及含义如下表4-6所示。
表4-6 Tacotron模型主要参数表
参数名
简单解释
encconvnum」ayei*s
编码器的卷积层数，设置为3
encconvkernelsize
编码器各层卷积核大小，均设置为5
encc onvc hannels
编码器卷积层通道数，设置为512
en coder lstm units
编码器lstm神经元数量，设置为256
attention dim
注意力网络维度，设置为128
attentionfilters
注意力网络卷积层数，设置为32
attentionkernel
注意力网络卷积核大小，均设置为31
decoder layers
解码器lstm单元层数，设置为2
dec oder lstm units
解码器每层lstm单元数量，设置为1024
max iters
解码器最大迭代步数，设置为2000
上述参数值的默认值设置参考来自官方实现模型和其他相关中文实现模型， 经本地采用测试确实能达到工程目标。本文训练时租用了矩池云的服务器进行训 练,相关的配置参数为:每秒浮点运算次数为7.29 TFLOPS,显卡内存大小为8 GB, GPU带宽为44&06 GB/s,通道数为16, PCIE带宽为15.75 GB/s,主板型号为 PRIME Z390-P, CPU 型号为 Intel (R) Core (E) i7-8700 CPU @ 3.20GHz,可使 用内核数为6 cores,可使用内存为16 GB,硬盘为INTEL SSDSC2KW25,硬盘 带宽为301.81 MB/s=其中多人语音模型训练时长3天，周星驰迁移模型训练时 长为8小时，本人音频训练时长为8小时。
本文采用了 tensorflow框架来训练模型，梯度下降的方式为adam方法，采 用tensorboard来观察模型损失值的变化，如前文所叙述由于采集大量单人音频 困难，本文采用了语音克隆文章的思路，通过一个特殊的编码器结构加入，训练 一个多人语音的模型，然后基于该模型做特殊语音合成的迁移训练。用于迁移训 练的多语音模型损失函数变化如图4-11。
58
stats/after_lcss
:巧：Tacotfon.fBGSeizssais.- arer_：oss
■ I
-	1	j	，
3	2k	4v	&	2”	嵌	总	迢	承	2&<
图4-11基本多人语音模型损失函数图
该模型采用zhthchs30清华大学开源数据集训练，从图中不难看出从一开始 的快速下降后损失值趋于平稳，到了 20k的训练步数时整个损失值已经几乎不再 下降。
然后为实现特殊语音的合成，本文基于该多人语音模型，使用周星驰的语音 数据进行了迁移训练，损失函数值变动如图4-12o
stats/after
tac： Tacctrwvfnocei/stats.' 3t?er.ioss
图4-12周星驰语音数据训练损失函数图
在迁移训练的过程中可以发现损失函数值有波动的下降趋势，本文在训练的 同时每隔100步长都会生成测试语音和梅尔频谱预测，观察到22.8k步长后模型 实际情况有劣化，出现了测试集的过拟合情况，所以在最后选取了道22.8k步长 为止的模型作为周星驰特殊音色的语音合成模型。在步长为22.8k时对比预测的 梅尔频谱图和实际的梅尔频谱图区别如图4-13o
59
Target Mel-Spectrogram
0	25	50	75	100	125
-5	-4	-3	-2	-1
Tacotron, 2021-01-22 16:39, step=22800, loss=0.14778
图4-13周星驰语音数据预测梅尔频谱对比图
从图中不难看出，端到端迁移模型的梅尔频谱预测图像己经非常接近于真实 的梅尔频谱图像，同时损失值也达到了 0.14778的低点值。
最后是由录音采集的作者本人的音频数据训练的模型，如图4-14所示。
图4-14本文作者语音数据训练损失函数图
对比预测的梅尔频谱图和实际的梅尔频谱图区别如图4-15。
60
Tacotron, 2021-01-24 18:01, step=24900, loss=0.10986
图4-15本文作者语音数据预测梅尔频谱对比图
和周星驰训练的规律类似，使用本人语音进行语音合成器模型训练后，预测 的梅尔频谱图像也与目标图像趋近，损失函数值降到了 0.10986的低值停止训练。 4.3.2声码器模块实现
本文主要采用的声码器模型为MelGAN和WaveFbw,以及基本的信号处理 格林方法，相关模型或算法的结构已经在前文中提到，这里不再赘述，首先训练 集采用了清华大学zhthchs30数据集进行初步的训练，然后分别使用周星驰的语 音数据进行了迁移训练（还是由于采集单人大量音频数据实现难度非常大，不得 不采用迁移训练的方法），因为声码器训练输入为梅尔频谱值，为了跟合成器完 整适配，声码器的目标梅尔频谱生成方法采用和语音合成器相同的办法，两个模 型均采用了相同的训练数据，采样率均为22050,梅尔频谱的通道数量均为80, 其他定制化参数均参考官方论文实验设置参数，而训练服务器的硬件配置也跟之 前语音合成器的配置相同，其中MelGAN模型基本的训练情况如图4-16, WaveFbw模型训练情况如图4-17。
61
meLreconstruction
tag !oss/nwif_jeco;istrucncn
trainiagjoss
图4-17 WaveFfow训练损失值变化图
其中WaveFlow模型训练步数远远高于MelGAN模型才能达到比较好的效果, 训练时长也相对更长。WaveFlow模型由于采用的对数方法作为损失函数，所以 损失值可以为负值，依然是越小越好。
为了选择合适的声码器模型作为最后对接语音合成器的声码器,MelGAN模 型的训练和WaveFlow采用相同的训练数据，以损失值无明显变化且生成语音效 果较好为训练停止。在测试时了相同的测试文本比较三种声码器的各项性能。比 较结果如下表4-7所示。
表4-7声码器模型对比表
模型/算法
训练时长
生成语音时长(cpu)
生成语音时长(gpu)
生成语音质量
格林算法
无
0.403
0.403
好
MelGAN
3天
0.412
0.0345
较好
WaveFlow
7天
0385
0.0189
一般
综合上表所述，传统的语音合成格林算法是能够直接满足生成语音的基本需 求的，但是在高并发要求实时性强的场景中，生成时间相对处于劣势，而神经网 络模型的方法能够满足实时性的要求，即实时地语音对话环境，但是MelGAN
62 在训练单人语音后迁移到生成其他语音进行波形生成时效果明显不好， WaveFlow模型也有这个问题，其实时性相比属于最佳，但训练需要时长更久， 且在语音生成质量上相比于MelGAN和传统算法来说不够好，综合考虑本文在 系统构建上由于考虑到多种音色的切换以及训练的时间成本开销，在特色语音模 型上选用MelGAN模型构建的声码器，而在普通音色的选择上使用格林算法作 为基本的声码器。
