4.1.2Wordembedding应用于关系抽取
Word embedding之所以为学术界和工业界所认可，其最主要的原因就是传 统上处理自然语言时，很难有一种方法能够将自然语言符号转化为数字，从而使 用数学工具进行分析和处理，而词向量做到了。现在的机器学习技术实际上是数 学模型与最优化算法在大量数据上的应用。
在上一节，提到了进一步优化关系抽取的方向，所以借助word embedding 技术，本文就能克服对于以前各种特征提取时自言语言符号难以量化的问题。比 如，传统的自然语言处理技术很难分辨cat和dog的关系，而且，即使使用了 WordNet[40],人们仍然只知道他们都是动物。这种情况还是无法进行量化，而且 cat和dog的关系过于狭隘，显然不能满足自言语言处理的需求。下面给出词向 量如何理解cat和dog这两个单词，见图4-2。通过计算余弦相似度，实验得到 dog和cat的相似度为0.8817。
如果词向量的信息是有效的，那么通过使用词向量增加特征信息，必然能够 提升一定的性能。由于特征提取属于基础工作，所以本文通过加入word embedding对于co-training关系抽取算法做了改进并进行了实验。
画亀蝴災0.2酹231 •如焼。禮	龄 S.5586伸・0.聽5也 ■凱旳就0.2淞的4 •就林3B孔的831
餘 9,网3机晦队29U獭转.。料7询準京25781麝；织33跆76 6*272403	-«.9«3743 132126 9.247474
臥gSGfi ",炒Bl G.229565 -£1.190422 0.1438^$ e.154292 W.734339 ^.109074 «0<276475 9.639774 *01102691 -
G.3&52^$ *0.294^56.0.081592：9.G.629S42 -0.86«833 0.419^29 -0.818011 0.191024 Q.1S1812 0.22M4S 鋤？
337•軌528：160 ：。.397双 臥监44丄8 •矶457W	46249"	•	•	.	•	.
(a)
也询779 連.295鮭《 .flLpeCTM..飢胳864&	:峯務E2 •.皿4G2 .尊・25心9	觐72 ••."12
蘭诲.192977 9.H4S36 -0.29282S 碗蜘 023弟39 -0.497942 9.2228S3 9.161793 -0.878479 0»1«1813 0.163«35 。•勢2m.Y.133a4S 牝gg •或涌264 •就213443 e.31dW ・0.44$498 -0.143CS8 .«.12712t 9.69321B •亀 198973 • &打7炀 •&够娅7 縛.043194 ^3S927B e4^15932 9.1HM7 弟歯7722 &669148 0.39111» 9.9664S7 0.037416 0.4148 检妍9.蘭3每S043W “滴般團* •况59痫14。	・.	：;" 代 泠"£
(b)
图 4-2 (a) dog, (b) cat 的词向量
28
