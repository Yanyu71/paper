4.3.1.2项目.项目关系嵌入模型
现实生活中，用户在进行网上浏览或购物时，总会产生一系列的由项目组成的 会话序列。这些会话序列与单词序列十分类似“7，国，因此可以借鉴词嵌入模型来学 习项目表示219］。与传统词嵌入模型使用的单域数据不同，本文中面向的是来自不 同域中的大量跨域数据。基于计数聚合在并行计算方面的优势，本文将项目-项目关 系嵌入模型等价变换为以计数聚合计算为主的矩阵分解模型以提升模型的训练效率。
给定一个单域会话序列，首先应用项目嵌入来捕捉序列内项目级的依赖关系。 具体地，基于项目集合方=｛叼｝3,给出会话序列印=｛既｝E1，通过最大化基于 skip-gram的目标函数可以得到如下的公式：
1 K
Litem-S = 7 L E 10gP(Wi+jM)	(4-3)
其中，%代表上下文窗口的大小。显然，目标函数的形式与skip-gram word2vec模型 是一致的。
Levy和Goldberg［2。］证明了带有负采样机制的skip-gram word2vec模型实际上等 价于对一个点相关的互信息(Point-wise Mutual Information, PMI)矩阵进行分解。一 个单词i和它的其他单词上下文j之间的PMI被定义为：
""黑瓷	I
71
其中，#(i) =	#(j) = 以及 O =	Levy 和 Goldberg网进一步
j	i	ij
提出通过对平移正定PMI (ShiftedPositive PMI, SPPMD矩阵进行维数约简(如SVD) 来进行词嵌入：
SPPMI(j,j) =	- log/i,0}	(4-5)
其中，力为控制SPPMI矩阵稀疏度的超参数。由于该模型能够很容易训练计数聚合数 据(如三元组)，所以它比skip-gram word2vec模型更适用于应用到大型 数据集中。因此，在提出的新方法中也将遵循相似的思路。
用XW况必表示会话序列的SPPMI矩阵，其中/为序列中项目的个数，J为项目 上下文的个数。根据上下文窗口大小L将会话分成更短的段。Xf/CX可以通过公式 (4-4)和公式(4-5)进行计算。由于贝叶斯个性化排序(Bayesian Personalized Ranking, BPR)模型DU在会话型推荐任务中的出色表现"7,22],本节也将基于BPR模型构建目 标函数。BPR模型是一种基于成对排序损失的矩阵分解方法，成对排序比较了作为 正例的项目和一个抽样的负例项目的得分。然后，通过构造一个最大后验估计来推 导出项目-项目关系嵌入的目标函数：
L'irem = ^。(叫 >i)
= lnp(>i |6)p(e)
=ln n b(如t)p(e)
(ij㈤ e 组	(4-6)
=£ lno(ilA) +lnp(0)
=£ Inb(得%)-%| 网 F
其中，>i是针对项目”设计的潜在偏好结构。名为训练集。矩阵分解的模型参 数为8 =仍,。)，P e员和Q e况"d代表维度为d的项目和上下文的嵌入向量。 xijk = Xij—xik, Xij = pfqj以及xik = pfqk- p. C P和句G Q代表维度为d的项目i和 上下文j的嵌入向量。b = l/(l+er)为sigmoid函数。4为模型的正则化参数。
最后，定义基于源域s和目标域，的会话序列的SPPMI矩阵为xse%，改尸和 基于公式(4-6)中的目标函数，通过最大化如下目标函数以完成对源域s
72
和目标域/的项目-项目关系嵌入：
L"em = E 1nb（专升）+ E Ino•（纵）—41 网 |2	（4-7）
其中，绥和”为针对源域s和目标域t的训练语料，即为针对e =（尸，尸,e,。） 的正则化参数。
