5.2跨模态注意力融合机制
近年来视频描述任务，可以看作是一种机器翻译任务的变种，输入是一系列 在视频数据中抽取的特征序列，输出结果是一段自然语言，过去大量的工作都是 基于视觉信息，但是大多数的视频数据都包含多种模态的信息，比如图像,音频， 文本。特别是，视频中主体产生音频信息，可能对内容产生重要的影响。比如， 当有人从另一边敲门时，视觉信息中只看到门，但音频信息可以帮助本文理解有 人在门后面，想要打开它进入。因此，单模态的信息的作用十分有限。此外，部
49
分类型的视频（如教学视频、体育视频或视频讲座）仅仅使用单模态对于视频描 述任务是具有挑战性。因此，本文考虑利用视频中的视觉信息和听觉信息，构建 出一种跨模态的自注意力机制模型进行更为有效的视频描述任务。
跨模态自注意力机制的编码器部分如下图5-1所示，共用3种模态的输入， 分别是经过位置编码的图像特征厂，动作特征JM,音频特征1/人，这些特征将会 经过N个encoder部分,encoder中包含三个部分，自注意力模块selfattention[53], 跨模态注意力模块cross-modal attention,以及前馈神经网络韶分。首先，各个模 态的特征会进行位置编码，各自进入各自模态的多头注意力机制模块，这里的多 头注意力MultiHeadAttention模块和原始tansformer的多头注意力模块形式一- 致，然后经过残差连接的结果进行拆分，一部分进入跨模态注意力机制，每个模
态的操作均是一致，最后在进入前馈神经网络部分。
Vfeif = MultiHeadAttention(V!, V1, V,y)	(5-1)
氏疔=MultiHeadAttentlon(VM, VM, VM)	(5-2)
黑if = MultiHeadAttention(yA, VA, VA)	(5-3)
Vcross = MultiHeadAttention(V'elf, V^,	(5-4)
V#oss = MultiH eadAttention(V^,	(5-5)
= MultiHeadAttention(y^f 如咯)	(5-6)
V}c = PC^ross)礙(喘 ss)	VA = FC(V40SS) (5-7) 在decoder部分，由encoder部分输出的特征序列和词向量来到decoder中的 进行cross modal attention的操作，三个部分输出的特征序列进行拼接降维，完成 decoder部分的操作，最后经过全连接层和softmax层进行最终结果的输出。
图5-2.跨模态注意力机制解码器decoder部分
