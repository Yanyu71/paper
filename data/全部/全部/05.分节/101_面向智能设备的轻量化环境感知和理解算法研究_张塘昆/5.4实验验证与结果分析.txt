5.4实验验证与结果分析
ActivityNet Captions dataset®],被认为是视频描述任务的重要数据集。不同 于传统的视频描述任务，该数据集用于密集视频描述任务，给予一段长视频，需 要模型去定位事件的发生并且进行视频片段描述，该数据集包含大约2万个来自 YouTube的视频，并分成50/25/25%的部分，分别用于训练、验证和测试。每个 视频平均包含3.65个视频片段的描述描述，每个大约13.65个单词。此外，验证 集中的每个视频都由不同的标注者进行了两次注释。本文使用验证集进行模型性 能的验证。大多数的视频包含众多活动。例如，在“人弹钢琴”的视频中，视频可 能还会包含另一个“人跳舞”或“人群鼓掌”。
本文中，本文仅使用该数据集进行视频描述任务，不进行事件检测任务，在 进行多模态融合模型的训练时，为了验证融合模型的有效性，需要保证特征的泛 化性和鲁棒性。为了进行更加公平的比较，本文使用的经过预训练的模型进行基 础特征的提取，本文使用的由M-SOSAnet提取的图像特征，动作特征上，本文 使用在kinetic上进行预训练I3D模型，音频特征上，使用在AudioSet ±进行预 训练的VGGish模型。在衡量视频描述性能的时候，本文使用常见的指标 METEOR, BLEU@3, BLEU@4。
训练细节如下：训练时，使用batchsize为64,采样正则化操作，本文使用 0.5的dropout和0.0005的权重衰减，使用Adam优化算法,初始学习率为0.005。 首先，由于不同的模态的特征大小不一致，为了解决这个问题，将各种特征进行 填充，以匹配特征大小最大的特征的维度。同时，由于英语单词可能存在多个同 义词或者标注存在错误，使用label smooth的数据增强方法缓解这种问题。当一
54 次输入整个特征序列，需要去预测下一个位置的单词，由于基于自注意力机制的 结构关注整体，为了防止在训练神经网络过程中从算法模型能够从整体看到下个 位置的信息，在解码器的t之后的位置进行掩码mask的操作。在视频描述生成 的训练中，为了形成一个批次，所有的特征被填充到同一批次中最长的序列。 Decoder部分采用1层解码器。
表5-1跨模态型视频描述算法性能对比
本文对比了仅仅使用自注意力机制和联合使用自注意力机制和跨模态注意 力机制的影响，证实了使用跨模态注意力机制的有效性。将跨模态注意力层替换 为自注意力层对比实验结果如下表5-2所示。
表5-2是否使用跨模态注意力层性能对比
BLEU@3
BLEU@4
METEOR
self-attention
self-attention+cross modal attention
本文还将对各种融合模型在不同设备上的效率进行对比，各个模型的基础特 征均相似，以下的效率对比，均指多模态融合模型的效率，不包含前置模型特征 提取的时间，具体如下表5-3所示。
表5-3.模型的实际效率对比
GTX3090ti
(Pytorch FP32)
Iashin [刃
Avg 90ms
Ours
Avg 50ms
由以上的图表可知，本文的跨模态自注意力机制结构的编码器-解码器的实验 结果在视频描述上取得了不错的性能，参数量要比原始的Transformer方法或者 是RNN方法减少一倍左右。更加有利于在智能设备的上的部署和使用，在效率 方面，也能够快于传统的RNN结构和Transformer结构。
