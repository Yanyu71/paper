4.2研究关键点
•利用知识蒸憾的方法辅助声音场景分类进行训练
声音场景分类是音频模式识别任务中重要的一项任务，目标判断给出的一段 音频中的片段是否含有分类体系的音频标识。早期的音频分类算法使用人工设计 的特征作为输入，例如音频的能量，mfcc音频特征，过零率等。近年来，随着深 度学习的技术快速发展，尤其是卷积神经网络已经在计算机视觉领域取得了巨大 的突破。同时，在音频领域，过去的使用卷积神经网络CNN进行音频分类任务 分方法也使得精度性能取得了巨大的突破。在使用CNN进行音频分类的时候， 神经网络的输入往往使用log mel频谱特征，计算过程如下所述，使用短时傅立 叶变换去计算音频波形的频谱，再使用mel滤波器提取，最后取对数，便得到了 log mel 特征。
AudioSet是一个大规模的音频数据集，拥有528个类别，该数据集的音频片 段是从YouTube的视频片段中提取而来的。其中，训练集包括2063839个音频 片段，每个类别至少拥有50个音频片段，评估集拥有20371个音频片段，总大 小高达1.1T量级。同时，AudioSet数据集存在大量的数据类别不均衡的问题， 其中对话和音乐大类占比极高，如果不对数据进行采样平衡，将会造成在样本数 43
量大的类别的过拟合，和样本数量少的类别的欠拟合，许多的研究，例如有的研 究尝试了对训练数据进行balanced sampling,确保在每个mini-batch里面样本类 别分布平衡，因此在AudioSet数据集上进行调参的成本是巨大的。
UrbansoundSK是目前使用十分广泛的环境音频分类模型，共包含有8732条 标注的数据,共分为10个类别，数据采集来自城市的各种环境音，如空调声， 汽车轰鸣，儿童玩耍，犬吠等，是较为理想的验证算法模型迁移学习能力的数据 集⑷】。在实验中，本文将10%的数据划分为验证集，剩余的90%的数据用于训 练使用。在从头训练UrbanSound8K数据集时，本文使用Adam优化算法,batchsize 为64,初始学习率为0.01,训练8K次迭代。在使用AudioSet数据集的预训练 模型的情况下，本文将学习率调小为0.0001,只进行4K次的迭代。对于提取的 音频，采样率为32K,由于UrbanSound8K数据集中的大部分音频为短音频，因 而本文取每段音频的5s进行log mel特征的提取，超过5s的音频进行截取，少 于5s的音频进行零填充。
由于AudioSet数据集以上的特性，常规地从头初始化神经网络训练AudioSet 数据集的成本往往是很高的，而且若是考虑计算资源的问题，构建一个面向低算 力智能设备的神经网络，进行从头训练，将会造成大量的时间成本的浪费，而且 精度无法保证。但是缺少了 AudioSet模型的预训练，音频分类算法的精度将会 产生很大的衰退。本文使用现有存在的基于AudioSet的预训练的CNN模型，在 UrbanSound8K数据集进行微调，同时对比了不使用AudioSet预训练模型直接对 Urbansound8K进行训练，对比如下图4-1所示。由图中可知，在使用了基于 AudioSet数据集进行预训练之后，在小规模的数据集上进行微调将会有完全领先 的性能，规模较大的CNN,由于没有进行预训练，最终可能会造成过拟合现象， 验证集上面的精度下降。综上所述，说明使用AudioSet数据集预训练对于音频 分类算法必要性。
图使用大规模数据集预训练和未使用预训练在中规模数扌居集上进行训练的精度对比
表使用AudioSet数据集预训练和不使用AudioSet数据集预训练网络各项指标
without AudioSet pretrained
AudioSet pretrained Acc
计算量
Flops
参数量
Params
DaiNet
mobilenetv2　　本文为了解决以上问题,利用知识蒸憾KD的方法，巧妙地避开了对AudioSet 数据集的复杂训练过程，保证在音频分类模型在有一定的AudioSet数据集的预 训练模型的先验背景下，定制各类迁移学习的任务。整体的方法流程如下图4-2, 图4-3所示。
Transfer Learning
基于AudioSet数据集训练的模型	使用AudioSet预训练模型，在UrbanSound8K上进行finetune
图4・2.使用已经在AudioSet数据集预训练的模型在UrbanSound8K _h进行微调finetune
Knowledge Distill
在 UrbanSound8K 上进行 finetune 完成的 Teacher 模型	Student 模型
图使用已经在AudioSet数据集预训练的模型在UrbanSound8K上进行微调finetune
知识蒸憾KD目标是将将复杂或者更优的教师模型所学到参数分布，去让学
生模型进行适配学习，通常认为教师模型具有更大的容量和更好的性能，可以
容量更小，复杂度更低的学生模型提高一种类似于soft target的目标去学习。一 般情况下，在进行知识蒸憾的时候，为了得到更好的学生模型，在进行学生模型 的训练时候，一般保障学生模型的输出和数据的真是标签一致，也要保证学生模 型在最后softmax之前的输出logits,和教师模型的中相对应的logits尽量的逼 近，一般在训练学生模型的时候,loss的设计至少包含有两项，一般形式是LOSS = CE(y, p) + XCE(o(logitsstudent, T), (logitsteacher, T)) , CE 表示分类任务常用的 交叉爛函数，y, p分别为学生模型的输出和数据真实的标签，。代表softmax函 数，T是人为设计的temperature参数，用于平滑数值，入用于协调两项loss值 之间的权重比。
传统的知识蒸馄只关注了单个样本在教师模型和学生模型输出的特征的相 关性，可以认为这种相关性是低阶的，本文所使用的知识蒸憾的方法，在关注教 师模型和学生模型的输出的基础上，将会构建岀样本之间的相似性，会用cosine 函数进行度量，最后将其规范到一个概率上面。知识蒸憾整体流程如下图4-4所 示［49】。
这种方法的是为了能够让学生模型学习到教师模型的概率分布，这比仅建立 两个模型输出之间的关系更加具有鲁棒性，在训练过程中，对于每个batchsize的 数据样本之间成对的交互进行建模，使其可以学到描述高级特征空间的能力，利 用特征空间中任意两个数据点的联合概率密度，对两个数据点之间的距离进行概 率分布建模。通过最小化教师模型与学生模型的联合密度概率估计的差异，实现 概率分布学习。联合概率密度公式如下。
训练中每个batch的数据都是随机抽样的，无法使用全局的数据，因此需要 使用条件概率密度代替联合概率密度的分布。在计算了每个batch数据的条件概 率密度之后，通过最小化教师模型的条件概率分布和学生模型的条件概率分布的 KL散度，实现概率上的知识蒸憎。使得学生模型可以学习到这类更高阶的相关 性⑷］。KL散度的公式如下所示。
图4-4.知识蒸憎方法细节图[49]
