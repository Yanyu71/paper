2.2.2内容维度的设计与优化
在目标的内容维度上，小目标检测的难度不仅来自于自身数量的稀少、在数 据集中所占比例稀少，而且还与训练过程中的极度不平衡密切相关。目标检测任 务集分类与定位于一体，所以不平衡包含了类别不平衡和空间不平衡。本节主要 介绍解决分类不平衡的Focal Loss卩2〕和解决空间不平衡的MateAnchor[33]：
(1)在解决类别不平衡的问题上，以Focal Loss为主要代表。类别不平衡 主要包括检测目标中正负样本不平衡、训练过程中简单样本和困难样本数量不平 衡。
对于简单困难样本不平衡的问题，在线难分样本挖掘(Online Hard Example Mining, OHEM)印】进行了最早的尝试，它使用模型的输出概率，选出部分困 难样本，然后根据这些样本的权重，更新网络参数，在一定程度上缓解了样本不 平衡造成的问题。OHEM算法虽然增加了困难样本的权重，但是忽略了简单样 本。而何凯明团队提出的Focal Loss是现阶段解决正负样本、简单困难样本不平 衡的主流方法。
何凯明团队提出一种新的损失函数 Focal Loss,这个损失函数是在标准 交叉矯损失基础上修改得到的，可以通过减少简单样本的权重，使得模型在训练 时更专注于困难样本。
对于二分类来说，标准的交叉燔(crossentropy, CE)损失函数如公式(2-2)
所示:
这里P*是anchor的标签，取值为1或者0,分别代表正负样本，p是预测其 输出为1的概率，取值范围为0〜1。当p* = l时，损失L = -log(p),厶与预 测输出的关系如图2-8左图所示：很显然，对于正样本的预测，预测输出越接近 真实样本标签p* = l，损失函数Z越小；预测输出越接近0,厶越大。
14
图2-8 L与p的关系图
而当p* = 0时：损失厶=-2og(l-p),厶与预测输出的关系如上右图：同 样，预测输出越接近真实样本标签0 (p值越小)，损失函数厶越小；预测输出越 接近1,厶越大。函数的变化趋势也完全符合实际需要的情况。
为了简化交叉矯损失，用氏代替p得到公式(2-3)：
(p ifp* = 1
Pt — (1 — p otherwise	(-丿
便可知CE(p, p*) =	=-Zog(pt)。既然在训练的时候正负样本的数量
差距很大，一种常见的做法就是给正负样本加上权重，负样本出现的频次多，那 么就降低负样本的权重，正样本数量少，就相对提高正样本的权重。具体调整如 下：
CE(p』=-at Iog(pt)	(2-4)
at的定义与pt类似，当p* = 1时，at = a；当p* = 0时，CQ = 1 - a, a的 范围也是0到1。因此可以通过设定a的值来控制正负样本对总的损失的共享 权重。当a = 0.5时退化为标准交叉矯。
公式(2-4)虽然可以控制正负样本的权重，但是没法控制简单样本和困难样 本的权重。类似于添加权重因子a的思想，Focal Loss提出调制系数(1 - pt)Y来 处理简单、困难样本的问题：
FL(p』=-(1 - Pt)Y log(pt)	(2-5)
这里的y称作聚焦参数(focusing parameter), y > 0o当一个样本被分错的 时候，Pt是很小的(结合公式(2-3),当p* = 1时，p < 0.5才是错分类，此时,pt 就比较小，反之当p* = 0时，p>0.5是错分)，因此调制系数就趋于1。当s趋 于1时(此时分类正确而且是简单样本)，调制系数趋于0,也就是对于总的损 失的贡献很小。当y = 0时，Focal Loss就是传统的交叉爛损失，当y增加的时 候，调制系数也会增加。Focal Loss的核心就是用一个合适的函数去度量困难样 本和简单样本对总的损失的贡献。
(2)在解决空间不平衡的问题上，以M^eAnchor为主要代表。从2.1节的
15 介绍可以知道，现阶段不管两步检测还是一步检测，都应用了 anchor机制，现阶 段anchor机制主要的问题有以下两个：1) anchor需要特别的设计，并且无法覆 盖特殊情况，如果设计得不好，会降低最后的检测效果，尤其是小目标检测的效 果；2)为了保持较高的召回率，需要大量的anchor,而其中大多数都是负样本， 又会出现前后背景及其不平衡的问题，影响小目标的检测精度。
Anchor的设计与优化同样能够增强小目标的检测效果，其中最具代表的是 MetaAnchoro Anchor将框空间(包括位置，大小，类等)划分为离散区间，并通 过相应区间中定义的anchor函数生成每个对象框。X表示从输入图像提取的特 征，那么，第i个区域的anchor公式(2-6):
珂(X；E)=(呛 s(x；o 严)，矚 eg(x；e「g))	(2-6)
其中bi G B (即先验框，在之前的工作中可以理解为FasterR-CNN原论文中 固定尺寸的9个anchor),同时，沪：：(•)判别是否存在一个目标框与第i个区域 相关联，笛了(•)则把目标框(如果有)的相对位置回归到先验S； $表示与 anchor相关的参数。
而MetaAnchor函数由6动态生成，而不是通过枚举每一个可能的边界框。 其在分别建模相应的anchor函数时，可写为公式(2-7)：
咒产 ggw)	(2-7)
其中，§(•)称之为anchor函数生成器，它把任一边界框b：映射到相应的 anchor函数几：。“•)函数建模学习anchor的尺寸，w为网络学习的参数。虽然 6仍需要先验知识提前设定，但起码初始化的anchor尺寸不再是一成不变的， 并且这样生产的anchor具有多样性。在MetaAnchor框架中，如公式(2-7)所 示，anchor函数生成器把S映射到相应的anchor函数，从而扮演了关键角色。 为利用神经网络建模，首先要假设对于不同的0, anchor函数共享同一方程，但 是参数不同，这意味着公式(2-8)：
珂=F(X； 0b.)	(2-8)
接着，由于每一个anchor函数的区别仅在于其参数，生成器由此可预测，如 公式(2-9)所示：
。如=w) = 0* + 7?(bf; w)	(2-9)
其中0*代表共享参数(独立于0且同样可学习)，残差项欠(0; w)取决于先 验通过一个简单的二层神经网络实现：
欠(0； w) = W2a(W1bi)	(2-10)
这里，必和购皆是可学习参数，"(•)是激活函数,隐藏神经元的数量用m
16
表示，实际上m通常远小于0乩的维数，这导致预测的权重聚集在一个明显低阶 的子空间，这就是为什么在公式（2-9）中把方程化为一个残差项，而不是直接使 用。为应用MetaAnchor,需要重新设计原始的anchor函数，保证其参数生成于 自定义6。首先要考虑如何编码S，—个包含位置、尺寸、分类等信息的向量。 在实验中，6主要于anchor尺寸相关，并表示为：
ahs awt	、
6 =盹丽」昭丽）	Q-ll）
其中a加和aw：是相应anchor的高和宽，AH, AW是作为正则化项的“标准 锚点框"的尺寸。至此，MetaAnchor,其anchor函数可由任意自定义的先验框动 态生成。加上权重预测，MetaAnchor可与大多数基于anchor的目标检测系统协 同工作，相较于预定义anchor方法，MetaAnchor对于anchor设置和边界框分布 更为鲁棒，并在迁移任务上变现出潜力。
