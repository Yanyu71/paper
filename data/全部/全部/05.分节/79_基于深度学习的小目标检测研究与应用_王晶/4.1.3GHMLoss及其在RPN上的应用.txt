4.1.3GHMLoss及其在RPN上的应用
在2.2.2节中提到的Focal Loss通过大大降低简单样本的分类损失来平衡正 负样本，但是其设计的Loss引入了两个需要通过大量实验来调整的超参数a和 Y，不能直接得到最优方案。
本节基于4.1.1的分析，模型训练的过程是一个在梯度优化的过程，如果梯 度优化的过程被简单样本过度影响，会极大的降低模型的整体性能。这些简单样 本的单个loss虽然不高，但由于数量众多，最终合起来会对总loss有很大的贡 献，从而导致梯度优化的时候过度关注这些简单样本，这样会使模型收敛到一个 不够好的结果。李教授团队卩0]在针对一步检测算法中简单、困难样本分布极度不 平衡问题，基于梯度优化提出了 GHM Losso接下来具体介绍GHMLoss。
对于一个样本，如果它能很容易地被正确分类，那么这个样本对模型来说就 是一个简单样本，模型很难从这个样本中得到更多的信息，从梯度的角度来说,
39
这个样本产生的梯度幅值相对较小。而对于一个错分的样本来说，它产生的梯度 信息则会更丰富，更能指导模型优化的方向。
如2.2.2节中提到，传统的分类损失通常采用cross-entropy (CE),即
-log(p) p* = 1
-log(l -p) p* = 0
其中P为模型预测的分类概率，pe[0,l]； p*为实际标签，p*e{0,l}o如果
用"表示模型的输出，则有：
图4-4样本梯度模长的分布统计图
于是某个样本的g值大小就可以表现这个样本是简单样本还是困难样本。从 一个收敛的检测模型中统计样本梯度的分布情况如图4-4所示。从图中可以看岀， 简单样本的数量要远远大于困难样本。也可以发现在g接近1的时候，样本比例 也相对较大，这里认为这是一些离群样本(outlier),可能是由于数据标注本身不 够准确或是样本比较特殊极难学习而造成的。对一个己收敛的模型来说，强行学 好这些离群样本可能会导致模型参数的较大偏差，反而会影响大多数已经可以较
40
好识别的样本的判断准确率。
基于以上现象与分析，李团队提出了梯度均衡机制，即根据样本梯度模长分 布的比例，进行一个相应的标准化，使得各种类型的样本对模型参数更新有更均 衡的贡献，进而让模型训练更加高效可靠。由于梯度均衡本质上是对不同样本产 生的梯度进行一个加权，进而改变它们的贡献量，而这个权重加在损失函数上也 可以达到同样的效果，此研究中，梯度均衡机制便是通过重构损失函数来实现的。 为了清楚地描述GHM损失函数，需要先定义梯度密度(gradient density)这一概 念。仿照物理上对于密度的定义(单位体积内的质量)，李团队把梯度密度定义 为单位取值区域内分布的样本数量。具体来说，将梯度模长的取值范围划分为若 干个单位区域(unitregion)。对于一个样本，若它的梯度模长为g,它的密度就 定义为处于它所在的单位区域内的样本数量除以这个单位区域的长度：
g)	，- c、
(g) = —Ug)—	()
其中N为每个batch中的样本总量，gk表示第k个样本的梯度模长，而且 有：
5£(gfc,g) = I1 g-|-gfc-g + f	(4-10)
0 otherwise
le(g) = min (g + |,l) -max(g-|,0)	(4-11)
所以梯度密度函数GD(g)就表示梯度落在区域［g - £/2, g + 8/2］的样本数量o 最后定义梯度密度协调参数(gradient density harmonizing parameter) p：
N
pi=歸	⑷⑵
其中N为样本的总数目。最后把GHM的思想应用于分类损失上，得到
GHM Loss：
LGHM = Pi^CE(Pi>Pi)= 丫舊'吧"	(4-⑶
一般模型训练初期会倾向于让模型对于各类的预测是相同概率的，但是对于 样本极度不平衡问题，这其实是非常不利的，因为负样本的损失会很大。所以在 将GHMLoss应用到RPN网络中时，本节还分析了最优初始化设置。
假定数据集中共有C个类别，样本量为M，其中正样本为弘。模型刚开始 对所有样本的预测概率值都是p，,根据厶防损失，那么对于所有正样本的损失和 为-Np ■ log (pz)>所有负样本的损失和为—QNSC — - log (1 — pA)o最后的厶CE 损失为正负损失和与心的比值，整理可得：
最优的初始化设置应该是要使整体损失最低，可以对公式（4-14）求导数来
确定p':
很容易计算出最优p'=Np/NsC,在模型训练之初，CT（wx + b） a CT（b） = p'=
Np/MC。那么可以计算出bias的最优初始化值为:
最后即可将GHMLoss应用于进行了最优初始化设置的RPN网络。综合清
洁度概念，优化后的RPN网络算法流程可展示为表4-2:
表4-2 RPN综合优化算法流程
输入：	GT、B、cZsq locjry a> N
It输入图像；GT:输入图像I中的ground truth; B： anchor集
clsjCz分类置信度；loc_Tz anchor的回归精度；a：调节因子，取0.5
N：正样本集的大小，取100
输出：分类和回归的损失：厶心和厶创
1： Apos = 0、Aneg = 0、S= 0、最优初始化
2: for gt E GT do
3:	indices = arg sort (I OU (B, gt))	■降序排列
4： Apos = Ap0SU{indices[Q:N]:gt}
5: end for
6: Aneg = {(B - Apos). indices: 0}
7: for b( G Apos do
8:	c= a • locr + (1 — a) ■ clsc
9:	S=SU{®:e}
10: end for
11: for bi G ^neg do S= SU{d： 0} end for
12: Lcls = LGHM = Pi^cE(Pi>€d =	'監:；?
13: Lreg =	smooth Ll
改进后的Faster R-CNN端到端结构如图4-5所示，后文通过实验证实了综 合优化的RPN训练有效性。
42
图4-5改进后的Faster R-CNN端到端结构图
