5.4PCA主成分分析算法
PCA(Principle Components Analysis)主成分分析算法是一种十分常见的线性 几何和统计学技术，常被用于神经学、人脸识别和图像压缩等应用领域，也是一 种从混乱的高维空间数据集合中寻找数据呈现模式和相关性的有效手段。
在空间中定义不同的坐标系来映射同一组数据，则这组数据在不同坐标系上 呈现出的分布模式、方差、均值等不都相同。通常情况下在信息处理的过程中存 在两个主要问题，一个是•信息中的噪声，这点通常釆用信噪比(SM?, Signal-to-Noise Ratio)来衡量。
SNR=^^	(5-9)
其中,明时。表示信号的方差，嘔函表示噪声的方差，的值越大，说明数 据越纯净，噪声越少，相反，海的值越小，说明数据中包含的噪声越大，越不 利于后续对数据的使用，应该尽快去噪。另一个是数据中的冗余，就是指数据中 包含的多余的信息，通常可以用协方差矩阵表示。解决这两个问题才能获得具有
39
更高利用价值的数据。由于许多数据集在收集特征的时候并不清楚哪些是需要的 特征而哪些是多余的噪声干扰或可有可无的特征，因此，通过坐标系的转换有助 于在一定程度上区分各个特征的重要性。主成分分析算法就是在此想法的基础上 进一步提取主要成分。
PCA理论基于一个直线性的强假设，从而原始数据被限制于两点：第一是 原始数据的映射只可能基于某几种可能数据基底；其二是数据呈连续性分布。虽 然真实情况下数据集合之间的关系大多并不呈现线性相关，但在大量数据集的情 况下，用线性情况可以作为非线性一个良好的近似。因此，PCA在表述的过程 中将数据看做在极坐标系下的线性组合。另外，PCA同样假设方差较大的数据 特征较其他特征重要，同时，均值和方差足以衡量的数据分布和特征的分布。
假设数据集X为MxiV维矩阵，X的自协方差矩阵可以表示为，
N	_	_
£(不_乂)(不_入)「
var(X)= 	—	 (5-10)
N-\
PX^Y	(5-11)
其中，维线性变换矩阵，丫为数据集X经过P线性变换后的MXJV维矩 阵，也可以代表原数据集在新数据坐标系下的投影矩阵或原数据集经过数据集拉 伸、压缩、旋转后的矩阵。
假设X的每一列为一组数据，用工况-表示，则不同列之间的协方差 表示为
<jy ={x.Xj)	(5-12)
则員自身的协方差矩阵可以表示为
1 t
Sx =——XXT	(5-13)
n-1
在公第i行第j列的元素环场表示澀第i组数据与第j组数据的协方差，为零表示这 两组数据线性不相关。&是维度为MxM维对称矩阵，其对角线元素表示每组数 据的方差。该矩阵表示了中所有数据对之间的线性相关性。假设$¥=六叱表示
1 数据集犬经过变换后的新数据集V,则V的数据特征应尽量满足出对角线之外矩阵 &的其他值趋向于零，即对角化同时会对角线上的元素需从大到小排列。
根据上述条件，新数据集y的协方差矩阵与可以表示为
SY =-~YYT =-^(PX)[PX>f =-^P[XXT^PT =-^PAPt (5-14) 其中，对称矩阵德=刀筋，因此，矩阵A可表示为幽玲，其中D为对角矩阵， 嚣是诚的特征向量组成的矩阵。令E = PL则
40
SY =—P(PtDP\Pt	(5-15)
n-\ v 7
又甲为正交矩阵，因此广，=尹，上式可改写为
SY =—P(p-lDP}P~l =—D	(5-16)
n-\ ' f n-1
PCA的计算过程总结概括如图5-1所示：
离线训练阶段：
初始化样本训练数据集T,转换为矩阵X,具体步骤为：
在线定位阶段：
1.将移动端收集到		2.用存储的矩阵P将X
的数据集转换成矩	—	变换为新数据集Y输入
阵X		定位模型
图5-1室内定位主成分分析法步骤
另一方面，PCA存在一些限制条件：
1.PCA对于数据呈线性相关的强假设并不符合大部分的数据分布特性，因 此，使用PCA的到的新的数据分布只能在一定程度上近似达到去噪和 去除冗余的目的；
2.PCA假设使用均值和方差这两种统计学特征足以表示数据的概率分布， 只有高斯分布满足这个条件，因此，其他的分布都会弱化PCA的结果。 也正是在这个号虽假设条件下，信噪比及协方差矩阵才能完全表示数据的 噪声和冗余。但是根据中心极限定理，在数据量充分的情况下，大部分 数据分布都呈现高斯分布特性。
PCA的另一假设条件是在协方差矩阵中，大的方差值对应重要的特征。 数据特征的主成分之间相互正交。这种假设使得PCA的求解过程可以依赖
41
线性几何的降解方法（SVD分解）。
