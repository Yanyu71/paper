6.3实验结果及结论
63.1实验结果
论文第四章第五章所提到的算法均在实际环境中进行了仿真。其中，表6-1
给出了使用特征选择算法之前各个定位算法的定位精度、模型训练时间及定位时
延的比较结果，表6-2是使用PCA降维到80维特征之后各个算法的比较，表&3
给出使用LDA算法进行特征选择降维到29维之后的算法性能比较结果，表6-4
为交叉验证阶段针对AdaBoost算法中单个树的深度对算法精度的影响，表6-5、
6-6为AdaBoost算法中弱分类器数量对算法精度的影响，表6-7及6-8分别给出
表6-1教3楼639室内使用特征选择算法之前各算法之间的精度及耗时比较
Method	　　　Accuracy(%)			　　Time(s)
<=3m	<=5m	　　<=10m	　　Train time	　　Predict time
NN	　　　32.9700	85.4223	　　99.8463 .	　　0.5550	　　0.4646
4NN	　　　34.9677	82.9677	　　99.6129	　'0.5547	　　0.4374
W5NN	　　　37.7688	85.6183	　　99.1935	　　0.4372	　　0.4587
Naive Bayes	　　　35.1572	70.5882	　　96.7168	　　1.7452	　　0.2178
SVC with linear	　　　36.6083	79.0040	　　98.9233	　　2.2831	　　0.1004
kernel
Linear SVC	　　　29.4872	66.6667	　　83.3333	　　25.5390	　　0.0015
SVC with	　　　23.9428	62.6984	　　87.8307	　　74.6986	　　0.5288
rbf kernel
SVC with	　　　39.3768	65.9211	　　99.7368	　　2.3069	　　0.1013
poly kernel
Decision	　　　25.9722	65.0	　　82.2222	　　0.6513	　　0.0071
Cart Tree
Decision	　　　28.6863	69.5710	　　90.6166	　　0.6453	　　0.0047
ID3 Tree
AdaBoost(
100 layers )	　　　42.06	78.29	　　99.59	　　57.8773	　　0.3173
48
K近邻算法和加权K近邻算法中不同K值对定位精度的影响，表6-9表示特征 压缩维度对不同核函数的支持向量机的影响。
从表6-1中对不同定位算法的不同性能比较可以观察到在不使用特征选择算 法之前K近邻算法对测试数据的预测精确度最高，线性SVM算法的定位精确度 次之。然而，在实际预测时，SVM算法的预测时延远远短于K近邻算法，若单 就预测时间进行比较，线性的SVM算法的预测效率仅次于效率最高的决策树算 法，有时，SVM的预测效率甚至会高于决策树算法。若比较训练时间，K近邻 算法的时间却好于SVM算法。因ikE「藤了精确度的比较，实际在线情况下的程 表6-2 教3楼639室内使用PCA特征选择算法各算法之间的精度及耗时比较
Method	　　　Accuracyf%)			　　　Ume(s)
(with PCA,	　　　<=3m	<=5m	　　<=10m	　　Train time	　　Predict time
shrinking to 80
dimensions)
NN	　　　30.9091	77.4026	　　93.1169	　　　0.05087	　　0.004191
4NN	•35.3180	71.9892	　　94.1813	　　　0.0409	　　0.002380
W5NN	　　　42.4204	79.6178	　　94.9045	　　　0.04142	　　0.002514
Naive Bayes	　　　27.2260	56.3871	　　79.4839	0.0279	　　　0.002122
SVC with	　　　24.1565	65.0472	　　97.0310	　　　1.8276	　　0.000586
linear kernel
Linear SVC	　　　31.5519	70.7953	　　93.6115	　　　2.5199	　　0.00089
SVC with	　　　23.2819	63.6746	　　96.4937	　　　4.4858	　　0.001224
rbf kernel
SVC with	40.6210	63.6481	　　92.3674	　　　5.8540	　　0.001203
poly kernel
Decision	40.0826	72.4518	　　95.1791	1.0928	　　0.000078
Cart Tree
Decision	46.0154	83.5476	　　98.2005	　　　6.2524	　　0.000058
ID3 Tree
AdaBoost(
100 layers )	48.1431	90.9216	　　99.8624	　　　50.9625	　　0.002166
序预测时间同样也是评估一个室内定位算法的重要方面。当很多使用者同时接入 定位服务器网络时，网络需要有控制并发状况的能力，而预测时间相对较短的算
49
法更有利于实际使用。
表6-3教3楼639室内使用LDA特征选择算法客算法之间的精度及耗时比较
Method (with
LDA, shrinking to
80 dimensions)	　　　Accuracy(%)			　　Time(s)
<=3m	<=5m	　　<=10m	　　Train time	　　Predict time
NN	　　　24.9686	70.0125	　　96.1104	　　0.01674	　　0.00523
4NN	　　　22.3419	72.5437	　　99.8654	　　0.01662	　　0.001559
W5NN	　　　16.0996	70.4188	　　99.8691	　　0.01645	　　0.000643
Naive Bayes	　　　18.7331	74.3935	　　94.3396	　　0.01139	　　0.001347
SVC with	　　　27.2487	69.5767	　　96.8254	　　0.3219	　　0,000235
linear kernel
Linear SVC	　　　21.6438	56.3014	　　86.9863	　　83.4667	　　0.000150
SVC with rbf	　　　20.3779	60.4588	　　78.6774	　　14.2821	　　0.001864
kernel
SVC with	　　　24.4696	75.1061	　　98.0198	　　0.2696	　　0.000564
poly kernel
Decision Cart	　　　25.2161	61.9597	　　93.0836	　　0.6136	　　0.000229
Tree
Decision ID3	　　　12.3944	58.5732	　　96.1972	　　2.0149	　　0.000235
Tree
AdaBoost(
100 layers)	　　　21.0174	67.7376	　　98.6613	　　46.8160	　　0.002164
通过表6-2和表6-3结果比较，结论显示经过特征选择过程处理之后的数据
除了缩短模型的训练时间外,	也可以增强模型的鲁棒性和泛化能力，降低模型的
复杂度。
走廊的仿真情况较室内环境好很多，主要原因在于走廊的新到环境没有室内 有多种设施人流的干扰，新到环境较为稳定。
表6-4 AdaBoost算法决策树深度对定位精度的影响比较(%)
算法	2	4	　　　　6	　　8
无压缩	21.04	55.66	　　　92.11	96.42
Pca(50%)	44.49	79.39	　　　94.95	99.42
Ida	37.21	83.71	　　　94.07	97.80
50
从表6-4可以看出，随着单个二叉决策树深度的增加，无论是经过特征选择 算法还是未经过特征选择筛选的算法定位精度都呈现提高的趋势，但是当单个决 策树的深度进一步增加时，有于决策树本身的特点，训练时间更长，模型更加复 杂，更容易出现过拟合。因此，单一决策树的深度需要实验折衷选择。
表6-5深度为8时提升算法弱分类器个数对定位精度的影响(％)
^^stimator
法	X.	10	15	20	25	30	35	40	45
无压缩	　96.86	　98.04	98.89	99.29	　99.29	9934	　99.50	　99.72
Pca(50%)	　98.30	99.01	98.89	99.13	　99.01	99.06	　99.36	　99.46
Ida	97.73	98.06	98.25	98.44	98.58	98.61	98.80	　98.70
表6-5表示当单一決策树的深度设定为8时，决策树的棵树的变化对定位精 度的影响。由表中可以看出，虽然定位精度相差不大，偶尔呈现波动的状态，但 是总体变化趋势仍然是随着决策树棵树的增加，定位精度对应提高。但由下表 6-6可知过多的决策树棵树同样会影响定位精度，实际运算时，弱分类器的个数 也是需要实验调整的重要参数。
表6-6深度为4时提升算法弱分类器个数对定位精度的影响(％)
、timator	50	70	90	　　no	　　130	　　　150	　　170	　　190
无压缩	71.31 81.77	85.71	　90.79	88.48	　　82.41	　　88.90	　86.59
Pca(50%)	90.86 93.48	• 94.40	　94.59	94.05	　　94.83	　　95.44	　95.40
Lda	94.21 93.55	93.84	　95.14	96.29	　　96.48	　　95.82	　97.12
表6-7 K近邻算法定位精度与引入特征选择算法之后的定位精度比较(%)
1	2	　　　3		　4	5	　　　6
K-vahie
算法\
无压缩		　　99.97	98.00	85.00		88.00	81.00	　　81.00
Pea		　　99.74	99.74	99.73		99.70	99.52	　　99.57
Lda		　　99.62	99.62	99.39		99.33	99.29	　　99.25
•在K近邻算法中，当K值为1时为最近邻算法。由上表可知虽然使用特征 选择算法之后，K近邻算法的定位精度并没有明显改善，但PCA算法却在原来 的基础上只保留了 10%相对重要的特征，而LDA算法也在原来特征的基础上压 缩至29维特征，虽然减少了大部分特征，但定位精度却没有明显降低，因此说 明特征选择算法的优势。结合6-8表的加权k近邻算法比较，可知压缩之后的定 位算法更加稳定。
对于支持向量机定位算法，使用特征选择算法之后与K近邻算法使用特征 选择之后的结果相类似，在不大幅度的降低定位精度的前提下使用特征选择算法
51
表6-8加权K近邻算法定位精度与引入特征选择算法
之后的定位精度比较(%)
章	　　　1	2	3	4	5	6
无压缩
Pea
Lda	　　99.97	98.00	85.00	88.00	81.00	81.00
99.74	99.74	99.73	99.70	99.52	99.57
99.62	99.62	99.39	99.33	99.29	99.25
可以简化定位模型，提高实时定位能力，缩短定位时长，甚至可以提高定位精度。
表6-9支持向量机算法定位精度与引入特征选择算法
之后的定位精度比较(%)
特征维数	无压缩	保留80%	保留50%	保留］0%	LDA保留29 维
LinearSVC	　　　99.74	99.62	　　98.98	80,52	　　　96.17
SVC	　　　99.88	99.91	　　99.79	97.47	　　　99.65
Rbf	　　　6035	53.67	　　49.63	50.91	　　　54.66
Poly	　　　99.88	99.86	　　99.86	99.50	　　　99.74
