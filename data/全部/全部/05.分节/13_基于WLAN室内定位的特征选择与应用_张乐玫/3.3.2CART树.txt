3.3.2CART树
CART (Classification And Regression Tree)树釆用的是二分递归分割技术，最 早是由Berkeley和Stanford的统计学家在1984年发表的分类器。CART的建树 过程和C4.5、ID3相似，不同的是CART树在每个节点挑选特征的时候所用过的 特征选择算法为基尼不纯度(Gini Impurity),该算法的具体介绍见4.4节。
CART树最重要的过程除了特征选择外，就是剪枝过程。当样本数据分类过 细时，数据样本中的噪声和异常值(outlier)会对模型的构建和树的形状产生影响， 使得训练所得到的模型对于新的接收数据预测的准确率远远小于训练时数据的 准确率，此时的树模型被称为过拟合状态。另外，在实际操作中，很难确定阈值 的大小或根据上述的停止条件在建树的过程中及时停止树的生长，因为所得到的 训练样本并能足以包含所有的问题，这被称为“水平效应”。剪枝就是减去冗余不 可靠的分支，这样也会使得较快速的分类。
CART的剪枝采用的最小化全局损失函数的方法，自底向上，将多余的分支 用叶子节点替代来进行剪枝。损失函数是叶子节点数量和误差率的函数，最小化 目标函数通常定义如下：
min egpEe(./),X)-err(T,X)	(3 ⑷
t ^leaves(T)| - Reaves (prune(T, r))|
其中，T表示己经建好的决策树集合，。表示验证是否需要剪枝的子树，X
25
表示样本集合，prune（T,t）表示已被剪枝的树集合，err（T,X）表示定义在数据 集X上的树T的全局误差率，err^prune（T,t）,X^表示定义在数据集X上的减去子 树[之后的树的全局误差率，，eaves（T）|表示叶子节点的个数,\leaves（prune（T,ty）\ 表示剪枝后的叶子节点的个数。对树的剪枝所用的数据应该是非训练集中的数 据。
