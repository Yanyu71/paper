3.3.1C4.5ID3
对不同树的定义主要基于构建树时所用的特征选择算法的差异。ID3树模型 所用的特征选择算法基于信息增益(IG Information Gain),而C4.5所使用的特征 选择算法为信息增益比。
在决策树中，每个节点对应某一非分类别属性的特征，节点的每一个分支对 应满足该特征的某一范围区间或该特征的某一确定值。叶子节点表示从根节点开 始，经过满足某一系列特征条件的路径之后，得到的类别属性值。
ID3算法是Ross Quinlan最早使用的用来建立决策树的方法。建树过程首先 将所有的样本点放在一个节点中，这个节点为根节点。在算法的每一次迭代中， 需要扩展每个节点时，算法会遍历所有未使用过的特征的集合，同时就每一个特 征计算它的嫡值或信息增益值，选择最小的嫡值或最大的信息增益值所对应的特 征作为该节点的特征(值)。然后，节点中的样本按照选择的特征或特征值进行划 分，按条件分别进入节点的左右孩子节点。算法如此循环直到遇到终止条件。
按这种方式建立的ID3树存在很多缺点：
1.ID3并不能保证全局最仇解，只能得到局部最优解；
2.ID3的构建算法属于贪婪算法，先选择最有利于分类的特征；
3.信息增益算法的计算有时依赖于特征出现的频率，特别是对于样本分布 不清的时候，该算法选择的特征更倾向于样本数量多的样本包含的特 征，但这些特征未必是最有利于分类的特征；
4.ID3是但决策树模型，也就是说，ID3在选择特征时只考虑了单一特征的
24
单一数值点，而忽略了多个特征之间的联系；
5.该树模型抗噪声能力差"这要求样本数据需要精心挑选，尽量避免冗余 数据的同时需要尽可能包含各个方面特征。
Ross Quinlan之后再ID3的基础上派生出C4.5算法。C4.5算法的特征选择 方式为信息增益比(IGR, Infbrmation Gain Ratio)。针对ID3, C4.5做出了如下改 进：
1.通过不等式阈值选择的方式,C4.5对连续特征和离散特征均能进行处理, 若遇到连续的特征值，C4.5先对其进行离散化处理；
2.对于丢失的特征值，C4.5不会进行处理；
3.在建树过程结束后,C4.5会进行决策树的剪枝操作以防止模型的过拟合。
C4.5也存在一些缺点：在构件树模型的过程中，需要对特征值和节点中的 数据进行逐一扫描，若数据量十分大时，训练时间会加长；另外，若样本数据有 很多噪声，树模型的建造也容易过拟合。
具体的信息增益和信息增益比算法请见4.1节。
