3.3室内定位决策树算法
决策树算法【"I实质上可以看做是一系列的if-then准则。室内定位决策树的 离线阶段过程主要包括三部分：移动接入点的选择、决策树的构建和决策树的剪 枝过程。最终，一棵决策树会包含一系列中间节点和许多叶子节点，对于二叉树
22
而言，中间节点通常会将当前落到节点中的数据样本分为两部分，具体数据的划 分方式需要根据对应的特征和对应的准则，这两部分数据一部分落到左节点，另 一部分落到该节点的有节点，左右节点上的数据在经过相同的准则寻找合适的特 征将数据进一步分离。就一棵树而言，叶子节点的数量一般与参考点的类别并不 相等，通常会大于参考点的数量。我们在空间中可以把每一个中间节点当作一次 空间的划分，对于非线性的样本分布而言，一次划分并不足以将一种类别的样本 数据与其他类别的样本数据分离，因此，可能需要多次划分过程。
构建一棵树的过程是按照自顶向下的方法构建，开始时防君样本均在根节点 中，而后再根据一定的特征选择算法将各个样本往下细分。树的剪枝过程与建树 过程恰好相反，自底向上进行剪枝。
若编程实现一棵决策树算法，对一批固定样本来说，构建树的停止条件有很 多种，当叶子节点的样本数量小于某一规定阈值时，当前的节点可以当作叶子节 点，不必往下继续分割；当当前节点错分样本的误差小于某一规定阈值时，当前 节点可以停止分割；当在某一节点处存在于该节点的特征己用尽，则通过举手表 决的形式选择该节点的标志参考点；当树的深度达到一定高度值时，当前节点可 不须在分；当某一节点钟的样本均属于同一类时，算法可以停止。
室内定位决策树算法的具体实现步骤如图3-3所示。
初始化生成样本训练数据集£特征集合A；
离线阶段：
1.设置算法循环次数从决策树深度飾叶子节点个数〃，定义决策树中特征选择算法， 增益、信息增益比或基尼指数等，根据特征选择算法设置误差阈值e 0;
for n=l to N,
2.根据决策树的特征选择算法计算，选择使得误差率e最小的对应接入点；
3.根据所选接入点及样本出现数值选择合适的阈值，生成节点；
end if e < e o , or if树深>=g or if叶子节点个数〉〃 else
4.从特征集合中除去已选特征，循环继续。
图3-3室内定位决策树算法实现步骤
决策树算法的优势在于：
1.树模型可以用可视化的方式实现，也就是说内部的模型形成方式可以直 观的观察到，这样利于模型的理解和解读；
2,不需要过多的训练数据，但训练数据需要尽量包含不同的移动接入点， 且移动参考点的划分应尽量合理,不能过疏，也不能过密；
23
3.在定位过程中可以讲信号接收强度的大小作为定位基准，也可以用1和 0来标注有或没有某个移动接入点信息。
另一方面，决策树算法同样存在许多缺点：
1.首先，树的形成过程并不是全局最优化的，最优化问题在这里属于NP- 困难问题，因此，大部分的树模型算法使用的是例如贪婪算法的启发式 算法；
2.当一棵树在模型训练阶段设定的树的深度过大、分支过多或者参考点釆 样接收到的移动接入点过多时，树模型很容易产生过拟合现象从而降低 实际的在线定位性能；
3.存在一些树模型无法解释的问题，比如异或、奇偶校验等问题。
室内定位常用的决策树算法有三种：C4.5决策树、ID3决策树和CART决策 树。对于一棵决策树而言，每个节点的特征选择过程极为重要，这关系到整棵树 性能的好坏。不同的特征选择算法将在下一章节进行详细的描述和比较。
