6.3.2分析及结论
定位系统的性能可以从安全和隐私、成本、定位精度和限制等方面来进行评 估。而定位算法的分析可以从鲁棒性和容错、预测效率、复杂性和定位精度等方 面进行分析。
定位系统已在论文2.3节进行了系统的分析，根据上一节的数据结果比较， 本节主要从定位算法的角度进行分析：
1.K-近邻算法：近邻算法中K值的算法对算法的精确度和模型的复杂度都 有重要的影响。K值可以直接被看做是训练模型的复杂性。K值较小意 味着模型较复杂，也就是说预测时需要从较小的邻域确定目标参考点。 反之，K值较大意味着模型相对较简单，预测是需要从更大的邻域范围 寻找目标参考点。由于K近邻算法的预测需要遍历数据库中的样本，因 此，当数据库很大时，算法的预测效率较低，训练模型的成本较低，模 型复杂度较低，而定位精度和泛化能力取决于K值的选取。因此在数据 量大的实际情况中不建议贏用。
2.朴素贝叶斯：该算法属于概率性算法，最终的输出结果的意义是该预测 特征集合属于某个特定参考点的概率大小，而论文中所提到的替他算法 均为确定性算法，输出为某一确定参考点的标志。朴素贝叶斯算法由于 是基于较强的条件独立性假设而在实际使用中模型的定位相较于其他 算法较低。算法的容错能力相对较差，预测效率也和数据库中的数据量
52
有关，因此大数据量时预测效率较低。
3.决策树算法：论文介绍了三种决策树算法，这三种决策树算法的区别在 于特征选择算法不同。该算法的训练过程在每一次特征选择的时候均需 要遍历一边所有样本的所有特征和每一特征的所有数值。因此，决策树 算法的训练效率较低，但生成的模型复杂度较低，预测效率基于一系列 i*hen条件判断，因此预测效率很高，由于决策树很容易出现过拟合的 问题，因此，若对决策树合理剪枝且样本覆盖较全面时会提高预测精度。
4.支持向量机算法：支持向量机算法通过引入核函数的方式解决了分布呈 现非线性分布数据的分类问题，但不同的核函数均需要进行参数选择， 因此，参数选择从一定程度上决定了定位的精度。另外，支持向量机只 与分布在分类边界的样本数据有关，而与其他数据的分布情况关系并不 大，因此，SVM算法的鲁棒性和容错能力一般较强。且通过一定的优 化算法，支持向量机的训练过程可以较快速的收敛。另外，虽然支持向 量机模型的预测效率比不上决策树算法，但SVM模型只需要记录预测 函数的对应参数，因此预测效率较高，特别是线性支持向量机的预测效 率更高。而SVM的定位精度也相对较高。SVM算法的另一个优点是可 以再一定程度上避免维度灾难，且在样本容量较小的情况下也可以得到 较好的效果。
5.提升算法AdaBoost：提升算法通过将多个弱分类器乘以权值并级联的形 式，用启发式的算法增强了模型对错分样本的关注度,提高了算法的泛 化能力。提升算法也可以非线性分布的样本进行分类，但不同与支持向 量机算法的是，后者对不同核函数的使用导致最终的分类超平面呈现核 函数类型的分布，而AdaBoot却可以模拟任何形状的分类平面。由于提 升算法是建立在决策树桩这种弱分类器基础上的算法，因此，该算法也 具有决策树的一系列优点，即预测效率较高等。
6.LDA特征选择算法分析:LDA算法的优势是在模型的训练阶段同时考虑 到类内间距和类间距离。LDA有时需建立在PCA算法的基础上先进行 特征压缩，然后再根据类别信息进行进一步特征压缩。虽然该算法属于 监督性算法，考虑到类别之间的差异，但LDA最终的压缩维数小于类 别种类数，因此，对于有些需要高维特征进行决策的数据集合，往往压 缩维数过高导致信息损失严重反而不利于分类。
7.PCA特征选择算法分析：主成分是指较大的特征向量所对应的特征 值，该算法考虑去除数据特征之间的线性相关性，从而更有利于找出同 类和不同类别的样本，达到降低特征空间维度，防止维度灾难的目的。
53
该方法不仅适用于图像处理，在很多领域均可应用来提取样本的主成 分。对于超高维的情况可以釆用SVD分解来求出举证的特征值和特征 向量。这种方法适用于信号内有用信息能量大于噪声，但若是信号的噪 声大于有用信息的情况下，该方法保留的主成分很可能是噪声。
54
