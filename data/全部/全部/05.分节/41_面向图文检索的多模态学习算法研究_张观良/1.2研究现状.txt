1.2研究现状
自从2006年，Hinton等人发表了如何利用限制玻尔兹曼机(Restricted Boltzmann Machine)学习深度信念网(Deep Belief Network),从而构建深层网络以 来⑶，深度学习⑶受到学术界的广泛关注。深度学习通过构建多层神经网络模拟 人脑的分层结构。这种多层网络对底层输入进行逐层编码，提取从底层到高层的 特征，从而建立了底层简单的数值信息到高层语义的映射关系。包括微软、谷歌 等跨国公司和百度等国内大型互联网公司相继投入到深度学习的研究，并使用深 度模型提高自身产品的质量⑶］。2011年，微软将深度学习应用到语音识别上， 使得识别率达到了 7Q80%； 2012年，谷歌公司启动Google Brain项目，该项目 由斯坦福大学机器学习专家Andrew Ng和计算机系统专家Jeff Dean负责。在该 项目中，利用16000个CPU Core组成一个并行计算平台，构建机器学习模型。 该平台有10亿多个节点，用来训练一种称为"深度神经网络"的结构。该实验中， 这套系统通过大量的无标签数据学习特征转换参数，没有人工干涉，系统自己领 悟这些概念。最后利用采样方式，发现系统自己学习到了如何找到猫的脸；同年， Hinton等人［5］将卷积神经网络带入了深度学习中，刷新了 ImageNet Large Scale Visual Recogition Challenge (ILSVRC)竞赛中图像分类等项目的准确率，使得卷 积神经网络成为提取图像特征的重要工具。
在文本方面，由于词袋模型的各种缺陷[35]，在2001年，Bengio等人卩7］通 过一个三层神经网络构建出语言模型。该模型中，单词的特征是实数值向量，单 词之间可以相互比较，文中的基本思路和后续的语言模型差别不大。2007年， Hinton等人［4］将受限玻尔兹曼机引入到自然语言模型中，文中从基本的受限玻 尔兹曼机，逐渐修改能量函数，最终得到对数双线性(Log-Bilinear)模型。除 了在自然语言模型上，Hinton等人［1］根据主题模型的思想【35】，利用受限玻尔兹 曼机的特性，建立起重复软最大化(Replicated Softmax)模型，该模型使用隐藏 层作为隐藏主题，构建主题和单词之间的关系，并能够使用大量无标签数据进行 训练。在［17］中，最后一层的Softmax模型有着大量参数，为减少参数个数，
Mikolov 等人［44］在 2010 年提出了循环神经网络(Recurrent Neural Netwok)，在 减少参数的同时，理论上可以充分利用到所有上文信息来预测下…个词。
图像和文本模型在各自模态下都有着重要发展，可以更好地进行特征提取。 基于图像与文本的多模态模型中，主要研究方向有图文的检索和基于图像的描述 生成。图像检索方面充分利用了深度学习在各领域的发展，例如J.Ngiam等人［45］ 提出了双模态深层自编码机(Bimodal Deep Auto-encoder),通过逐层的训练和联 合特征，实现多模态语义关系的学习。此外还有Frome等人［41］和Karpathy等人 ［42］结合卷积神经网络与词向量模型，构建多模态模型。在基于图像的描述生成 方面，主要是在自然语言模型的基础之上，引入图像特征，学习如何根据图像影 响单词的生成概率，例如Vinyals等人［11］在以循环神经网络为基础的模型上实 现了文本生成。基于完整图像与文本的特征的多模态模型中［41,45］,分析了如何 使用深度学习模型提取特征，并根据特征的特点提出多模态特征学习的方法，但 忽略了一个固定长度的特征无法描述复杂图片或句子的问题。在基于细粒度特征 的多模态模型中［42］,提出了图像片段与文本片段之间的关系学习方法，但文本 中任意两个单词的关系根据依赖树人为确定，忽略了更多单词之间的语义关系。
