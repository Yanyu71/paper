2.1.3重复软最大化模型
概率主题模型【14】主要用于从大量文本集中分析和提取语义主题。主题模型一 般假设每个文档都可以由多个主题表示，每个主题是不同单词的概率分布。主题 模型也可以看作是一个图模型，其中每个隐藏主题都与文档中的代表词汇有连接 关系。上述连接关系可用RBM模型进行建模，其中隐藏层的单元相当于隐藏的 主题，而输入层的单元个数则为单词字典的大小，单元的值是该文档中某个单词 的出现个数，该模型被称为重复软最大化模型(Replicated Sofhnax Model)⑴。假 定ve{l,...,K}气 其中K是字典的大小，而D是文档的长度。Ag{0,1}f作为随 机二元主题特征。V是一个Kx。的二元矩阵，用于表示字典特征，此时#=1表 示文档中第i个单词在字典中的位置是ko类似于式(2-3)可以得到能量模型：
E(V,h) =	-f
i=1 J=\ k-\	/=) A=1	y-i
F K	K	F
*4 1,	(2-16)
丿=1 AT	A=1
其中{附,以}是模型参数，俨相当于计算第k个单词出现的次数。在训练过程中， 给定一个长度为N的句子，生成隐藏层的算法与RBM模型相同，但从隐藏层生 成输入层时，模型需要进行N次采样，将釆样结果进行求和。由于输入层是N 个单词向量求和得到的K维向量，因此展开后每一行是K维二值向量，只有一 个单元为1,其他为0,表示为一个单词的向量，如图2-l(a)右侧所示。每个单 词向量使用同一个权重，如图2-l(a)左侧所示。隐藏层在实际使用中可以不进行 二元化，直接输出sigmoid函数的结果，此时输入层的每个单元是单词的出现次 数，隐藏层的每个单元则是主题的概率。
(a)
11
Latent Topics
图2-1 (a)中的右图是字典矩阵K,其中每一列都公用一个权重值。(b)是实际训练的一层
RBM模型，输入unit是每个单词的出现次数，隐藏单元则是主题。
