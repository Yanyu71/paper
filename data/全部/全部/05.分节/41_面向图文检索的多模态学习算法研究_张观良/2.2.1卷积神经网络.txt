2.2.1卷积神经网络
卷积神经网络(Convolution Neural Network, CNN)'从全连通的神经网络简 化而成，其中局部感知受医学研究的启发，视觉皮层的神经元是局部接受信息的, 即这些神经元只响应某些特定区域的刺激。卷积神经网络与前向神经网络相比, 有三个主要差异：局部感受野、权重共享和空间池化层。
对于一个普通的神经网络，每个神经元都与下一层的全部神经元有连接。在 隐藏层中，每个神经元的响应都与输入层的每个单元的值有关，但在视觉识别中， 神经元更倾向于与局部的单元有关。例如在一张图片中，相邻的像素在颜色和方 向上有着强烈的相关性，而两个相隔较远的像素则相关性较小或者没有关系。因 此在计算机视觉中，很多特征表示都是基于局部特征，如SIFT【9】和HOG卩5】。为 了能够获取这种局部信息，CNN模型限制每个神经元只依赖于上一层中的局部区域的值。例如给定一个32x32的图像输入，隐藏层中的某个神经元只依赖于图 片中的某个8x8的子区域。因此在输入层中，影响隐藏层中某个神经元响应的单 元就是该神经元的感受野。更直观地说，这部分就是神经元能看到的部分。
第二个因素是这些局部感受野的权重是共享的，每个感受野与隐藏层中的每 个神经元之间使用同一套参数。相当于用同一个权重对输入层中每个子窗口的单 元进行合并，得到隐藏层中的神经元响应。因此CNN是学习一个权重集合 F = {Fi\i=l,.../i ,每个权重都会作用到输入图像的所有子窗口上。权重共享逼 迫网络去学习一个普适的编码方式，同时减少学习参数，使得网络可以高效地进 行训练。由于这个计算过程相当于用一个卷积核对图像进行卷积运算，因此权重 集合也是一个卷积核的集合，通过卷积可以得到卷积响应图。
最后一个因素是池化(pooling)层。使用池化层的目的是减少卷积响应图的维 度，同时引入微小的平移不变性，其标准的方法是空间池化SI。对于卷积响应 图，首先将其分成AMX”的方格，之后通过池化函数计算每个方格的响应，这个 过程最后得到的是一个维度为mx”的响应图。假如池化函数是求最大值，则找 出方格中最大的值作为该方格的响应值；假如是求平均值，则求方格的平均数， 如图2-2所示。
2	　4	　6	　8		3	7
2	4	　6	　8
1	3	　5	　7		　　　4	4
7	5	　3	　1
卷积响应			Poo 1 ed响应
图2-2平均池化，对于卷积响应，分成一个2x2的方格，每个方格是2x2。针对每个方格求 平均值，得到一个2x2的池化响应。阴影部分则是(6+6+8+8)/4=7。
除了卷积层和池化层外，局部对比度归一化(Local Response Normalization, LRN)层和剔除(Dropout)㈣层也用于CNN网络。局部对比度归一化层主要针 对局部特征进行归一化，它会迫使同一特征图中的相邻特征进行局部竞争，还会 迫使在不同特征图上同一空间位置的特征进行竞争。该处理也是由计算神经科学 模型启发得到的，如式(2-18)所示。
min(iV-l,i+?/2)
%=《,y/(* + a Z (<y)2)	(2-18)
7=max(0,i-n/2)
是由卷积核i得到的特征图中，位置为(x,y)的响应。其中求和的n是相邻卷 积特征图的个数，N是该层的所有卷积特征图个数。
在深度模型中，由于参数众多，容易造成过拟合，特别是在最后的全连通层 ±o因此单元的剔除经常用于避免复杂网络出现过拟合，并模拟了多个神经网络 的融合。在CNN模型中，全连通层后一般都会接着剔除层。对于全连通层，输 入和输出的单元个数都为4096,因此输入与输出之间有着非常复杂的非线性关 系。在数据缺少的情况下，模型会对一些噪音进行建模，而这些噪音可能只出现 在训练集当中，这也导致网络出现过拟合。一般有许多方法用于解决这个问题， 例如当模型在验证集上的效果变差时就停止训练，或者通过L1和L2正则化， 惩罚过于复杂的网络参数。在有无限的计算资源时，则使用多个模型进行融合, 随机森林就是通过多个随机树来避免单一决策树的过拟合。但对于复杂模型，多 个模型的计算量巨大，另外模型合并需要各个模型有明显差异，其中可能需要用 不同的网络结构和不同的训练集。
剔除是从网络中剔除部分单元，但只是暂时将这些部分从网络去除，包括单 元之间关联，如图2-3所示。
图2-3 Dropout神经网络冋，(a)中为有两个隐藏层的标准神经网络，(b)是部分单元被 dropout后的神经网络，其连接也去拝了 o
每次剔除的单元是随机选择，每个单元的剔除概率可以根据验证集选择或者直接 使用0.5。
根据图2-3,模型具体计算如下，给定一个有L个隐藏层的神经网络， 是｛1,...妇是每个隐藏层的索引，z（'）表示对应层［的输入，俨）则为输出，时）和 砂）分别是权重参数和偏置项。前向神经网络的前向传播如下所示：
z件）=时+?（，）+涉S	（2-19）
北（5=/（*5）	（2-20）
/?是非线性函数，使用dropout后，前向传播方式如下：
驴?Bernoulli］ °）	（2-21）
刊）="）* 尹）	（2-22）
z；s）=讨“俨）+々如）	（2-23）
孝）=代件））	（2-24）
相比标准的神经网络，剔除网络需要先根据给定的概率p确定每个单元是否需要 去除，单元剔除后的操作与前向网络相同。在后向传播过程中，被剔除的单元不 再考虑，相当于对应参数的导数为0。由于单元剔除的引入，网络的训练时间加 长，整个训练时间一般为标准网络的2到3倍。训练时间加长的一个重要原因是 参数在更新时，每个训练数据都在训练一个新的随机网络，因此导数不是最终网 络的导数，导致训练时间变长
CNN网络目前主要使用已经训练好的模型，这些模型利用大量的数据进行 分类器的训练，比较常用的网络有AlexNetB】、GoogleNet^和VGG-16网。这些 网络主要是在卷积层和池化层的组合方式和参数上有差异，利用CNN网络提取 的特征一般以输入到最后一层分类器的特征为主。
