4.2.3Cafie概述
对于双向循环神经网络，隐藏状态在序列中传递，沿着状态传递序列来看, 该网络是一个深层网络。假如输入句子长度为30,此时网络有30个隐藏层，只 是每一层的参数共享，因此训练速度较慢。为解决该问题，本文在Caflfe【3。］上实 现细粒度多模态模型。
Caffe是一个以layer作为模型基础的深度学习框架，每个layer都有自己功 能，其中主要分为 data layer、vision layer、loss layer 和普通 layer。
data layer主要负责数据的读取，其中包括从规定格式的数据库或者内存 中读取数据。本文主要使用其中的memory layer,该layer主要从内存中 读取数据，用户需要将数据以及标签的指针传送到该layer中。
vision layer主要包括卷积、池化和局部对比度归一化层。
loss layer则是训练网络的最后一层，其中定义各种损失函数相关的layer, 本文所使用排列损失函数也需要定义为loss layer。
普通layer主要包括各种数据操作，例如reshape、transport和resize等。 本文根据实际需求定义的一些数据操作都为普通layer<,
一个完整的网络是根据这些layer搭建出来，每个layer根据定义对输入进行 操作，然后将结果输出。数据操作定义在每个layer中的forward和backward函 数礼 同时实现了 CPU和GPU版本，所以新增layer时，实现的操作都需要有 两个版本。
除了数据操作外，Caffe定义了用于数据传输与保持的Blob类。由于Caflfe 同时管理者内存和显存，因此为使得存取数据时，保持数据的一致性和操作的透 明性，Caffe定义了 SyncedMemory类来完成底层的数据存取操作。在 SyncedMemory中保存着指向内存和显存的指针，同时利用own_cpu_data_标记 最新数据的存放位置。假如一个数据先存放在内存中，那么在获取内存中的数据 时，则不需要搜索显存。当获取显存中的数据数据时，由于最新数据保存在内存 中，因此SyncedMemory会将数据从内存转移到显存里。另外当用户是获取可变 动显存数据时，则认为最新数据保存在显存中，再次获取显存中的数据时，则不 需要与内存同步。因此own_cpu_data」J^^数据存取更加迅速，在编写layer操作 时，数据的操作都需要注意取出的数据是否需要修改。layer定义了前向传播和 后向传播的操作，而Blob类则是每个layer之间的桥梁，其中包括保存前向传播 数据的data_和后向传播数据的di町，两者都是SyncedMemory类。
每-一个layer允许有多个输入和多个输出，在创建网络时，layer会检查输入 和输出数目是否符合自身定义。输入称为layer的bottom,而输出则是top, bottom 和top都是Blob类的实例。Caffe定义了 Net用于管理这些layer, Net的主要工 作是根据定义创建网络，存放layer参数，为用户提供透明的前向传播和后向传 播操作，无需担心layer的执行顺序。在训练网络时，Caffe定义了一组Sovler 类，用于管理训练的参数，例如迭代次数、学习率的变化方式等。一个Sovler 保存了一个训练Net和多个测试Net,初始化时会自动根据定义创建完成。Caffe 的网络定义和训练方式是通过编写配置文件完成的，配置文件使用Protocol Buffer语言。
