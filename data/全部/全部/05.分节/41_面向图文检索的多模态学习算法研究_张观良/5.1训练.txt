5.1训练
对于多模态深度玻尔兹曼模型，本文首先利用更大的数据集MIR Flickr^】 对网络进行预训练。该数据集包含一百万张图片和用户给的标签，这些数据来源 于Flickro在这一百万张图片中，有25000张图片标记了 24个主题，例如:bird、 tree、people、indoor> sky和night等。该数据集中有2000个高频标签，平均每 个图像有5.15个标签，标签数量的标准差为5.13。另外，在25000张图片中， 有4551是没有文本输入。使用VGG-16【38］的CNN网络中最后一个连通层的输出 作为深度玻尔兹曼模型的图片特征，那么图片的高斯RBM模型有4096个输入 单元，之后两层RBM的隐藏单元数目均设定为1024。对于文本部分，结合MIR
Flickr的高频标签和Flickr8k的高频词汇，得到2000高频单词，因此重复软最大 化模型有2000个输入单元，表示字典大小为2000,之后两层RBM的隐藏单元 数目同样是1024。对于最顶层的隐藏层，其单元数目设定为2048。每层RBM 模型都使用PCD【3i］算法来初始化DBM模型，PCD算法是CD算法的改进。在 CD算法中，每次吉布斯釆样的初始值为数据本身，而PCD则是使用上一次吉布 斯采样后的值，PCD优点是随着采样的进行，更接近均衡分布。而CD算法则在 每次迭代时，需要以输入数据作为初始值，进行多次吉布斯采样，从而更接机均 衡分布【31】。训练高斯RBM中，设定式(3-9)中的方差为1,并且保持不变，相当 于不对方差进行训练。预训练的初始学习率为0.001,参数使用方差为0.01、均 值为0的高斯函数进行随机初始化，遍历训练集25次，学习率线性降到原来的 0.01倍。完成预训练后，使用Flickr8k对模型微调，图片使用相同方式处理，根 据字典统计文本中句子的单词数目作为文本特征。训练算法使用PCD算法，初 始学习率为0.0001,遍历训练集15次，最后3次学习率变为原来的0.1倍。
在依赖树递归神经网络中，文本需要根据依赖树生成句子特征。因此本文使 用斯坦福大学的语言分析器〔39】为句子生成依赖树，同时使用word2vec【4。］预先训 练好的词向量矩阵，对于没有词向量的单词，使用特定的向量替换。图片特征依 然使用基于VGG-16的CNN特征，转换后的维度设定为600?训练使用随机梯 度下降，batch size为100,参数通过高斯函数随机初始化，初始学习率为0.01, 每当遍历数据集10次后变为原来的0.1倍，直到变为0.0001则不再下降。损失 函数中的△为3,当损失函数趋于平缓时停止训练。
对于细粒度多模态模型，分别利用Selective Search和EdgeBoxes提取2000 个候选边界匡。使用基于Caffe的R-CNN模型得到边界框中存在物体的概率， 由于使用EdgeBoxes提取的边界框过于集中，因此使用非极大值抑制对边界框进 行过滤刖，其中IoU分别设定为0.5和0.6。如4.2.1节中介绍的流程，获取20 个边界框。边界框内的图像特征分别使用R-CNN模型和VGG-16的CNN网络 进行提取，但训练完成后对VGG-16的CNN网络进行微调。文本使用word2vec 的词向量，去除句子中的标点符号以及”a”、”the”和”an”，罗马数字统一由一个 词向量代替。训练初始学习率为0.001,遍历数据库20次，最后4次学习率变为 原来的0.1倍，batch size为100。使用VGG-16的多模态模型，训练完成后微调 CNN网络中的最后两个全连通层，此时提取的特征是pool5层输出，其大小为 (n,512,7,7),初始学习率为0.0001,遍历数据库15次，最后3次学习率变为原来 的0.1倍。双向循环神经网络中的隐藏状态维度为600,输出维度为1200。式(4-6) 的△为10, △从1到10的时候，检索结果的topi、top5和toplO都有提高，到 10以后则基本不变，但中值以及平均正确检索值则继续降低，说明△的增大，可以提高模型的整体性能。
对于自然语言多模态模型，对数双线语言模型使用word2vec的词向量初始 化,所有上下文参数矩阵使用的weight decay为1.0 x 1 而单词特征使用1.0 x 10书。 batch size为20,初始学习率为0.2,以指数方式下降，参数为0.998,上下文特 征的步长为5,相当于5-gram。
