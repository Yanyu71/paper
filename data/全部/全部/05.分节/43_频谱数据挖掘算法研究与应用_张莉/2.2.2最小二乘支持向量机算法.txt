2.2.2最小二乘支持向量机算法
Suykens和Vandewalle于1999年提出了最小二乘支持向量机(Least Square Support Vector Machine, LSSVM), LSSVM 算法的提出是为了解决 SVM 算法在 面对大量数据集时优化问题求解困难的问题。相比于SVM,LSSVM有两点改进, 一个是约束条件变成等式的形式，另一个是目标函数中使用误差的平方项。经过 这两个地方改动后，求解优化问题被转换成了在满足KKT(Karush-Kuhn-Tucker) 条件下求解一组N维线性方程组。也就是说，LSSVM比SVM明显降低了计算 的复杂度，提高了训练效率，更适合于大规模数据集的求解。LSSVM算法数据 处理的流程图如图2-5所示。
图2- 5 LSSVM算法数据处理流程图
LSSVM的原理描述如下：
样本数据集表示为*: R” 一 H伉，舞｝n，N为样本数量，输入数据％ €玲是n 维，输出数据队€矿对应输入数据叫。对于非线性回归问题，通常需要输入数据 2?映射到％维特征空间"得到敏矽，"tH是一个非线性映射函数，输出数 据在H空间运算表达式为：
v(x) = ?'w(x) + 8	(2-17)
式中，3 e R“"是咼维的加权向量，6是偏差值。
Suykens等根据VC维理论和结构风险最小化原则，把上述的回归拟合问题 推演为求解下列问题：
J .	]	。
min J (w.e) = - w1 w + 7 — e?
岫e，2	2 会*
s.t.y* = wT(p(xk)+b+ek,k =
其中，t用于平衡拟合精度和泛化能力，称为正则化参数；e*为样本的误差变量; 状贫是用来控制学习模型的学习复杂度，。
式（2-18）描述的优化问题计算量大而复杂，因为其解在高维特征空间，通 常会通过引入拉格朗日乘子%20,k = l,..,N将式（2-18）转化为其对偶问题：
N
L(w,b,e;a) =	(边,e)-】气｛方頌％) + b + -队｝	(2-19)
根据Wolf对偶定理，对式（2-19）各变量求偏导后消去及e“得出线性方程:
(2-20)
其中 口=亶,％宀。/；1〃	/ 为单位矩阵，y =WiR2，..RnK ： Q为 NxN 对
称方阵％ =以%）’（X%） = K（%，呵），底Z =	； K（%，叼）为满足Mercer条件的核函
利用式（2-20）计算出的b和a能得到对于全新样本z的函数估计式为:
g（￡）二 ^2气K（z，气）+ b	（2-21）
k=l
其中新的样本八历史数据％.直接影响核函数K（/,*）。本研究选用核函数是高斯 径向基函数，因其具有全局收敛性的，其表达式为：
K（z, %） = exp（-———	）	（2-22 ）
2o2
其中。是核宽度。常用的核函数有线性核函数、高斯径向基核函数、多项式核函 数等。
固定核函数后，LSSVM下一步需要固定的参数是：核宽度“及正则化参数7
（T和°被合称为超参数）。核宽度a影响用于训练的样例的分布特性，正则化参 数了主要影拟合精度和泛化能力。现在尚无规定的高效方式用于计算正则化参数 T和核宽度a, 一般采用的试凑法，但其计算时间长而且不能确保一定能找打全 局最优解，本文采用耦合模拟退火（CSA）算法加Nelder-Mead下山单纯形算法 来优化正则化参数7和核宽度＜?■的取值。
