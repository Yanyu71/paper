2.2.1支持向量回归机算法
1.线性支持向量回归机
假设样本数据集表示为夕：庞伉必KL，其中N是样本数量，输入数据 %€矿是n维的，输出数据y"”与输入数据叫对应。假设线性回归函数为：
f(X)= wTx + b
其中w e Rn ,b e R,w是法向量。损失函数使用如下：
I _，W|
丿 ￡	\y-f (jv)| _ ￡, othenvise
其中，，为预设可容忍损失函数，式2-4表示的物理状态如图2- 4所示:
将线性回归函数/Xx）沿》轴向上和向下平移误差接受值￡所囊括的区域形成 带在满足实际生产精度要求的条件下，￡-带的间隔较大时，可以将数据集中 的大多数点集中在带内，此时的损失函数值是最小的也就是回归拟合效果是 最好的。两条虚线间距离为2/|恻|,算法的优化目标是要最大化间距，即极小化 对数距离的倒数。最后将寻找的最适回归函数的过程转换为：
min^-||w||2	（2-5）
引入惩罚参数C、松弛因子眾；,回归估计对应的优化问题转变为：
mm P = min ||w||2 + Cg + ）	（2-6）
其应该满足的约束条件为：
y^w1 xt—b< E + <^i，i =
-w’Xj +b-.	+	= 1,...,N	（2-7）
W：20,i = l,..,N
通常会引入拉格朗日对偶函数来求解求解上面的凸二次规划问题。构造拉格 朗日对偶函数如下：
1	N	N
：l|w|| +c￡g+￡?）-￡%（￡+§-丸+/旳+8）	（2-8）
z	/=!	/=1
N	N
一￡a：（￡ + g； + yt~ Xj -b）-+ + & ）
其中引入了拉格朗日乘子a,,a；*,〃：20,则将问题转化为求解：
max. min,A（w0，U，S*0，Q*，〃，〃.）	（2-9）
,刀，矿
将拉格朗日对偶函数分别对W,如洁求导并等于零，可将这对偶问题转化成凸二 次规划问题：
max JD（a,a*）=
ata
（% 一a；）（at -ak）xI'xk -（a, + a：） + ZZ（% -a；）
L 妇=1	/=!	i=l
其条件是：
N
￡（%-ar：） = O
i=l
%a； g [0,c]
最终得到的目标拟合回归函数表示为：
/(x) = 2(a,-a；)x； x + b	(2-12)
i=l
通过推演可知，在SVM中支持向量的个数是有限的，是那些％不为零对应 的输入样本X,,只有满足这条件的输入向量才对SVM模型有贡献，这就是SVM 的解的稀疏性。
2.非线性支持向量回归机
前面讨论的是线性回归函数的构造方法主要是针对样本集为线性情况，但是 日常生活中大部分的场景下都是非线性的数据，为解决样本集的非线性回归问 题，SVM算法使用了非常经典的核技巧，即引入了核函数K(x,,x,) = °(x,)'的)， 其中。(X,)是非线性映射函数。0(x,)的任务是将输入数据从低维非线性的经函数映 射成高维线性状态，接着用前面讨论的线性SVM得到支持向量机的回归模型。
由于映射函数00)很难有效确定，学术界就采用了核技巧，其思想是只定义 核函数K(x?Xj)从而避开定义映射函数，避开了维数灾难和复杂的高维空间的内 积运算。常用的核函数有多项式核函数(Polynormial kernel function)＞多层感知 机函数(Multi-Layer Perceptron,MLP)和高斯核函数(Gaussian kernel function, 又被称为径向基核函数Radical Basis Function,RBF)、字符串核函数(String kernel funciton)等。其中，多项式核函数有较优的泛化能力属于全局核函数，但付出 的代价是对非线性问题的拟合效果差；径向基核函数和多层感知机函数对非线性 问题的拟合效果好是局部核函数，但是牺牲了泛化能力即预测效果差；字符串核 函数主要用于信息检索、文本分类及生物信息学等方向。核函数的选择也依赖于 领域知识，其选择的有效性也需要通过实验验证。
选定核函数后，就可以用核函数K(x,,x,) = ?(x,)?(x,)替换线性SVM中的x,x,, 则非线SVM的问题可表示为求解下列问题：
max	=
a,a
]N	*	N	N
(% -a；)(% 一状)K(x,,也)-&Z0 + W) + ZN(%
L k,i=l	z=l	/=!
其条件是：
N
￡(%—a：) = 0
/=]
以G [O,C]
最终得到的目标拟合回归函数式为：
N
/(X)= ￡0 -a；)K(x,,x) + b
16
当缺少样本数据相关领域的先验知识时，一般会选择径向基核函数作为核函 数，它比其他核函数能更好地平衡拟合效果和泛化能力。径向基核函数的表达式 如下：
K(x ,x) = exp( ~	)	(2-16)
其中。是表征核宽度的参数。
