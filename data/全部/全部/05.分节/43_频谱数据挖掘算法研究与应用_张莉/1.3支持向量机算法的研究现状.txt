1.3支持向量机算法的研究现状
支持向量机(Support Vector Machine, SVM)是以统计学习理论为发展起来 的机器学习技法[9][10]o SVM可以比较恰当处理小规模样本、非线性、高维度数 据等复杂问题，理论上该算法可以解决用最小化经验风险为目标的BP算法具有 的局部极值和出现过拟合的缺点，具有较好的泛化能力。最小二乘支持向量机 (LSSVM)主要承袭了 SVM算法的特长，同时LSSVM把衡量误差的二范数项 替换掉了 SVM的损失函数值。用等式约束条件等价于SVM的不等式约束条件， 从而将计算凸二次规划问题转化为计算线性方程组的解，明显降低算法复杂度 tll]o正是因为这些优点，LSSVM已经被大量地应用在模式识别领域及函数估计 等问题中。
在提高LSSVM的效能上，许多学者针对算法的不同方面给出了诸多的改进 方案。为解决不同类型样本数值差异较大而产生的模型效果不佳的问题，Suykens 首先给出对每一类别样本额外添加权重值的加权最小二乘支持向量机 (Weighted-LSSVM,WLSSVM)算法【⑵，针对训练样本的误差值给样本赋予不 同的权重，这种方法明显提高了 LSSVM算法的鲁棒性；李丽娟等针对在线预测 模型，提出来局部加权最小二乘支持向量机(Local Weighted -LSSVM)模型， 根据训练样本与测试样本的欧式距离确定样本的权重，在提高稀疏性与快速性卩3】 上有一定的效果；周伟等从核密度计算的角度，进一步改进了样本加权的方式， 提出来核密度加权最小二乘支持向量机(Kernel Density Weighted LSSVM)模型 [14]
o
以上基本是从不同类别样本重要性差异的角度来改进算法，而没有考虑不同 维度的特征对模型的贡献程度和不同特征数据的量纲差异对模型引入的噪声。为 此，在SVM算法中，汪廷华等人提出来特征加权的想法mi,先利用信息增益独 立计算特征重要度，再用所取得特征重要程度对核函数中的内积和欧氏距离进行 加权计算，这使传统的SVM具有更好的鲁棒性和分类能力，然而其未考虑特征 的重要性在不同模型中的差异，且该方法只能针对分类模型不能用于回归模型； LeiGuo等人对特征加权支持向量机中的特征加权向量imp和支持向量机中原有 模型复杂度的参数c和核函数参数"同时进行联合寻优[16)[17],其未考虑将这些参 数分步进行寻找，或者合并参数减少搜寻最优参数的个数。
通过分析已有的相关改进算法，本文提出了两种自适应地寻找特征加权向量 的方法。一种是针对预处理数据有归一化时，使用耦合模拟退火算法加 Nelder-Mead下山单纯形法联合搜寻特征加权后模型中的超参数，此时将核函数 参数合并到加权向量中减少寻优参数的个数；另一种是针对预处理数据无归一化 时，分两步寻优得到模型的超参数，第一步固定特征加权向量沥p，寻找最优的 正则化参数T和核宽度参数b,第二步使用其中使用自免疫算法迭代得到最优的 特征加权向量。特征加权向量的引入可以避免由于不同特征数据的量纲差异及不 相干的特征项对核函数的计算带来的干扰，而自适应的方式寻找合适的特征加权 向量也提高了模型的预测精度。利用EUNITE大赛的数据进行数值实验［⑶，结 果表明本文提出的两种方法可以实现快速的收敛，并且所得模型在电力负荷数据 的问题中表现出预测精度的稳定提升。
