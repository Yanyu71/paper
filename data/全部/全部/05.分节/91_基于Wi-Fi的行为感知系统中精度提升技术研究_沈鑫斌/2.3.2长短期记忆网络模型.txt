2.3.2长短期记忆网络模型
循环神经网络(RecurrentNeural Network, RNN)是一种擅长学习序列隐藏 特征的递归神经网络。与CNN相比，RNN的隐藏层的神经元之间存在权值，从 而建立了一种关联，即当图像序列在输入网络时，隐藏层神经元的计算不仅依赖 于输入层当前的输入数据，同时还依赖于上一时刻隐藏层各神经元的数据。
但RNN中缺少了用于存储及输出帧信息的存储单元，因此对于序列中较长 时间间隔的图像帧，RNN的学习能力将会受到制约。与标准RNN不同的是，长 短期记忆网络(Long Short-Term Memory, LSTM)使用了能够存储及输出帧信息 的存储单元，因此对于较长时间前的输入数据，LSTM拥有RNN所缺少的记忆 能力。
14
X,
图2-8 LSTM模型示意图阙
LSTM模型包含了输入层恐、隐藏层结构和输出层饥。其中，隐藏层结构存 在着输入门0、遗忘门齐、存储单元耳和输出门0-这样的门结构确保了 LSTM模 型能够实现保留更有效的信息和遗忘无效的信息，以更新LSTM单元的状态，解 决了传统RNN存在的问题。具体结构如图2-8所示，各部分的更新情况如下所 示［49】：
it = ^(WxiXt +	+ bi)
(2-13)
ft = ^(WxfXt + Uhfht-! + bf}
(2-14)
往=tanh(Wxcxt + Uhcht_r + bc)
(2-15)
5 = ft O Si + it O Q
(2-16)
0上=+ U/ioht—i + b。)
(2-17)
ht = ottanh(ct)
(2-18)
其中，％、Wxf. %、必。是吐到4、齐、頁和5的权重，%、%、%、%是 隐藏层到4、ft、往和5的权重，$、bf、亠、bo是耳、ft、往和4的偏置，o■是sigmoid 激活函数，血加是将输入值规范到［-1,1］的激活函数。因此，可以看出，⑴、/t和 Ot是由拼接向量乘以相应的权重并加上偏置后，再通过sigmoid激活函数将其转 化为［0,1］之间的值，进而产生的门控状态。
LSTM存在三个重要的阶段：
15
(1) 选择记忆阶段。这一阶段主要是关于选择性记忆当前节点的输入，即“记 住重要的信息”。其主要依靠门控S，用于控制存储单元耳中哪些需要着重记录保 留下来。
(2) 遗忘阶段。这一阶段主要是关于选择性遗忘来自前一个节点的输入，即 “遗忘不重要的信息”。其主要依靠门控心，用于控制上一个节点的输入中，
哪些需要遗忘和不需要遗忘。
(3) 输出阶段。这一阶段主要是关于决定当前状态的输出。首先由上述两个 阶段的结果相加即可得到下一个状态5,随后依靠门控5进行控制，并通过tcm/i 激活函数完成放缩，得到短。
最终，和RNN—样，通过饥变化得到输出％。
在基于Wi-Fi的行为感知系统中，目标行为随着时间推移发生持续性变化。 由于从商用Wi-Fi网卡获取到的CSI序列是时间序列，因此该序列中蕴含着关于 目标行为的上下文信息。得益于LSTM在处理时间序列的极佳表现，选择LSTM 处理CSI序列能够实现对目标状态信息的长期记忆，从而完成对CSI序列的时 间特征的提取。这将更有利于模型对于目标行为的深层次学习。
