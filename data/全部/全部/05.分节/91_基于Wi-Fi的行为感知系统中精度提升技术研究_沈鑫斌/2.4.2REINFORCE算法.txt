2.4.2REINFORCE算法
在强化学习环境MDP〈S, A, R, P,兀〉下，智能体与环境发生交互，从感知到 状态s后根据策略兀执行动作a开始，持续完成与环境的交互，最终计算最大的累 积奖励值。这个累积奖励的期望被称为动作价值函数，又称为Q函数(Q-value function) Qn(s, a)[50h
Qn(s, a) = E[rt+1 + yrt+2 + Y2rt+s + - \st = s,at = a],	(2-19)
其中，y是折扣因子，r是基于状态s完成动作a得到的奖励值。按照贝尔曼方程,
公式(2-19)可以被分解为[呵：
Qn(s,a) = Es，,j[R(s,a) + YQAS', a],	(2-20)
17
其中，S，和出分别为下一个状态和动作。
令T = So，ao，Si，ai，	,s“az为一组"状态-动作"序列，尺（丫） =	at）
为T的累积回报，P（T；0）为T的概率，因此，梯度的目标函数a〕：
丿（0） = E（旻=oR（St，at）；7Te） = ETP（T；0）R（T）。	（2-21）
其中，策略梯度作为一个优化问题，需要通过寻找到最优的0,使得/（&）最大。
常用梯度上升求解该问题：
6 = 0 + aV0/（0）o	（2-22）
在求解该问题时，计算出丿（0）的导数为：
=XTP（T； 0）VelogP（T；0）/?（r）o	（2-23）
将（2-23）的公式转化为m条序列，可以得到bo〕：
=汨陰％ 10gP（T； 0） R（T）。	（2-24）
对于m条序列中的一条，T的概率p（T；e）为：
P（Ti;0）= H£=oP（sf+i|sf,a；）7r8（a；|sf）。	（2-25）
由（2-24）、（2-25）得到目标函数的梯度如下：
如⑹=号器iCXo^log眄何应）尺（刃）。	（2-26）
将（2-20）引入（2-26）,求解得目标函数的梯度为：
W 丿⑹=E问［yelog7r8（s,a）Q”（s,a）］。	（2-27）
在蒙特卡洛（Monte-Carlo）策略梯度REINFORCE算法冋中，可以用价值函 数对“状态-动作对”的Q函数进行无偏估计。REINFORCE算法如下：
算法1： REINFORCE算法
1：输入：MDP〈S, A, R, P}, y, 7ig ,迭代次数N
2：初始化：e
3： for episode = 1,... ...,N do
4：	初始化状态S］；
5：	将状态Si输入到策略yr。中，最终得到{sn a1（ r2,..., sT, aT, rT+1）；
6： for t = 1,... ...,N do
7： vt <- rt+1 + yrt+2 + …+ yT_trr+i；
8：	0 <- 0 + V0 log7r0（st,at）vt
9： end
10： end
11： 0*8
18
基于此，我们通过REINFORCE算法得到了智能体的最大累积奖励，也就是Q值 最大时的参数0。
