4.3基于强化学习的智能链路选择策略
本章所提出的智能链路选择策略不仅需要使得强化学习智能体的奖励最大 化，同时应该尽量减少分类网络的损失。基于此，我利用REINFORCE算法更新 智能体的参数％ = {&。,為,九}，并使用误差反向传播算法对分类网络的参数為进 行优化。
由于奖励函数的目标是增加神经网络的置信度，也就是激励智能体寻找出信 息更丰富的链路，从而逐步提高模型的准确性。考虑到预测分布S与链路选择的 趋势相关联，因此选择利用s计算智能体的奖励。基于此，令智能体在上时刻收到 的奖励为：
Rt = Pt,gt —	>	（4-7 ）
其中，Pt,cZ为类别C?在t时刻的概率值，以为行为的真实值标签。
受本文第2.4.2节中，策略梯度的REINFORCE算法启发，我将智能体的目 标设为：
丿（％） = lueu 兀（"Is；盅）G,	（4-8 ）
其中，G是长期的折扣回报。对于第t时刻，该长期折扣回报的累积值可以表示 为：
Gt = Hk=oYk^t-k,	（4-9）
其中，y是折扣因子。由式（4-9）可以看出，参数在更新方向上的位移最大，因 此有利的行为可以获得最高的回报。
目标函数/（%）的梯度为：
刃（％） = Xueu 兀（u|s;%）%2o9 7i（u|s;%）G。	（4-10）
若采用蒙特卡罗采样法估计所求梯度，则式（4-10）可以转化为：
St=o log n（utik\stik; B^Gt，	（4-11）
其中，K为样本数，T是最后一个时刻。通过蒙特卡罗随机梯度下降，可以最小化 损失函数，即式（4-12）,用于更新参数％。
厶丿（％） ~ —^°9 7l（y-t,k\st,k>	（4-12）
我将损失定义为动作空间中每个动作（即对各个链路的选择）的二元交叉爛 损失的均值：
5（%）= -扭 f=Q£=o 碗血9兀（如），	（4-13）
40
其中，兔,i是第：条链路在上时刻的概率，碣是第i条链路在上时刻的真实值，只能为 0或者1。因此，智能体的损失为两种损失的加权总和：
Lagent (気)=血厶丿(％) +厶［/(%)，	(4-14)
其中，血是一个常数。
由于本章的出发点是基于Wi-Fi的目标行为感知，故除了智能体的损失，还 应考虑到行为标签对于损失函数的影响。因此，将CNN分类网络的损失定义为 每个行为的二进制交叉矯损失的平均值：
其中，n为行为的类别个数，円为第j类的概率，丙是丿•类的真实值，只能为0或者 lo
综上，模型的总目标是使包括智能体和CNN分类网络二者的损失函数最小 化：
LOSS=久2 厶agent(°7t) + 厶戸(％)，	(4-16)
其中，忑是一个常数。
