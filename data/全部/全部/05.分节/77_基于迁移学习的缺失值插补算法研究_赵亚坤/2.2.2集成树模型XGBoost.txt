2.2.2集成树模型XGBoost
XGBoost(eXtreme Gradient Boosting)网是 Boosting 算法的其中一种。Boosting 算法的思想是将许多弱分类器集成在一起形成一个强分类器。在XGBoost中， 弱学习器为CART树，弱学习器集成的强学习器相比于原本的弱学习器，具有较 小的偏差(Bias)。XGBoost原理可分为Boosting和Gradient两个部分。
图2-3 GBDT的残差拟合示意图
XGBoost算法的Boosting思想是不断在之前训练的树模型上增添新的树模 型，来逐渐逼近真实值。其每次训练一棵新树，目标都是上一次树模型预测值与 真实值的残差。最终这个残差会在树模型的不断增长过程中越来越小，我们得到 的结果是所有的树预测值的总和。以实例来阐述这一过程：令训练集为[A, B, C, D],集合中的每个元素是数据集中的一条样本，label为[14,16,24,26],分别对
20
应训练样本。我们需要训练一棵决策树，使之可以找到训练集与label的对应关 系，例如输入一个样本A,树模型可以输出结果14。
如图2-3所示，每个方框的第一行数字代表上一棵树预测的数值，第二行代 表该节点剩余的数据标签。假设我们的决策树生成方式是以数据的均值作为划分 点切分,在第一棵树生成过程中，我们可以使用均值20作为第一棵树的切分点， 将数据切分成两部分，第一部分是以15为均值的［14,16］,第二部分是以25为均 值的［24, 26］,在此基础上继续生成新树逐渐逼近真实值。在第二棵树生成之前， 我们将上一次计算的值和真实值做一次残差计算，将计算过的残差作为第二棵树 模型生成的依据。
表2~4梯度提升算法描述
输入：训练数据集 T= {（x1,j/!）, （x2,y2（x^y^},xiex^Rn,yiEYQR，,损失函数
L(y，Ax))
输出：回归树市） 1）初始化：
f0(^)=argmmL{y.,c)
2)对m = 1,2,…,M：
a)对i = 1,2,…,N,计算：
_"(力/U))
5- g)
」网=4_13
(2-16)
(2-17)
b）对Gi拟合一个回归树，得到第加棵树的叶结点区域Rmj，j = l,2，…,J
c）对j=l,2,…,J,计算:
Cmj=argmm	上供工」（匹）+c）	（2-18）
更就(x) =fm.x (x) + * 1 cmjI(xERmj)-,
3）得到回归树:
加）九（x）= W tW t为”烧"叨）	（2/9）
在第一棵树的生成过程中，我们生成了两个结点：均值为15的［14,16］和均值 为25的［24,26］,将此两个结点的真实值减去第一棵树预测的值，得出了第一棵
21
树计算的两个残差［-1,1］和［-1,1］,那么第二棵树的计算便按照这残差来进行。我 们第二棵树也是按照均值划分，以0作为第一棵树左结点残差的划分点，将左结 点切分成-1和1两个子结点；同理第三棵树将右结点切分。这样切分完之后便得 到了一个由三棵树组成的集成决策树模型。那么我们如果输入一个样本A,该模 型可以正确的计算出A的标签值为14,计算过程为每棵树的预测值之和。即 A:15+(-l)=14o
XGBoost的Gradient概念指的是将梯度下降的思想引入进决策树模型。通过 某种巧妙地方法，我们可以将Boosting和梯度下降结合起来，使得Boosting训 练器在迭代过程中沿着梯度下降的方向不断优化，这也从理论上证明了该模型是 收敛的。在XGBoost中，我们利用损失函数的负梯度作为当前模型需要拟合的 残差。梯度提升算法的流程如表2必所示。
对算法全过程作详细解释：第1)步为算法的初始化步骤，初始化一个损失函 数使其极小化，作为只有一个根节点的树；第2)a步骤计算第1)步初始化的损失 函数的梯度负值和当前模型的值，求出模型残差；第2)b步拟合回归树，得到回 归树叶结点区域；第2)c步使用优化算法使得损失函数极小化；第2(d)步更新回 归树。第3步得到最终的模型f(x)。
