3.3.1选取弱学习器
TradaBoost算法是基于决策树模型作为弱学习器的算法，因此我们选取的不 同弱学习器，都是决策树模型。弱学习器供选择如下：
1)	CART (Classification And Regression Tree)模型，最早的统一了分类树和回 归树的模型。也是基础决策树算法之一；
2)	RF (RandomForest)模型，随机森林模型是集成学习的一种，将多棵决策树 的结果投票，选取投票的结果作为最终结果；
3)	GBDT (Gradient Boost Decision Tree)模型，GBDT 是集成学习的一种，每棵 树都在学习上一轮的残差，使得迭代结果越来越逼近真实值。其代表作主要 是 XGBoost o
决策树模型还有一种是AdaBoost, TradaBoost的原理也是基于AdaBoost修 改而来，所以两种算法的原理非常类似。在对相同的样本进行权重迭代时，可能 会出现AdaBoost计算的权重和TradaBoost迭代计算的权重相同，在AdaBoost下 不能正确分类的样本，TradaBoost同样不能正确分类。这会导致最终结果泛化性 能下降，对结果造成干扰，故不选用。
现实中数据集的缺失值是随机的，为了更好地模拟这一特性，我们将待实验 的数据集中随机产生一定比例的缺失值。在本次数据集中，由于数据集的收缩压 特征本身具有30%的缺失数据，在设计实验的时候不能提前对这30%的缺失值 进行处理。所以我们设置缺失值比例时，从30%的缺失比例开始，按照30%至 100%的区间分成5等份，所以我们随机设置30%, 44%, 58%, 72%, 86%, 100%
31
比例的缺失值。接着我们按照相同比例缺失值的数据集中，采用不同弱学习器构 建的迁移学习方法来作为实验组，实验组相互对照。我们可以根据最终结果选用 最佳的弱学习器。
表35几种树模型对缺失值插补的影响
短失百分比	GBDT	RF	CART
30%	0.632	0.635	0.630
44%	0.622	0.616	0.612
58%	0.580	0.578	0.578
72%	0.547	0.546	0.550
86%	0.516	0.501	0.508
100%	0.514	0.503	0.505
如表3-4是不同算法在是数据集上的缺失值插补表现。其中第一列是数据集 特列的缺失值比例，表格中数据是插补缺失值之后整个数据集的AUC指标。
由图中分析可知，不同的弱学习器对最终的缺失值插补效果是有影响的。在 不同缺失值比例下，不同算法作为弱学习器的性能排名是不一样的。但是综合来 看，XGBoost算法效果最好，CART次之，随机森林(RandomForest)最差。通 过本次实验可以得出结论，在本糖尿病数据集下，弱学习器选用XGBoost效果 最优。并选用以XGBoost算法作为弱学习器的TradaBoost算法作为本次实验缺 失值插补算法。
