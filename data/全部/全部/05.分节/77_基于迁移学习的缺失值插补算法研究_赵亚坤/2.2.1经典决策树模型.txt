2.2.1经典决策树模型
决策树算法最早由Quinlan在1986年提出卬】。其是一种非参数算法，能够再 数据中建立一种规则，让数据流沿着规则来得到划分的结果，不需要得到数据样 本的分布情况。
图2-2决策树结构示意图[28]
接下来给出决策树的理论解释，设训练集为7 = {(而,弘),(马,%),…,(x“，Mv)}， 其中玉=(染,X?…H"))7'是特征向量，〃是特征的数量，乂 g{1,2,…,K}表示标 签。最终根据规则将所有样本划分到如图2-2所示的一个树形结构的叶节点上, 每一个叶节点就是一个单独的类别。
表2-3 CART算法描述 输入：数据集D,损失函数loss
输出：树小)
在训练数据集所在空间内，循环的将每个区域划分子区域，构建二叉树:
1)选择最优切分变量，与切分点s,求解:
2)
min [mi〃/。ss(x,c)]
遍历扫描j和s,使之能够最小化损失函数； 用选定的(M)对划分区域并决定相应的输出值：
RiO'，s) = {对X⑺ < s})	R2(j,s') = {x|xw > s}
2很=a W Fj xERm, 07=1,2
x,^Rm(j,s)
(2-12)
(2-13)
(2-14)
3)重复直到满足停止条件；
4)将输入空间划分为〃个区域R1,R2,…,Rm，生成决策树：
加)=〉	21nl(x e Rm)	(2-15)
19
决策树有回归树和分类树之分，在二者的区别中，回归树是采用最大均方误 差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树 是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投票决 定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。 回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好 地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更 好的从整体把握这个数据集的分类。
CART算法将回归树和分类树统一起来，通过更换损失函数的方法来调整决 策树是分类树还是回归树的属性。CART的算法如表2-3所示。
